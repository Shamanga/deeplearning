{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.utils import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import warnings\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our targets\n",
    "   > If first number is larger than second number = 0 \n",
    "   \n",
    "   > If the numbers are equal = 1\n",
    "   \n",
    "   > If the second number is larger than the first number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistPairs(Dataset):\n",
    "    \n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self):\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "    \n",
    "    @property\n",
    "    def train_classes(self):\n",
    "        warnings.warn(\"train_classes has been renamed classes\")\n",
    "        return self.classes\n",
    "    \n",
    "    @property\n",
    "    def test_classes(self):\n",
    "        warnings.warn(\"test_classes has been renamed classes\")\n",
    "        return self.classes\n",
    "    \n",
    "    @property\n",
    "    def processed_folder(self):\n",
    "        return os.path.join(self.root, self.__class__.__name__, 'processed')\n",
    "    \n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(os.path.join(self.processed_folder, self.training_file)) and \\\n",
    "            os.path.exists(os.path.join(self.processed_folder, self.test_file))\n",
    "    \n",
    "    def mnist_to_pairs(self, nb, input, target):\n",
    "        input = torch.functional.F.avg_pool2d(input, kernel_size = 2)\n",
    "        a = torch.randperm(input.size(0))\n",
    "        a = a[:2 * nb].view(nb, 2)\n",
    "        input = torch.cat((input[a[:, 0]], input[a[:, 1]]), 1)\n",
    "        classes = target[a]\n",
    "        target = (classes[:, 0] <= classes[:, 1]).long()\n",
    "        return input, target, classes\n",
    "\n",
    "    def generate_pair_sets(self, nb):\n",
    "        \n",
    "        if self.train:\n",
    "            dataset = datasets.MNIST(self.root + '/mnist/', train = True, download = True)\n",
    "            dataset_input = dataset.train_data.view(-1, 1, 28, 28).float()\n",
    "            dataset_target = dataset.train_labels\n",
    "        \n",
    "        else:\n",
    "            dataset = datasets.MNIST(self.root + '/mnist/', train = False, download = True)\n",
    "            dataset_input = dataset.test_data.view(-1, 1, 28, 28).float()\n",
    "            dataset_target = dataset.test_labels\n",
    "\n",
    "        return self.mnist_to_pairs(nb, dataset_input, dataset_target)\n",
    "    \n",
    "    def __init__(self, root, nb = 1000, train=True, transform=None, target_transform=None):\n",
    "\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "        \n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.')\n",
    "            \n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "        \n",
    "        self.data ,self.targets, self.classes = self.generate_pair_sets(nb)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        \n",
    "        img_1 = Image.fromarray(img[0].numpy(), mode='L')\n",
    "        img_2 = Image.fromarray(img[1].numpy(), mode='L')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_1 = self.transform(img_1)\n",
    "            img_2 = self.transform(img_2)\n",
    "            \n",
    "            img = torch.stack((img_1.reshape(img_1.shape[1],img_1.shape[2]),img_2.reshape(img_2.shape[1],img_2.shape[2])),dim=0)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_dataset = MnistPairs('data/',train=True, transform=None)\n",
    "test_dataset = MnistPairs('data/',train=False, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=128, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the network          \n",
    "class CNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        #Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #Max pool 1\n",
    "        #self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        #Max pool 2\n",
    "        #self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        #Fully Connected 1\n",
    "        self.fc1 = nn.Linear(32*14*14, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        #Max pool 1\n",
    "        #out = self.maxpool1(out)\n",
    "        \n",
    "        #Convolution 2\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        #Max pool 2\n",
    "        #out = self.maxpool2(out)\n",
    "        \n",
    "        #Resize\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        #Dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        #Fully connected 1\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instance of model\n",
    "model = CNNModel()\n",
    "#Create instance of loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#Create instance of optimizer (Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [1/8], Loss: 6.4525, Accuracy: 46.09%\n",
      "Epoch [1/25], Step [2/8], Loss: 43.5459, Accuracy: 51.56%\n",
      "Epoch [1/25], Step [3/8], Loss: 18.1544, Accuracy: 53.91%\n",
      "Epoch [1/25], Step [4/8], Loss: 4.9901, Accuracy: 53.12%\n",
      "Epoch [1/25], Step [5/8], Loss: 1.1381, Accuracy: 53.12%\n",
      "Epoch [1/25], Step [6/8], Loss: 0.9469, Accuracy: 58.59%\n",
      "Epoch [1/25], Step [7/8], Loss: 1.0300, Accuracy: 47.66%\n",
      "Epoch [1/25], Step [8/8], Loss: 1.1054, Accuracy: 40.38%\n",
      "Epoch [2/25], Step [1/8], Loss: 0.8472, Accuracy: 47.66%\n",
      "Epoch [2/25], Step [2/8], Loss: 0.7792, Accuracy: 50.00%\n",
      "Epoch [2/25], Step [3/8], Loss: 0.7520, Accuracy: 47.66%\n",
      "Epoch [2/25], Step [4/8], Loss: 0.7228, Accuracy: 50.78%\n",
      "Epoch [2/25], Step [5/8], Loss: 0.6695, Accuracy: 58.59%\n",
      "Epoch [2/25], Step [6/8], Loss: 0.6613, Accuracy: 57.03%\n",
      "Epoch [2/25], Step [7/8], Loss: 0.6831, Accuracy: 53.12%\n",
      "Epoch [2/25], Step [8/8], Loss: 0.6684, Accuracy: 59.62%\n",
      "Epoch [3/25], Step [1/8], Loss: 0.6668, Accuracy: 59.38%\n",
      "Epoch [3/25], Step [2/8], Loss: 0.6482, Accuracy: 64.84%\n",
      "Epoch [3/25], Step [3/8], Loss: 0.6300, Accuracy: 66.41%\n",
      "Epoch [3/25], Step [4/8], Loss: 0.5993, Accuracy: 78.91%\n",
      "Epoch [3/25], Step [5/8], Loss: 0.6156, Accuracy: 67.19%\n",
      "Epoch [3/25], Step [6/8], Loss: 0.5971, Accuracy: 68.75%\n",
      "Epoch [3/25], Step [7/8], Loss: 0.5950, Accuracy: 68.75%\n",
      "Epoch [3/25], Step [8/8], Loss: 0.5978, Accuracy: 65.38%\n",
      "Epoch [4/25], Step [1/8], Loss: 0.6699, Accuracy: 72.66%\n",
      "Epoch [4/25], Step [2/8], Loss: 0.5563, Accuracy: 71.88%\n",
      "Epoch [4/25], Step [3/8], Loss: 0.6039, Accuracy: 71.09%\n",
      "Epoch [4/25], Step [4/8], Loss: 0.5638, Accuracy: 71.88%\n",
      "Epoch [4/25], Step [5/8], Loss: 0.6426, Accuracy: 69.53%\n",
      "Epoch [4/25], Step [6/8], Loss: 0.5629, Accuracy: 74.22%\n",
      "Epoch [4/25], Step [7/8], Loss: 0.6031, Accuracy: 66.41%\n",
      "Epoch [4/25], Step [8/8], Loss: 0.5407, Accuracy: 70.19%\n",
      "Epoch [5/25], Step [1/8], Loss: 0.6104, Accuracy: 71.09%\n",
      "Epoch [5/25], Step [2/8], Loss: 0.5352, Accuracy: 72.66%\n",
      "Epoch [5/25], Step [3/8], Loss: 0.5579, Accuracy: 71.09%\n",
      "Epoch [5/25], Step [4/8], Loss: 0.4730, Accuracy: 80.47%\n",
      "Epoch [5/25], Step [5/8], Loss: 0.5673, Accuracy: 74.22%\n",
      "Epoch [5/25], Step [6/8], Loss: 0.5362, Accuracy: 74.22%\n",
      "Epoch [5/25], Step [7/8], Loss: 0.5242, Accuracy: 72.66%\n",
      "Epoch [5/25], Step [8/8], Loss: 0.5189, Accuracy: 73.08%\n",
      "Epoch [6/25], Step [1/8], Loss: 0.5695, Accuracy: 71.09%\n",
      "Epoch [6/25], Step [2/8], Loss: 0.5369, Accuracy: 73.44%\n",
      "Epoch [6/25], Step [3/8], Loss: 0.5311, Accuracy: 73.44%\n",
      "Epoch [6/25], Step [4/8], Loss: 0.4553, Accuracy: 78.12%\n",
      "Epoch [6/25], Step [5/8], Loss: 0.5537, Accuracy: 71.88%\n",
      "Epoch [6/25], Step [6/8], Loss: 0.5084, Accuracy: 78.12%\n",
      "Epoch [6/25], Step [7/8], Loss: 0.5190, Accuracy: 71.88%\n",
      "Epoch [6/25], Step [8/8], Loss: 0.5118, Accuracy: 75.96%\n",
      "Epoch [7/25], Step [1/8], Loss: 0.5453, Accuracy: 71.88%\n",
      "Epoch [7/25], Step [2/8], Loss: 0.5364, Accuracy: 74.22%\n",
      "Epoch [7/25], Step [3/8], Loss: 0.5116, Accuracy: 71.88%\n",
      "Epoch [7/25], Step [4/8], Loss: 0.4381, Accuracy: 77.34%\n",
      "Epoch [7/25], Step [5/8], Loss: 0.5442, Accuracy: 71.88%\n",
      "Epoch [7/25], Step [6/8], Loss: 0.4993, Accuracy: 76.56%\n",
      "Epoch [7/25], Step [7/8], Loss: 0.5400, Accuracy: 72.66%\n",
      "Epoch [7/25], Step [8/8], Loss: 0.4734, Accuracy: 76.92%\n",
      "Epoch [8/25], Step [1/8], Loss: 0.5723, Accuracy: 71.09%\n",
      "Epoch [8/25], Step [2/8], Loss: 0.5047, Accuracy: 76.56%\n",
      "Epoch [8/25], Step [3/8], Loss: 0.4695, Accuracy: 78.91%\n",
      "Epoch [8/25], Step [4/8], Loss: 0.4005, Accuracy: 82.03%\n",
      "Epoch [8/25], Step [5/8], Loss: 0.4888, Accuracy: 76.56%\n",
      "Epoch [8/25], Step [6/8], Loss: 0.4996, Accuracy: 75.78%\n",
      "Epoch [8/25], Step [7/8], Loss: 0.5148, Accuracy: 75.00%\n",
      "Epoch [8/25], Step [8/8], Loss: 0.4844, Accuracy: 77.88%\n",
      "Epoch [9/25], Step [1/8], Loss: 0.4813, Accuracy: 78.91%\n",
      "Epoch [9/25], Step [2/8], Loss: 0.4990, Accuracy: 79.69%\n",
      "Epoch [9/25], Step [3/8], Loss: 0.4926, Accuracy: 82.03%\n",
      "Epoch [9/25], Step [4/8], Loss: 0.3983, Accuracy: 79.69%\n",
      "Epoch [9/25], Step [5/8], Loss: 0.5046, Accuracy: 71.88%\n",
      "Epoch [9/25], Step [6/8], Loss: 0.4637, Accuracy: 80.47%\n",
      "Epoch [9/25], Step [7/8], Loss: 0.4665, Accuracy: 80.47%\n",
      "Epoch [9/25], Step [8/8], Loss: 0.4535, Accuracy: 77.88%\n",
      "Epoch [10/25], Step [1/8], Loss: 0.5333, Accuracy: 74.22%\n",
      "Epoch [10/25], Step [2/8], Loss: 0.4758, Accuracy: 78.91%\n",
      "Epoch [10/25], Step [3/8], Loss: 0.4633, Accuracy: 82.03%\n",
      "Epoch [10/25], Step [4/8], Loss: 0.3757, Accuracy: 83.59%\n",
      "Epoch [10/25], Step [5/8], Loss: 0.5028, Accuracy: 76.56%\n",
      "Epoch [10/25], Step [6/8], Loss: 0.4299, Accuracy: 78.91%\n",
      "Epoch [10/25], Step [7/8], Loss: 0.4599, Accuracy: 78.91%\n",
      "Epoch [10/25], Step [8/8], Loss: 0.4144, Accuracy: 80.77%\n",
      "Epoch [11/25], Step [1/8], Loss: 0.4921, Accuracy: 78.12%\n",
      "Epoch [11/25], Step [2/8], Loss: 0.4494, Accuracy: 76.56%\n",
      "Epoch [11/25], Step [3/8], Loss: 0.4720, Accuracy: 76.56%\n",
      "Epoch [11/25], Step [4/8], Loss: 0.3690, Accuracy: 82.81%\n",
      "Epoch [11/25], Step [5/8], Loss: 0.5035, Accuracy: 73.44%\n",
      "Epoch [11/25], Step [6/8], Loss: 0.4506, Accuracy: 78.12%\n",
      "Epoch [11/25], Step [7/8], Loss: 0.4859, Accuracy: 78.91%\n",
      "Epoch [11/25], Step [8/8], Loss: 0.3989, Accuracy: 81.73%\n",
      "Epoch [12/25], Step [1/8], Loss: 0.4847, Accuracy: 78.91%\n",
      "Epoch [12/25], Step [2/8], Loss: 0.4409, Accuracy: 82.81%\n",
      "Epoch [12/25], Step [3/8], Loss: 0.4378, Accuracy: 82.03%\n",
      "Epoch [12/25], Step [4/8], Loss: 0.3756, Accuracy: 84.38%\n",
      "Epoch [12/25], Step [5/8], Loss: 0.4459, Accuracy: 75.78%\n",
      "Epoch [12/25], Step [6/8], Loss: 0.4321, Accuracy: 75.78%\n",
      "Epoch [12/25], Step [7/8], Loss: 0.4271, Accuracy: 81.25%\n",
      "Epoch [12/25], Step [8/8], Loss: 0.3930, Accuracy: 81.73%\n",
      "Epoch [13/25], Step [1/8], Loss: 0.4837, Accuracy: 81.25%\n",
      "Epoch [13/25], Step [2/8], Loss: 0.4445, Accuracy: 81.25%\n",
      "Epoch [13/25], Step [3/8], Loss: 0.4183, Accuracy: 79.69%\n",
      "Epoch [13/25], Step [4/8], Loss: 0.3662, Accuracy: 83.59%\n",
      "Epoch [13/25], Step [5/8], Loss: 0.3915, Accuracy: 82.03%\n",
      "Epoch [13/25], Step [6/8], Loss: 0.3895, Accuracy: 85.16%\n",
      "Epoch [13/25], Step [7/8], Loss: 0.4465, Accuracy: 79.69%\n",
      "Epoch [13/25], Step [8/8], Loss: 0.3681, Accuracy: 83.65%\n",
      "Epoch [14/25], Step [1/8], Loss: 0.4424, Accuracy: 82.03%\n",
      "Epoch [14/25], Step [2/8], Loss: 0.4112, Accuracy: 85.94%\n",
      "Epoch [14/25], Step [3/8], Loss: 0.3834, Accuracy: 82.03%\n",
      "Epoch [14/25], Step [4/8], Loss: 0.3413, Accuracy: 87.50%\n",
      "Epoch [14/25], Step [5/8], Loss: 0.4279, Accuracy: 75.00%\n",
      "Epoch [14/25], Step [6/8], Loss: 0.3669, Accuracy: 85.16%\n",
      "Epoch [14/25], Step [7/8], Loss: 0.4251, Accuracy: 80.47%\n",
      "Epoch [14/25], Step [8/8], Loss: 0.3341, Accuracy: 81.73%\n",
      "Epoch [15/25], Step [1/8], Loss: 0.4281, Accuracy: 81.25%\n",
      "Epoch [15/25], Step [2/8], Loss: 0.4330, Accuracy: 79.69%\n",
      "Epoch [15/25], Step [3/8], Loss: 0.4050, Accuracy: 78.91%\n",
      "Epoch [15/25], Step [4/8], Loss: 0.3123, Accuracy: 89.06%\n",
      "Epoch [15/25], Step [5/8], Loss: 0.4235, Accuracy: 76.56%\n",
      "Epoch [15/25], Step [6/8], Loss: 0.3505, Accuracy: 83.59%\n",
      "Epoch [15/25], Step [7/8], Loss: 0.4187, Accuracy: 84.38%\n",
      "Epoch [15/25], Step [8/8], Loss: 0.3219, Accuracy: 86.54%\n",
      "Epoch [16/25], Step [1/8], Loss: 0.4586, Accuracy: 80.47%\n",
      "Epoch [16/25], Step [2/8], Loss: 0.3738, Accuracy: 83.59%\n",
      "Epoch [16/25], Step [3/8], Loss: 0.3839, Accuracy: 82.03%\n",
      "Epoch [16/25], Step [4/8], Loss: 0.3188, Accuracy: 85.94%\n",
      "Epoch [16/25], Step [5/8], Loss: 0.4049, Accuracy: 80.47%\n",
      "Epoch [16/25], Step [6/8], Loss: 0.4285, Accuracy: 78.91%\n",
      "Epoch [16/25], Step [7/8], Loss: 0.4335, Accuracy: 79.69%\n",
      "Epoch [16/25], Step [8/8], Loss: 0.3199, Accuracy: 83.65%\n",
      "Epoch [17/25], Step [1/8], Loss: 0.3950, Accuracy: 81.25%\n",
      "Epoch [17/25], Step [2/8], Loss: 0.4268, Accuracy: 78.12%\n",
      "Epoch [17/25], Step [3/8], Loss: 0.3185, Accuracy: 87.50%\n",
      "Epoch [17/25], Step [4/8], Loss: 0.3050, Accuracy: 85.16%\n",
      "Epoch [17/25], Step [5/8], Loss: 0.3474, Accuracy: 84.38%\n",
      "Epoch [17/25], Step [6/8], Loss: 0.3202, Accuracy: 85.94%\n",
      "Epoch [17/25], Step [7/8], Loss: 0.4073, Accuracy: 82.03%\n",
      "Epoch [17/25], Step [8/8], Loss: 0.2920, Accuracy: 88.46%\n",
      "Epoch [18/25], Step [1/8], Loss: 0.3903, Accuracy: 82.03%\n",
      "Epoch [18/25], Step [2/8], Loss: 0.4211, Accuracy: 80.47%\n",
      "Epoch [18/25], Step [3/8], Loss: 0.3502, Accuracy: 85.94%\n",
      "Epoch [18/25], Step [4/8], Loss: 0.2899, Accuracy: 87.50%\n",
      "Epoch [18/25], Step [5/8], Loss: 0.4004, Accuracy: 82.03%\n",
      "Epoch [18/25], Step [6/8], Loss: 0.3467, Accuracy: 85.94%\n",
      "Epoch [18/25], Step [7/8], Loss: 0.3951, Accuracy: 82.03%\n",
      "Epoch [18/25], Step [8/8], Loss: 0.2641, Accuracy: 89.42%\n",
      "Epoch [19/25], Step [1/8], Loss: 0.3681, Accuracy: 86.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Step [2/8], Loss: 0.3166, Accuracy: 85.94%\n",
      "Epoch [19/25], Step [3/8], Loss: 0.3485, Accuracy: 83.59%\n",
      "Epoch [19/25], Step [4/8], Loss: 0.2943, Accuracy: 87.50%\n",
      "Epoch [19/25], Step [5/8], Loss: 0.3280, Accuracy: 84.38%\n",
      "Epoch [19/25], Step [6/8], Loss: 0.3302, Accuracy: 82.81%\n",
      "Epoch [19/25], Step [7/8], Loss: 0.3803, Accuracy: 79.69%\n",
      "Epoch [19/25], Step [8/8], Loss: 0.2770, Accuracy: 88.46%\n",
      "Epoch [20/25], Step [1/8], Loss: 0.3715, Accuracy: 82.81%\n",
      "Epoch [20/25], Step [2/8], Loss: 0.3379, Accuracy: 83.59%\n",
      "Epoch [20/25], Step [3/8], Loss: 0.3492, Accuracy: 88.28%\n",
      "Epoch [20/25], Step [4/8], Loss: 0.2988, Accuracy: 85.94%\n",
      "Epoch [20/25], Step [5/8], Loss: 0.3362, Accuracy: 87.50%\n",
      "Epoch [20/25], Step [6/8], Loss: 0.3129, Accuracy: 87.50%\n",
      "Epoch [20/25], Step [7/8], Loss: 0.3810, Accuracy: 82.03%\n",
      "Epoch [20/25], Step [8/8], Loss: 0.2652, Accuracy: 89.42%\n",
      "Epoch [21/25], Step [1/8], Loss: 0.3769, Accuracy: 85.16%\n",
      "Epoch [21/25], Step [2/8], Loss: 0.3277, Accuracy: 88.28%\n",
      "Epoch [21/25], Step [3/8], Loss: 0.3130, Accuracy: 85.16%\n",
      "Epoch [21/25], Step [4/8], Loss: 0.2404, Accuracy: 88.28%\n",
      "Epoch [21/25], Step [5/8], Loss: 0.3047, Accuracy: 86.72%\n",
      "Epoch [21/25], Step [6/8], Loss: 0.3086, Accuracy: 87.50%\n",
      "Epoch [21/25], Step [7/8], Loss: 0.3655, Accuracy: 85.94%\n",
      "Epoch [21/25], Step [8/8], Loss: 0.3326, Accuracy: 84.62%\n",
      "Epoch [22/25], Step [1/8], Loss: 0.3711, Accuracy: 85.94%\n",
      "Epoch [22/25], Step [2/8], Loss: 0.3650, Accuracy: 84.38%\n",
      "Epoch [22/25], Step [3/8], Loss: 0.3461, Accuracy: 83.59%\n",
      "Epoch [22/25], Step [4/8], Loss: 0.2587, Accuracy: 89.06%\n",
      "Epoch [22/25], Step [5/8], Loss: 0.2769, Accuracy: 89.84%\n",
      "Epoch [22/25], Step [6/8], Loss: 0.2895, Accuracy: 86.72%\n",
      "Epoch [22/25], Step [7/8], Loss: 0.3024, Accuracy: 85.94%\n",
      "Epoch [22/25], Step [8/8], Loss: 0.2358, Accuracy: 89.42%\n",
      "Epoch [23/25], Step [1/8], Loss: 0.3010, Accuracy: 85.94%\n",
      "Epoch [23/25], Step [2/8], Loss: 0.2657, Accuracy: 91.41%\n",
      "Epoch [23/25], Step [3/8], Loss: 0.2480, Accuracy: 88.28%\n",
      "Epoch [23/25], Step [4/8], Loss: 0.2701, Accuracy: 89.84%\n",
      "Epoch [23/25], Step [5/8], Loss: 0.2818, Accuracy: 89.06%\n",
      "Epoch [23/25], Step [6/8], Loss: 0.2736, Accuracy: 85.94%\n",
      "Epoch [23/25], Step [7/8], Loss: 0.2906, Accuracy: 88.28%\n",
      "Epoch [23/25], Step [8/8], Loss: 0.2035, Accuracy: 91.35%\n",
      "Epoch [24/25], Step [1/8], Loss: 0.3291, Accuracy: 87.50%\n",
      "Epoch [24/25], Step [2/8], Loss: 0.2674, Accuracy: 89.84%\n",
      "Epoch [24/25], Step [3/8], Loss: 0.2846, Accuracy: 87.50%\n",
      "Epoch [24/25], Step [4/8], Loss: 0.2185, Accuracy: 91.41%\n",
      "Epoch [24/25], Step [5/8], Loss: 0.2761, Accuracy: 87.50%\n",
      "Epoch [24/25], Step [6/8], Loss: 0.2708, Accuracy: 87.50%\n",
      "Epoch [24/25], Step [7/8], Loss: 0.2995, Accuracy: 85.94%\n",
      "Epoch [24/25], Step [8/8], Loss: 0.2119, Accuracy: 94.23%\n",
      "Epoch [25/25], Step [1/8], Loss: 0.2737, Accuracy: 89.06%\n",
      "Epoch [25/25], Step [2/8], Loss: 0.2424, Accuracy: 89.84%\n",
      "Epoch [25/25], Step [3/8], Loss: 0.2716, Accuracy: 86.72%\n",
      "Epoch [25/25], Step [4/8], Loss: 0.1728, Accuracy: 92.19%\n",
      "Epoch [25/25], Step [5/8], Loss: 0.2562, Accuracy: 89.06%\n",
      "Epoch [25/25], Step [6/8], Loss: 0.3180, Accuracy: 85.16%\n",
      "Epoch [25/25], Step [7/8], Loss: 0.2782, Accuracy: 88.28%\n",
      "Epoch [25/25], Step [8/8], Loss: 0.2180, Accuracy: 92.31%\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "iter = 0\n",
    "for epoch in range(25):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward propagation \n",
    "        outputs = model(images)      \n",
    "        \n",
    "        #Calculating loss with softmax to obtain cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Backward propation\n",
    "        loss.backward()\n",
    "        \n",
    "        #Updating gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        #Total number of labels\n",
    "        total = labels.size(0)\n",
    "        \n",
    "        #Obtaining predictions from max value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        #Calculate the number of correct answers\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        \n",
    "        #Print loss and accuracy\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "              .format(epoch + 1, 25, i + 1, len(train_loader), loss.item(),\n",
    "                      (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 71.7 %\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 1000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
