{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      " Start of the Network 2 \n",
      "The number of parameters of the network is  8862\n",
      "Epoch [2/25],Step [1/64],Loss: 16.0171, Acc_digits: 10/128 (8%) ,Acc_labels: 27/64 (42%)\n",
      "Epoch [2/25],Step [2/64],Loss: 7.6165, Acc_digits: 12/128 (9%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [2/25],Step [3/64],Loss: 3.8902, Acc_digits: 13/128 (10%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [2/25],Step [4/64],Loss: 3.1956, Acc_digits: 17/128 (13%) ,Acc_labels: 29/64 (45%)\n",
      "Epoch [2/25],Step [5/64],Loss: 3.1694, Acc_digits: 11/128 (9%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [2/25],Step [6/64],Loss: 3.0226, Acc_digits: 20/128 (16%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [2/25],Step [7/64],Loss: 2.9954, Acc_digits: 16/128 (12%) ,Acc_labels: 30/64 (47%)\n",
      "Epoch [2/25],Step [8/64],Loss: 3.0332, Acc_digits: 17/128 (13%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [2/25],Step [9/64],Loss: 3.0150, Acc_digits: 14/128 (11%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [2/25],Step [10/64],Loss: 2.9748, Acc_digits: 22/128 (17%) ,Acc_labels: 31/64 (48%)\n",
      "Epoch [2/25],Step [11/64],Loss: 2.9475, Acc_digits: 19/128 (15%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [2/25],Step [12/64],Loss: 2.9333, Acc_digits: 21/128 (16%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [2/25],Step [13/64],Loss: 2.9481, Acc_digits: 17/128 (13%) ,Acc_labels: 31/64 (48%)\n",
      "Epoch [2/25],Step [14/64],Loss: 2.9587, Acc_digits: 23/128 (18%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [2/25],Step [15/64],Loss: 2.8532, Acc_digits: 25/128 (20%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [2/25],Step [16/40],Loss: 2.8789, Acc_digits: 18/80 (22%) ,Acc_labels: 23/40 (57%)\n",
      "Time needed to train  0.24960930389352143\n",
      "\n",
      "Test set: Loss: 2.8898,Acc_digits: 478/2000 (24%),Acc_labels: 570/1000 57%\n",
      "Epoch [3/25],Step [1/64],Loss: 2.8536, Acc_digits: 23/128 (18%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [3/25],Step [2/64],Loss: 2.8712, Acc_digits: 24/128 (19%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [3/25],Step [3/64],Loss: 2.8993, Acc_digits: 22/128 (17%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [3/25],Step [4/64],Loss: 2.8938, Acc_digits: 23/128 (18%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [3/25],Step [5/64],Loss: 2.8668, Acc_digits: 24/128 (19%) ,Acc_labels: 23/64 (36%)\n",
      "Epoch [3/25],Step [6/64],Loss: 2.8471, Acc_digits: 26/128 (20%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [3/25],Step [7/64],Loss: 2.8298, Acc_digits: 33/128 (26%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [3/25],Step [8/64],Loss: 2.9326, Acc_digits: 16/128 (12%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [3/25],Step [9/64],Loss: 2.8603, Acc_digits: 24/128 (19%) ,Acc_labels: 30/64 (47%)\n",
      "Epoch [3/25],Step [10/64],Loss: 2.8651, Acc_digits: 25/128 (20%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [3/25],Step [11/64],Loss: 2.8287, Acc_digits: 24/128 (19%) ,Acc_labels: 28/64 (44%)\n",
      "Epoch [3/25],Step [12/64],Loss: 2.7178, Acc_digits: 33/128 (26%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [3/25],Step [13/64],Loss: 2.8549, Acc_digits: 34/128 (27%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [3/25],Step [14/64],Loss: 2.8885, Acc_digits: 17/128 (13%) ,Acc_labels: 31/64 (48%)\n",
      "Epoch [3/25],Step [15/64],Loss: 2.7803, Acc_digits: 35/128 (27%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [3/25],Step [16/40],Loss: 2.8310, Acc_digits: 18/80 (22%) ,Acc_labels: 25/40 (62%)\n",
      "Time needed to train  0.23188940796535462\n",
      "\n",
      "Test set: Loss: 2.7698,Acc_digits: 671/2000 (34%),Acc_labels: 583/1000 58%\n",
      "Epoch [4/25],Step [1/64],Loss: 2.8207, Acc_digits: 26/128 (20%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [4/25],Step [2/64],Loss: 2.7778, Acc_digits: 20/128 (16%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [3/64],Loss: 2.7926, Acc_digits: 26/128 (20%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [4/25],Step [4/64],Loss: 2.8485, Acc_digits: 24/128 (19%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [4/25],Step [5/64],Loss: 2.7346, Acc_digits: 34/128 (27%) ,Acc_labels: 27/64 (42%)\n",
      "Epoch [4/25],Step [6/64],Loss: 2.8159, Acc_digits: 26/128 (20%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [4/25],Step [7/64],Loss: 2.7626, Acc_digits: 25/128 (20%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [4/25],Step [8/64],Loss: 2.8860, Acc_digits: 20/128 (16%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [4/25],Step [9/64],Loss: 2.9245, Acc_digits: 19/128 (15%) ,Acc_labels: 29/64 (45%)\n",
      "Epoch [4/25],Step [10/64],Loss: 2.8520, Acc_digits: 25/128 (20%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [4/25],Step [11/64],Loss: 2.7819, Acc_digits: 33/128 (26%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [4/25],Step [12/64],Loss: 2.6111, Acc_digits: 42/128 (33%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [4/25],Step [13/64],Loss: 2.7093, Acc_digits: 37/128 (29%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [4/25],Step [14/64],Loss: 2.8036, Acc_digits: 28/128 (22%) ,Acc_labels: 30/64 (47%)\n",
      "Epoch [4/25],Step [15/64],Loss: 2.6411, Acc_digits: 42/128 (33%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [16/40],Loss: 2.7206, Acc_digits: 20/80 (25%) ,Acc_labels: 24/40 (60%)\n",
      "Time needed to train  0.22273752302862704\n",
      "\n",
      "Test set: Loss: 2.6300,Acc_digits: 731/2000 (37%),Acc_labels: 595/1000 60%\n",
      "Epoch [5/25],Step [1/64],Loss: 2.7670, Acc_digits: 36/128 (28%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [5/25],Step [2/64],Loss: 2.7377, Acc_digits: 28/128 (22%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [3/64],Loss: 2.8045, Acc_digits: 30/128 (23%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [5/25],Step [4/64],Loss: 2.7438, Acc_digits: 36/128 (28%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [5/25],Step [5/64],Loss: 2.7913, Acc_digits: 29/128 (23%) ,Acc_labels: 30/64 (47%)\n",
      "Epoch [5/25],Step [6/64],Loss: 2.7924, Acc_digits: 28/128 (22%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [5/25],Step [7/64],Loss: 2.6946, Acc_digits: 41/128 (32%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [5/25],Step [8/64],Loss: 2.7186, Acc_digits: 30/128 (23%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [5/25],Step [9/64],Loss: 2.8018, Acc_digits: 25/128 (20%) ,Acc_labels: 29/64 (45%)\n",
      "Epoch [5/25],Step [10/64],Loss: 2.8787, Acc_digits: 24/128 (19%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [5/25],Step [11/64],Loss: 2.7763, Acc_digits: 38/128 (30%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [5/25],Step [12/64],Loss: 2.5514, Acc_digits: 35/128 (27%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [5/25],Step [13/64],Loss: 2.7945, Acc_digits: 34/128 (27%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [5/25],Step [14/64],Loss: 2.7701, Acc_digits: 30/128 (23%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [5/25],Step [15/64],Loss: 2.6412, Acc_digits: 38/128 (30%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [5/25],Step [16/40],Loss: 2.4827, Acc_digits: 25/80 (31%) ,Acc_labels: 25/40 (62%)\n",
      "Time needed to train  0.23667105997446924\n",
      "\n",
      "Test set: Loss: 2.5305,Acc_digits: 795/2000 (40%),Acc_labels: 607/1000 61%\n",
      "Epoch [6/25],Step [1/64],Loss: 2.6682, Acc_digits: 43/128 (34%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [6/25],Step [2/64],Loss: 2.6991, Acc_digits: 34/128 (27%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [6/25],Step [3/64],Loss: 2.6992, Acc_digits: 32/128 (25%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [6/25],Step [4/64],Loss: 2.6604, Acc_digits: 38/128 (30%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [6/25],Step [5/64],Loss: 2.6357, Acc_digits: 35/128 (27%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [6/25],Step [6/64],Loss: 2.6132, Acc_digits: 37/128 (29%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [6/25],Step [7/64],Loss: 2.6323, Acc_digits: 40/128 (31%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [6/25],Step [8/64],Loss: 2.7726, Acc_digits: 27/128 (21%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [6/25],Step [9/64],Loss: 2.7454, Acc_digits: 32/128 (25%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [6/25],Step [10/64],Loss: 2.7257, Acc_digits: 36/128 (28%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [6/25],Step [11/64],Loss: 2.6854, Acc_digits: 42/128 (33%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [6/25],Step [12/64],Loss: 2.5109, Acc_digits: 45/128 (35%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [6/25],Step [13/64],Loss: 2.7094, Acc_digits: 35/128 (27%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [6/25],Step [14/64],Loss: 2.7029, Acc_digits: 32/128 (25%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [6/25],Step [15/64],Loss: 2.4796, Acc_digits: 45/128 (35%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [6/25],Step [16/40],Loss: 2.4159, Acc_digits: 28/80 (35%) ,Acc_labels: 23/40 (57%)\n",
      "Time needed to train  0.2253375370055437\n",
      "\n",
      "Test set: Loss: 2.4382,Acc_digits: 849/2000 (42%),Acc_labels: 610/1000 61%\n",
      "Epoch [7/25],Step [1/64],Loss: 2.6184, Acc_digits: 39/128 (30%) ,Acc_labels: 30/64 (47%)\n",
      "Epoch [7/25],Step [2/64],Loss: 2.6630, Acc_digits: 39/128 (30%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [7/25],Step [3/64],Loss: 2.6479, Acc_digits: 39/128 (30%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [7/25],Step [4/64],Loss: 2.5425, Acc_digits: 44/128 (34%) ,Acc_labels: 37/64 (58%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25],Step [5/64],Loss: 2.6598, Acc_digits: 40/128 (31%) ,Acc_labels: 30/64 (47%)\n",
      "Epoch [7/25],Step [6/64],Loss: 2.6474, Acc_digits: 44/128 (34%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [7/25],Step [7/64],Loss: 2.5786, Acc_digits: 41/128 (32%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [7/25],Step [8/64],Loss: 2.6646, Acc_digits: 38/128 (30%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [7/25],Step [9/64],Loss: 2.6152, Acc_digits: 38/128 (30%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [7/25],Step [10/64],Loss: 2.7168, Acc_digits: 35/128 (27%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [7/25],Step [11/64],Loss: 2.5006, Acc_digits: 48/128 (38%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [7/25],Step [12/64],Loss: 2.5139, Acc_digits: 45/128 (35%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [7/25],Step [13/64],Loss: 2.6488, Acc_digits: 38/128 (30%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [7/25],Step [14/64],Loss: 2.5878, Acc_digits: 45/128 (35%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [7/25],Step [15/64],Loss: 2.4785, Acc_digits: 46/128 (36%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [7/25],Step [16/40],Loss: 2.4668, Acc_digits: 30/80 (38%) ,Acc_labels: 26/40 (65%)\n",
      "Time needed to train  0.23086643195711076\n",
      "\n",
      "Test set: Loss: 2.3757,Acc_digits: 926/2000 (46%),Acc_labels: 609/1000 61%\n",
      "Epoch [8/25],Step [1/64],Loss: 2.5415, Acc_digits: 45/128 (35%) ,Acc_labels: 31/64 (48%)\n",
      "Epoch [8/25],Step [2/64],Loss: 2.5918, Acc_digits: 37/128 (29%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [8/25],Step [3/64],Loss: 2.6187, Acc_digits: 44/128 (34%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [8/25],Step [4/64],Loss: 2.7305, Acc_digits: 35/128 (27%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [8/25],Step [5/64],Loss: 2.6208, Acc_digits: 45/128 (35%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [8/25],Step [6/64],Loss: 2.5436, Acc_digits: 45/128 (35%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [8/25],Step [7/64],Loss: 2.5610, Acc_digits: 41/128 (32%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [8/25],Step [8/64],Loss: 2.6620, Acc_digits: 35/128 (27%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [8/25],Step [9/64],Loss: 2.5558, Acc_digits: 43/128 (34%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [8/25],Step [10/64],Loss: 2.5714, Acc_digits: 41/128 (32%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [8/25],Step [11/64],Loss: 2.6073, Acc_digits: 39/128 (30%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [8/25],Step [12/64],Loss: 2.4556, Acc_digits: 47/128 (37%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [8/25],Step [13/64],Loss: 2.5541, Acc_digits: 42/128 (33%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [8/25],Step [14/64],Loss: 2.5699, Acc_digits: 42/128 (33%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [8/25],Step [15/64],Loss: 2.4023, Acc_digits: 54/128 (42%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [8/25],Step [16/40],Loss: 2.3158, Acc_digits: 31/80 (39%) ,Acc_labels: 26/40 (65%)\n",
      "Time needed to train  0.26772460399661213\n",
      "\n",
      "Test set: Loss: 2.2815,Acc_digits: 975/2000 (49%),Acc_labels: 614/1000 61%\n",
      "Epoch [9/25],Step [1/64],Loss: 2.6206, Acc_digits: 48/128 (38%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [9/25],Step [2/64],Loss: 2.5474, Acc_digits: 42/128 (33%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [9/25],Step [3/64],Loss: 2.4591, Acc_digits: 41/128 (32%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [9/25],Step [4/64],Loss: 2.4685, Acc_digits: 48/128 (38%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [9/25],Step [5/64],Loss: 2.4963, Acc_digits: 52/128 (41%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [9/25],Step [6/64],Loss: 2.5474, Acc_digits: 41/128 (32%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [9/25],Step [7/64],Loss: 2.4151, Acc_digits: 48/128 (38%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [9/25],Step [8/64],Loss: 2.5600, Acc_digits: 41/128 (32%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [9/25],Step [9/64],Loss: 2.4633, Acc_digits: 46/128 (36%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [9/25],Step [10/64],Loss: 2.5566, Acc_digits: 45/128 (35%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [9/25],Step [11/64],Loss: 2.3746, Acc_digits: 62/128 (48%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [9/25],Step [12/64],Loss: 2.3961, Acc_digits: 54/128 (42%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [9/25],Step [13/64],Loss: 2.5184, Acc_digits: 46/128 (36%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [9/25],Step [14/64],Loss: 2.4869, Acc_digits: 49/128 (38%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [9/25],Step [15/64],Loss: 2.2957, Acc_digits: 53/128 (41%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [9/25],Step [16/40],Loss: 2.4020, Acc_digits: 28/80 (35%) ,Acc_labels: 29/40 (72%)\n",
      "Time needed to train  0.2683468480827287\n",
      "\n",
      "Test set: Loss: 2.1988,Acc_digits: 1076/2000 (54%),Acc_labels: 626/1000 63%\n",
      "Epoch [10/25],Step [1/64],Loss: 2.4299, Acc_digits: 49/128 (38%) ,Acc_labels: 31/64 (48%)\n",
      "Epoch [10/25],Step [2/64],Loss: 2.5468, Acc_digits: 38/128 (30%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [10/25],Step [3/64],Loss: 2.4795, Acc_digits: 46/128 (36%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [10/25],Step [4/64],Loss: 2.3701, Acc_digits: 54/128 (42%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [10/25],Step [5/64],Loss: 2.4558, Acc_digits: 42/128 (33%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [10/25],Step [6/64],Loss: 2.5565, Acc_digits: 45/128 (35%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [10/25],Step [7/64],Loss: 2.3199, Acc_digits: 58/128 (45%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [10/25],Step [8/64],Loss: 2.3810, Acc_digits: 51/128 (40%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [10/25],Step [9/64],Loss: 2.4657, Acc_digits: 44/128 (34%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [10/25],Step [10/64],Loss: 2.4870, Acc_digits: 43/128 (34%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [10/25],Step [11/64],Loss: 2.3508, Acc_digits: 56/128 (44%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [10/25],Step [12/64],Loss: 2.2570, Acc_digits: 52/128 (41%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [10/25],Step [13/64],Loss: 2.3218, Acc_digits: 54/128 (42%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [10/25],Step [14/64],Loss: 2.5044, Acc_digits: 45/128 (35%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [10/25],Step [15/64],Loss: 2.2823, Acc_digits: 48/128 (38%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [10/25],Step [16/40],Loss: 2.2201, Acc_digits: 33/80 (41%) ,Acc_labels: 28/40 (70%)\n",
      "Time needed to train  0.2376759359613061\n",
      "\n",
      "Test set: Loss: 2.0929,Acc_digits: 1137/2000 (57%),Acc_labels: 651/1000 65%\n",
      "Epoch [11/25],Step [1/64],Loss: 2.3769, Acc_digits: 49/128 (38%) ,Acc_labels: 33/64 (52%)\n",
      "Epoch [11/25],Step [2/64],Loss: 2.5148, Acc_digits: 47/128 (37%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [11/25],Step [3/64],Loss: 2.5467, Acc_digits: 41/128 (32%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [11/25],Step [4/64],Loss: 2.3902, Acc_digits: 56/128 (44%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [11/25],Step [5/64],Loss: 2.3916, Acc_digits: 54/128 (42%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [11/25],Step [6/64],Loss: 2.4621, Acc_digits: 41/128 (32%) ,Acc_labels: 32/64 (50%)\n",
      "Epoch [11/25],Step [7/64],Loss: 2.3119, Acc_digits: 52/128 (41%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [11/25],Step [8/64],Loss: 2.4854, Acc_digits: 38/128 (30%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [11/25],Step [9/64],Loss: 2.4386, Acc_digits: 49/128 (38%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [11/25],Step [10/64],Loss: 2.5026, Acc_digits: 48/128 (38%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [11/25],Step [11/64],Loss: 2.3979, Acc_digits: 50/128 (39%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [11/25],Step [12/64],Loss: 2.2027, Acc_digits: 56/128 (44%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [11/25],Step [13/64],Loss: 2.4550, Acc_digits: 44/128 (34%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [11/25],Step [14/64],Loss: 2.3585, Acc_digits: 47/128 (37%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [11/25],Step [15/64],Loss: 2.2009, Acc_digits: 58/128 (45%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [11/25],Step [16/40],Loss: 2.2189, Acc_digits: 31/80 (39%) ,Acc_labels: 26/40 (65%)\n",
      "Time needed to train  0.24908393493387848\n",
      "\n",
      "Test set: Loss: 2.0210,Acc_digits: 1190/2000 (60%),Acc_labels: 659/1000 66%\n",
      "Epoch [12/25],Step [1/64],Loss: 2.3420, Acc_digits: 53/128 (41%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [12/25],Step [2/64],Loss: 2.4918, Acc_digits: 42/128 (33%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [12/25],Step [3/64],Loss: 2.6004, Acc_digits: 39/128 (30%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [12/25],Step [4/64],Loss: 2.2873, Acc_digits: 53/128 (41%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [12/25],Step [5/64],Loss: 2.3943, Acc_digits: 45/128 (35%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [12/25],Step [6/64],Loss: 2.3511, Acc_digits: 47/128 (37%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [12/25],Step [7/64],Loss: 2.2972, Acc_digits: 47/128 (37%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [12/25],Step [8/64],Loss: 2.3832, Acc_digits: 46/128 (36%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [12/25],Step [9/64],Loss: 2.3522, Acc_digits: 47/128 (37%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [12/25],Step [10/64],Loss: 2.4419, Acc_digits: 49/128 (38%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [12/25],Step [11/64],Loss: 2.2158, Acc_digits: 56/128 (44%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [12/25],Step [12/64],Loss: 2.1132, Acc_digits: 56/128 (44%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [12/25],Step [13/64],Loss: 2.3000, Acc_digits: 50/128 (39%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [12/25],Step [14/64],Loss: 2.2959, Acc_digits: 53/128 (41%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [12/25],Step [15/64],Loss: 2.1407, Acc_digits: 62/128 (48%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [12/25],Step [16/40],Loss: 2.1884, Acc_digits: 33/80 (41%) ,Acc_labels: 30/40 (75%)\n",
      "Time needed to train  0.22968355496414006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.8918,Acc_digits: 1303/2000 (65%),Acc_labels: 657/1000 66%\n",
      "Epoch [13/25],Step [1/64],Loss: 2.3798, Acc_digits: 50/128 (39%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [13/25],Step [2/64],Loss: 2.3555, Acc_digits: 48/128 (38%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [13/25],Step [3/64],Loss: 2.3749, Acc_digits: 49/128 (38%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [13/25],Step [4/64],Loss: 2.2701, Acc_digits: 54/128 (42%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [13/25],Step [5/64],Loss: 2.4479, Acc_digits: 46/128 (36%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [13/25],Step [6/64],Loss: 2.3777, Acc_digits: 50/128 (39%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [13/25],Step [7/64],Loss: 2.2476, Acc_digits: 55/128 (43%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [13/25],Step [8/64],Loss: 2.2930, Acc_digits: 49/128 (38%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [13/25],Step [9/64],Loss: 2.4397, Acc_digits: 41/128 (32%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [13/25],Step [10/64],Loss: 2.2923, Acc_digits: 50/128 (39%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [13/25],Step [11/64],Loss: 2.1430, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [13/25],Step [12/64],Loss: 2.0427, Acc_digits: 61/128 (48%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [13/25],Step [13/64],Loss: 2.3666, Acc_digits: 48/128 (38%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [13/25],Step [14/64],Loss: 2.2180, Acc_digits: 51/128 (40%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [13/25],Step [15/64],Loss: 2.1493, Acc_digits: 55/128 (43%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [13/25],Step [16/40],Loss: 2.1529, Acc_digits: 34/80 (42%) ,Acc_labels: 29/40 (72%)\n",
      "Time needed to train  0.24037670600228012\n",
      "\n",
      "Test set: Loss: 1.8209,Acc_digits: 1364/2000 (68%),Acc_labels: 666/1000 67%\n",
      "Epoch [14/25],Step [1/64],Loss: 2.0973, Acc_digits: 60/128 (47%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [14/25],Step [2/64],Loss: 2.4326, Acc_digits: 47/128 (37%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [14/25],Step [3/64],Loss: 2.2395, Acc_digits: 53/128 (41%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [14/25],Step [4/64],Loss: 2.2885, Acc_digits: 55/128 (43%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [14/25],Step [5/64],Loss: 2.2404, Acc_digits: 59/128 (46%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [14/25],Step [6/64],Loss: 2.2864, Acc_digits: 58/128 (45%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [14/25],Step [7/64],Loss: 2.1958, Acc_digits: 59/128 (46%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [14/25],Step [8/64],Loss: 2.3031, Acc_digits: 54/128 (42%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [14/25],Step [9/64],Loss: 2.2647, Acc_digits: 50/128 (39%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [14/25],Step [10/64],Loss: 2.2481, Acc_digits: 57/128 (45%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [14/25],Step [11/64],Loss: 2.0840, Acc_digits: 63/128 (49%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [14/25],Step [12/64],Loss: 2.1253, Acc_digits: 56/128 (44%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [14/25],Step [13/64],Loss: 2.2470, Acc_digits: 51/128 (40%) ,Acc_labels: 35/64 (55%)\n",
      "Epoch [14/25],Step [14/64],Loss: 2.0256, Acc_digits: 62/128 (48%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [14/25],Step [15/64],Loss: 2.0020, Acc_digits: 64/128 (50%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [14/25],Step [16/40],Loss: 2.0643, Acc_digits: 41/80 (51%) ,Acc_labels: 28/40 (70%)\n",
      "Time needed to train  0.24412174697499722\n",
      "\n",
      "Test set: Loss: 1.7706,Acc_digits: 1363/2000 (68%),Acc_labels: 676/1000 68%\n",
      "Epoch [15/25],Step [1/64],Loss: 2.2557, Acc_digits: 51/128 (40%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [15/25],Step [2/64],Loss: 2.1904, Acc_digits: 59/128 (46%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [15/25],Step [3/64],Loss: 2.1871, Acc_digits: 51/128 (40%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [15/25],Step [4/64],Loss: 2.1869, Acc_digits: 60/128 (47%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [15/25],Step [5/64],Loss: 2.1521, Acc_digits: 60/128 (47%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [15/25],Step [6/64],Loss: 2.1671, Acc_digits: 55/128 (43%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [15/25],Step [7/64],Loss: 2.1412, Acc_digits: 55/128 (43%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [15/25],Step [8/64],Loss: 2.3005, Acc_digits: 51/128 (40%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [15/25],Step [9/64],Loss: 2.2811, Acc_digits: 53/128 (41%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [15/25],Step [10/64],Loss: 2.3584, Acc_digits: 48/128 (38%) ,Acc_labels: 34/64 (53%)\n",
      "Epoch [15/25],Step [11/64],Loss: 2.0404, Acc_digits: 65/128 (51%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [15/25],Step [12/64],Loss: 1.8444, Acc_digits: 70/128 (55%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [15/25],Step [13/64],Loss: 2.4392, Acc_digits: 45/128 (35%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [15/25],Step [14/64],Loss: 2.1116, Acc_digits: 53/128 (41%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [15/25],Step [15/64],Loss: 2.0641, Acc_digits: 63/128 (49%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [15/25],Step [16/40],Loss: 1.9489, Acc_digits: 42/80 (52%) ,Acc_labels: 28/40 (70%)\n",
      "Time needed to train  0.25165476906113327\n",
      "\n",
      "Test set: Loss: 1.7026,Acc_digits: 1464/2000 (73%),Acc_labels: 682/1000 68%\n",
      "Epoch [16/25],Step [1/64],Loss: 1.9776, Acc_digits: 65/128 (51%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [16/25],Step [2/64],Loss: 2.2474, Acc_digits: 55/128 (43%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [16/25],Step [3/64],Loss: 2.3147, Acc_digits: 53/128 (41%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [16/25],Step [4/64],Loss: 2.1629, Acc_digits: 61/128 (48%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [16/25],Step [5/64],Loss: 2.1610, Acc_digits: 60/128 (47%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [16/25],Step [6/64],Loss: 2.1988, Acc_digits: 50/128 (39%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [16/25],Step [7/64],Loss: 2.1171, Acc_digits: 60/128 (47%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [16/25],Step [8/64],Loss: 2.0826, Acc_digits: 57/128 (45%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [16/25],Step [9/64],Loss: 2.1862, Acc_digits: 51/128 (40%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [16/25],Step [10/64],Loss: 2.1153, Acc_digits: 58/128 (45%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [16/25],Step [11/64],Loss: 2.0398, Acc_digits: 68/128 (53%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [16/25],Step [12/64],Loss: 1.9182, Acc_digits: 64/128 (50%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [16/25],Step [13/64],Loss: 2.2323, Acc_digits: 58/128 (45%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [16/25],Step [14/64],Loss: 2.0695, Acc_digits: 58/128 (45%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [16/25],Step [15/64],Loss: 2.0178, Acc_digits: 61/128 (48%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [16/25],Step [16/40],Loss: 1.8221, Acc_digits: 48/80 (60%) ,Acc_labels: 28/40 (70%)\n",
      "Time needed to train  0.22487665992230177\n",
      "\n",
      "Test set: Loss: 1.5992,Acc_digits: 1487/2000 (74%),Acc_labels: 676/1000 68%\n",
      "Epoch [17/25],Step [1/64],Loss: 1.9476, Acc_digits: 70/128 (55%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [17/25],Step [2/64],Loss: 1.9910, Acc_digits: 62/128 (48%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [17/25],Step [3/64],Loss: 2.1303, Acc_digits: 50/128 (39%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [17/25],Step [4/64],Loss: 2.1400, Acc_digits: 55/128 (43%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [17/25],Step [5/64],Loss: 1.9897, Acc_digits: 62/128 (48%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [17/25],Step [6/64],Loss: 2.1522, Acc_digits: 64/128 (50%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [17/25],Step [7/64],Loss: 1.9013, Acc_digits: 66/128 (52%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [8/64],Loss: 2.0946, Acc_digits: 53/128 (41%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [17/25],Step [9/64],Loss: 2.1527, Acc_digits: 60/128 (47%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [17/25],Step [10/64],Loss: 2.1704, Acc_digits: 56/128 (44%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [17/25],Step [11/64],Loss: 1.9812, Acc_digits: 65/128 (51%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [17/25],Step [12/64],Loss: 1.9397, Acc_digits: 62/128 (48%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [13/64],Loss: 2.2327, Acc_digits: 55/128 (43%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [17/25],Step [14/64],Loss: 1.9398, Acc_digits: 61/128 (48%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [17/25],Step [15/64],Loss: 1.9320, Acc_digits: 69/128 (54%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [17/25],Step [16/40],Loss: 1.8747, Acc_digits: 34/80 (42%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.2796650360105559\n",
      "\n",
      "Test set: Loss: 1.5780,Acc_digits: 1500/2000 (75%),Acc_labels: 694/1000 69%\n",
      "Epoch [18/25],Step [1/64],Loss: 2.0607, Acc_digits: 56/128 (44%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [18/25],Step [2/64],Loss: 2.1422, Acc_digits: 62/128 (48%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [18/25],Step [3/64],Loss: 1.9988, Acc_digits: 57/128 (45%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [18/25],Step [4/64],Loss: 1.9928, Acc_digits: 65/128 (51%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [18/25],Step [5/64],Loss: 1.9504, Acc_digits: 66/128 (52%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [18/25],Step [6/64],Loss: 1.9676, Acc_digits: 70/128 (55%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [18/25],Step [7/64],Loss: 2.0114, Acc_digits: 67/128 (52%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [18/25],Step [8/64],Loss: 2.0559, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [18/25],Step [9/64],Loss: 1.9317, Acc_digits: 70/128 (55%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [18/25],Step [10/64],Loss: 2.1217, Acc_digits: 63/128 (49%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [18/25],Step [11/64],Loss: 1.8784, Acc_digits: 64/128 (50%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [18/25],Step [12/64],Loss: 1.7241, Acc_digits: 70/128 (55%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [13/64],Loss: 2.2052, Acc_digits: 54/128 (42%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [18/25],Step [14/64],Loss: 2.1217, Acc_digits: 56/128 (44%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [18/25],Step [15/64],Loss: 2.0705, Acc_digits: 58/128 (45%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [18/25],Step [16/40],Loss: 1.8904, Acc_digits: 44/80 (55%) ,Acc_labels: 30/40 (75%)\n",
      "Time needed to train  0.23788811301346868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.5662,Acc_digits: 1493/2000 (75%),Acc_labels: 692/1000 69%\n",
      "Epoch [19/25],Step [1/64],Loss: 1.9785, Acc_digits: 60/128 (47%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [19/25],Step [2/64],Loss: 2.0319, Acc_digits: 63/128 (49%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [19/25],Step [3/64],Loss: 1.9979, Acc_digits: 60/128 (47%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [19/25],Step [4/64],Loss: 1.9976, Acc_digits: 68/128 (53%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [19/25],Step [5/64],Loss: 1.9650, Acc_digits: 62/128 (48%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [19/25],Step [6/64],Loss: 1.9988, Acc_digits: 69/128 (54%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [19/25],Step [7/64],Loss: 1.9021, Acc_digits: 60/128 (47%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [8/64],Loss: 2.1068, Acc_digits: 58/128 (45%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [19/25],Step [9/64],Loss: 1.9065, Acc_digits: 60/128 (47%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [10/64],Loss: 2.2053, Acc_digits: 57/128 (45%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [19/25],Step [11/64],Loss: 1.9878, Acc_digits: 62/128 (48%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [19/25],Step [12/64],Loss: 1.7193, Acc_digits: 74/128 (58%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [19/25],Step [13/64],Loss: 2.0779, Acc_digits: 59/128 (46%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [19/25],Step [14/64],Loss: 1.9036, Acc_digits: 66/128 (52%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [19/25],Step [15/64],Loss: 1.8554, Acc_digits: 62/128 (48%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [16/40],Loss: 1.9837, Acc_digits: 42/80 (52%) ,Acc_labels: 30/40 (75%)\n",
      "Time needed to train  0.24435330601409078\n",
      "\n",
      "Test set: Loss: 1.4834,Acc_digits: 1526/2000 (76%),Acc_labels: 709/1000 71%\n",
      "Epoch [20/25],Step [1/64],Loss: 2.0096, Acc_digits: 70/128 (55%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [20/25],Step [2/64],Loss: 2.1967, Acc_digits: 48/128 (38%) ,Acc_labels: 37/64 (58%)\n",
      "Epoch [20/25],Step [3/64],Loss: 1.8667, Acc_digits: 67/128 (52%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [4/64],Loss: 2.0624, Acc_digits: 65/128 (51%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [20/25],Step [5/64],Loss: 2.0940, Acc_digits: 63/128 (49%) ,Acc_labels: 36/64 (56%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.9884, Acc_digits: 64/128 (50%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [20/25],Step [7/64],Loss: 1.8394, Acc_digits: 72/128 (56%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [20/25],Step [8/64],Loss: 1.9927, Acc_digits: 61/128 (48%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [20/25],Step [9/64],Loss: 1.9248, Acc_digits: 67/128 (52%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [10/64],Loss: 2.0443, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [20/25],Step [11/64],Loss: 1.8412, Acc_digits: 68/128 (53%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [12/64],Loss: 1.6814, Acc_digits: 71/128 (55%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [13/64],Loss: 1.9796, Acc_digits: 66/128 (52%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [20/25],Step [14/64],Loss: 1.8606, Acc_digits: 73/128 (57%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [20/25],Step [15/64],Loss: 1.8254, Acc_digits: 71/128 (55%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [16/40],Loss: 1.8240, Acc_digits: 43/80 (54%) ,Acc_labels: 28/40 (70%)\n",
      "Time needed to train  0.2822760280687362\n",
      "\n",
      "Test set: Loss: 1.4404,Acc_digits: 1542/2000 (77%),Acc_labels: 724/1000 72%\n",
      "Epoch [21/25],Step [1/64],Loss: 2.1341, Acc_digits: 65/128 (51%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [21/25],Step [2/64],Loss: 2.0694, Acc_digits: 64/128 (50%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [21/25],Step [3/64],Loss: 1.8630, Acc_digits: 65/128 (51%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [21/25],Step [4/64],Loss: 2.0585, Acc_digits: 65/128 (51%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.8881, Acc_digits: 71/128 (55%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [21/25],Step [6/64],Loss: 2.0249, Acc_digits: 60/128 (47%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [21/25],Step [7/64],Loss: 1.8135, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [21/25],Step [8/64],Loss: 1.9358, Acc_digits: 65/128 (51%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [21/25],Step [9/64],Loss: 1.7744, Acc_digits: 72/128 (56%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [21/25],Step [10/64],Loss: 1.7944, Acc_digits: 71/128 (55%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [21/25],Step [11/64],Loss: 2.0270, Acc_digits: 67/128 (52%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [21/25],Step [12/64],Loss: 1.6485, Acc_digits: 67/128 (52%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [21/25],Step [13/64],Loss: 1.9132, Acc_digits: 64/128 (50%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [21/25],Step [14/64],Loss: 1.8997, Acc_digits: 64/128 (50%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [21/25],Step [15/64],Loss: 1.7344, Acc_digits: 77/128 (60%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [21/25],Step [16/40],Loss: 1.6021, Acc_digits: 46/80 (57%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.3116218450013548\n",
      "\n",
      "Test set: Loss: 1.3864,Acc_digits: 1546/2000 (77%),Acc_labels: 734/1000 73%\n",
      "Epoch [22/25],Step [1/64],Loss: 1.9306, Acc_digits: 61/128 (48%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [2/64],Loss: 2.0099, Acc_digits: 65/128 (51%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [22/25],Step [3/64],Loss: 2.0153, Acc_digits: 56/128 (44%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [22/25],Step [4/64],Loss: 2.0643, Acc_digits: 59/128 (46%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [22/25],Step [5/64],Loss: 1.6967, Acc_digits: 76/128 (59%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [22/25],Step [6/64],Loss: 2.0658, Acc_digits: 64/128 (50%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [22/25],Step [7/64],Loss: 1.8265, Acc_digits: 63/128 (49%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [8/64],Loss: 1.8770, Acc_digits: 66/128 (52%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [22/25],Step [9/64],Loss: 1.8695, Acc_digits: 66/128 (52%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [22/25],Step [10/64],Loss: 1.7997, Acc_digits: 70/128 (55%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [22/25],Step [11/64],Loss: 1.8196, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [22/25],Step [12/64],Loss: 1.5615, Acc_digits: 76/128 (59%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [22/25],Step [13/64],Loss: 1.9684, Acc_digits: 64/128 (50%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [22/25],Step [14/64],Loss: 1.9356, Acc_digits: 57/128 (45%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [22/25],Step [15/64],Loss: 1.7504, Acc_digits: 69/128 (54%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [22/25],Step [16/40],Loss: 1.6493, Acc_digits: 45/80 (56%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.24592677596956491\n",
      "\n",
      "Test set: Loss: 1.3385,Acc_digits: 1539/2000 (77%),Acc_labels: 733/1000 73%\n",
      "Epoch [23/25],Step [1/64],Loss: 1.8398, Acc_digits: 69/128 (54%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [23/25],Step [2/64],Loss: 2.1011, Acc_digits: 61/128 (48%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [23/25],Step [3/64],Loss: 1.9708, Acc_digits: 56/128 (44%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [23/25],Step [4/64],Loss: 1.9182, Acc_digits: 71/128 (55%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.8327, Acc_digits: 71/128 (55%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [23/25],Step [6/64],Loss: 2.1746, Acc_digits: 64/128 (50%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [23/25],Step [7/64],Loss: 1.7460, Acc_digits: 78/128 (61%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [23/25],Step [8/64],Loss: 1.9168, Acc_digits: 67/128 (52%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [23/25],Step [9/64],Loss: 1.8309, Acc_digits: 66/128 (52%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [23/25],Step [10/64],Loss: 1.9269, Acc_digits: 71/128 (55%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [23/25],Step [11/64],Loss: 1.7563, Acc_digits: 73/128 (57%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [23/25],Step [12/64],Loss: 1.5008, Acc_digits: 84/128 (66%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [13/64],Loss: 1.9762, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [23/25],Step [14/64],Loss: 1.6527, Acc_digits: 76/128 (59%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [15/64],Loss: 1.7472, Acc_digits: 70/128 (55%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [16/40],Loss: 1.4792, Acc_digits: 55/80 (69%) ,Acc_labels: 30/40 (75%)\n",
      "Time needed to train  0.22781428799498826\n",
      "\n",
      "Test set: Loss: 1.2973,Acc_digits: 1537/2000 (77%),Acc_labels: 736/1000 74%\n",
      "Epoch [24/25],Step [1/64],Loss: 1.9324, Acc_digits: 73/128 (57%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [24/25],Step [2/64],Loss: 1.8585, Acc_digits: 70/128 (55%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [24/25],Step [3/64],Loss: 1.8553, Acc_digits: 68/128 (53%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [24/25],Step [4/64],Loss: 1.8081, Acc_digits: 78/128 (61%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [24/25],Step [5/64],Loss: 1.8211, Acc_digits: 71/128 (55%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [24/25],Step [6/64],Loss: 1.8779, Acc_digits: 65/128 (51%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [24/25],Step [7/64],Loss: 1.8755, Acc_digits: 75/128 (59%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [24/25],Step [8/64],Loss: 1.7480, Acc_digits: 66/128 (52%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [9/64],Loss: 1.6222, Acc_digits: 75/128 (59%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [24/25],Step [10/64],Loss: 1.8104, Acc_digits: 67/128 (52%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [11/64],Loss: 1.7001, Acc_digits: 75/128 (59%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [24/25],Step [12/64],Loss: 1.5728, Acc_digits: 81/128 (63%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [13/64],Loss: 1.7476, Acc_digits: 81/128 (63%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [24/25],Step [14/64],Loss: 1.7271, Acc_digits: 66/128 (52%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [24/25],Step [15/64],Loss: 1.5820, Acc_digits: 80/128 (62%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [24/25],Step [16/40],Loss: 1.4465, Acc_digits: 52/80 (65%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.22815999505110085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.2605,Acc_digits: 1561/2000 (78%),Acc_labels: 736/1000 74%\n",
      "Epoch [25/25],Step [1/64],Loss: 1.8164, Acc_digits: 77/128 (60%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [25/25],Step [2/64],Loss: 2.0353, Acc_digits: 71/128 (55%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [25/25],Step [3/64],Loss: 1.8768, Acc_digits: 72/128 (56%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [25/25],Step [4/64],Loss: 1.8918, Acc_digits: 75/128 (59%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [25/25],Step [5/64],Loss: 1.7679, Acc_digits: 70/128 (55%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.8755, Acc_digits: 64/128 (50%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [25/25],Step [7/64],Loss: 1.8044, Acc_digits: 73/128 (57%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.8199, Acc_digits: 67/128 (52%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [25/25],Step [9/64],Loss: 1.8027, Acc_digits: 70/128 (55%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [10/64],Loss: 1.8437, Acc_digits: 72/128 (56%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [25/25],Step [11/64],Loss: 1.8299, Acc_digits: 71/128 (55%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [25/25],Step [12/64],Loss: 1.4933, Acc_digits: 76/128 (59%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [13/64],Loss: 1.9011, Acc_digits: 69/128 (54%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [25/25],Step [14/64],Loss: 1.5532, Acc_digits: 81/128 (63%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [15/64],Loss: 1.7442, Acc_digits: 76/128 (59%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [25/25],Step [16/40],Loss: 1.6730, Acc_digits: 44/80 (55%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.21925816498696804\n",
      "\n",
      "Test set: Loss: 1.2410,Acc_digits: 1600/2000 (80%),Acc_labels: 760/1000 76%\n",
      "Epoch [26/25],Step [1/64],Loss: 1.6351, Acc_digits: 86/128 (67%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [26/25],Step [2/64],Loss: 1.9427, Acc_digits: 65/128 (51%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [26/25],Step [3/64],Loss: 1.9498, Acc_digits: 70/128 (55%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [26/25],Step [4/64],Loss: 1.9386, Acc_digits: 76/128 (59%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.9509, Acc_digits: 68/128 (53%) ,Acc_labels: 39/64 (61%)\n",
      "Epoch [26/25],Step [6/64],Loss: 1.9583, Acc_digits: 61/128 (48%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [26/25],Step [7/64],Loss: 1.8831, Acc_digits: 73/128 (57%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [26/25],Step [8/64],Loss: 1.7337, Acc_digits: 72/128 (56%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [26/25],Step [9/64],Loss: 1.7374, Acc_digits: 74/128 (58%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [10/64],Loss: 1.7300, Acc_digits: 76/128 (59%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [26/25],Step [11/64],Loss: 1.8288, Acc_digits: 69/128 (54%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [26/25],Step [12/64],Loss: 1.6067, Acc_digits: 70/128 (55%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [13/64],Loss: 1.7501, Acc_digits: 77/128 (60%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [26/25],Step [14/64],Loss: 1.6541, Acc_digits: 75/128 (59%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [15/64],Loss: 1.5466, Acc_digits: 90/128 (70%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [26/25],Step [16/40],Loss: 1.5730, Acc_digits: 53/80 (66%) ,Acc_labels: 24/40 (60%)\n",
      "Time needed to train  0.22835800994653255\n",
      "\n",
      "Test set: Loss: 1.2059,Acc_digits: 1588/2000 (79%),Acc_labels: 747/1000 75%\n",
      "Epoch [2/25],Step [1/64],Loss: 1.7672, Acc_digits: 81/128 (63%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [2/25],Step [2/64],Loss: 2.1034, Acc_digits: 54/128 (42%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [2/25],Step [3/64],Loss: 1.9018, Acc_digits: 62/128 (48%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [4/64],Loss: 1.6016, Acc_digits: 82/128 (64%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [2/25],Step [5/64],Loss: 1.6510, Acc_digits: 76/128 (59%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [2/25],Step [6/64],Loss: 1.8918, Acc_digits: 69/128 (54%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [2/25],Step [7/64],Loss: 1.6095, Acc_digits: 76/128 (59%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [2/25],Step [8/64],Loss: 1.8086, Acc_digits: 71/128 (55%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [9/64],Loss: 1.4521, Acc_digits: 79/128 (62%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [2/25],Step [10/64],Loss: 1.6462, Acc_digits: 71/128 (55%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [11/64],Loss: 1.6706, Acc_digits: 79/128 (62%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [12/64],Loss: 1.5855, Acc_digits: 78/128 (61%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [2/25],Step [13/64],Loss: 1.7297, Acc_digits: 78/128 (61%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [2/25],Step [14/64],Loss: 1.5102, Acc_digits: 81/128 (63%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [2/25],Step [15/64],Loss: 1.5519, Acc_digits: 83/128 (65%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [2/25],Step [16/40],Loss: 1.3378, Acc_digits: 53/80 (66%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.22789138101506978\n",
      "\n",
      "Test set: Loss: 1.1708,Acc_digits: 1573/2000 (79%),Acc_labels: 762/1000 76%\n",
      "Epoch [3/25],Step [1/64],Loss: 1.7475, Acc_digits: 78/128 (61%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [3/25],Step [2/64],Loss: 1.8897, Acc_digits: 69/128 (54%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [3/25],Step [3/64],Loss: 1.8841, Acc_digits: 69/128 (54%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [4/64],Loss: 1.7879, Acc_digits: 71/128 (55%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [3/25],Step [5/64],Loss: 1.8952, Acc_digits: 74/128 (58%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [3/25],Step [6/64],Loss: 1.9059, Acc_digits: 70/128 (55%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [3/25],Step [7/64],Loss: 1.6604, Acc_digits: 78/128 (61%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [3/25],Step [8/64],Loss: 1.6428, Acc_digits: 78/128 (61%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [3/25],Step [9/64],Loss: 1.7295, Acc_digits: 72/128 (56%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [10/64],Loss: 1.8544, Acc_digits: 76/128 (59%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [3/25],Step [11/64],Loss: 1.6290, Acc_digits: 77/128 (60%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [12/64],Loss: 1.5126, Acc_digits: 76/128 (59%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [3/25],Step [13/64],Loss: 1.6766, Acc_digits: 76/128 (59%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [3/25],Step [14/64],Loss: 1.6699, Acc_digits: 71/128 (55%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [3/25],Step [15/64],Loss: 1.5989, Acc_digits: 89/128 (70%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [3/25],Step [16/40],Loss: 1.2898, Acc_digits: 53/80 (66%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.23050750500988215\n",
      "\n",
      "Test set: Loss: 1.1632,Acc_digits: 1603/2000 (80%),Acc_labels: 758/1000 76%\n",
      "Epoch [4/25],Step [1/64],Loss: 1.7831, Acc_digits: 79/128 (62%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [2/64],Loss: 2.0530, Acc_digits: 72/128 (56%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [3/64],Loss: 1.7622, Acc_digits: 76/128 (59%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [4/64],Loss: 1.6435, Acc_digits: 81/128 (63%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [5/64],Loss: 1.5490, Acc_digits: 81/128 (63%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [6/64],Loss: 2.0203, Acc_digits: 67/128 (52%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [4/25],Step [7/64],Loss: 1.5459, Acc_digits: 80/128 (62%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.7699, Acc_digits: 67/128 (52%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [4/25],Step [9/64],Loss: 1.5338, Acc_digits: 77/128 (60%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [10/64],Loss: 1.8073, Acc_digits: 72/128 (56%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [4/25],Step [11/64],Loss: 1.6464, Acc_digits: 72/128 (56%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [12/64],Loss: 1.4193, Acc_digits: 78/128 (61%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [13/64],Loss: 1.5357, Acc_digits: 85/128 (66%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [4/25],Step [14/64],Loss: 1.6242, Acc_digits: 75/128 (59%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [4/25],Step [15/64],Loss: 1.6407, Acc_digits: 86/128 (67%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [4/25],Step [16/40],Loss: 1.8579, Acc_digits: 50/80 (62%) ,Acc_labels: 30/40 (75%)\n",
      "Time needed to train  0.22855827992316335\n",
      "\n",
      "Test set: Loss: 1.1745,Acc_digits: 1603/2000 (80%),Acc_labels: 759/1000 76%\n",
      "Epoch [5/25],Step [1/64],Loss: 1.5819, Acc_digits: 83/128 (65%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [5/25],Step [2/64],Loss: 1.7953, Acc_digits: 78/128 (61%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [5/25],Step [3/64],Loss: 1.7648, Acc_digits: 75/128 (59%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [4/64],Loss: 1.7250, Acc_digits: 83/128 (65%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [5/25],Step [5/64],Loss: 1.7721, Acc_digits: 75/128 (59%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [6/64],Loss: 1.7146, Acc_digits: 75/128 (59%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [7/64],Loss: 1.6668, Acc_digits: 79/128 (62%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [5/25],Step [8/64],Loss: 1.6405, Acc_digits: 78/128 (61%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [9/64],Loss: 1.5646, Acc_digits: 78/128 (61%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [10/64],Loss: 1.5923, Acc_digits: 77/128 (60%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [11/64],Loss: 1.6610, Acc_digits: 76/128 (59%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [12/64],Loss: 1.2915, Acc_digits: 86/128 (67%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [13/64],Loss: 1.6929, Acc_digits: 78/128 (61%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [14/64],Loss: 1.5361, Acc_digits: 80/128 (62%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [15/64],Loss: 1.5356, Acc_digits: 80/128 (62%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [16/40],Loss: 1.2161, Acc_digits: 58/80 (72%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.2225508359260857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.0921,Acc_digits: 1640/2000 (82%),Acc_labels: 766/1000 77%\n",
      "Epoch [6/25],Step [1/64],Loss: 1.7243, Acc_digits: 78/128 (61%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [6/25],Step [2/64],Loss: 1.6642, Acc_digits: 73/128 (57%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [6/25],Step [3/64],Loss: 1.6463, Acc_digits: 76/128 (59%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [4/64],Loss: 1.6733, Acc_digits: 80/128 (62%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [6/25],Step [5/64],Loss: 1.7233, Acc_digits: 75/128 (59%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [6/25],Step [6/64],Loss: 1.6802, Acc_digits: 78/128 (61%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [6/25],Step [7/64],Loss: 1.8098, Acc_digits: 74/128 (58%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.4419, Acc_digits: 81/128 (63%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [9/64],Loss: 1.5471, Acc_digits: 82/128 (64%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [10/64],Loss: 1.6748, Acc_digits: 75/128 (59%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [11/64],Loss: 1.6426, Acc_digits: 74/128 (58%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [6/25],Step [12/64],Loss: 1.2818, Acc_digits: 89/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [13/64],Loss: 1.9497, Acc_digits: 69/128 (54%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [6/25],Step [14/64],Loss: 1.9201, Acc_digits: 75/128 (59%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [6/25],Step [15/64],Loss: 1.5715, Acc_digits: 75/128 (59%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [16/40],Loss: 1.3158, Acc_digits: 57/80 (71%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.23104305297601968\n",
      "\n",
      "Test set: Loss: 1.1215,Acc_digits: 1669/2000 (83%),Acc_labels: 761/1000 76%\n",
      "Epoch [7/25],Step [1/64],Loss: 1.5890, Acc_digits: 73/128 (57%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [2/64],Loss: 1.9652, Acc_digits: 67/128 (52%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [7/25],Step [3/64],Loss: 1.6814, Acc_digits: 73/128 (57%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [4/64],Loss: 1.5400, Acc_digits: 80/128 (62%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [7/25],Step [5/64],Loss: 1.7494, Acc_digits: 75/128 (59%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [7/25],Step [6/64],Loss: 1.5847, Acc_digits: 80/128 (62%) ,Acc_labels: 40/64 (62%)\n",
      "Epoch [7/25],Step [7/64],Loss: 1.7770, Acc_digits: 76/128 (59%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [7/25],Step [8/64],Loss: 1.5216, Acc_digits: 80/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [9/64],Loss: 1.6351, Acc_digits: 82/128 (64%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [10/64],Loss: 1.7775, Acc_digits: 66/128 (52%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [11/64],Loss: 1.6242, Acc_digits: 78/128 (61%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [12/64],Loss: 1.4624, Acc_digits: 85/128 (66%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [13/64],Loss: 1.6389, Acc_digits: 78/128 (61%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [14/64],Loss: 1.4852, Acc_digits: 85/128 (66%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [7/25],Step [15/64],Loss: 1.5568, Acc_digits: 86/128 (67%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [16/40],Loss: 1.2793, Acc_digits: 62/80 (78%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2509134670253843\n",
      "\n",
      "Test set: Loss: 1.1074,Acc_digits: 1673/2000 (84%),Acc_labels: 763/1000 76%\n",
      "Epoch [8/25],Step [1/64],Loss: 1.6620, Acc_digits: 84/128 (66%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [2/64],Loss: 1.6465, Acc_digits: 81/128 (63%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [8/25],Step [3/64],Loss: 1.7340, Acc_digits: 75/128 (59%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [4/64],Loss: 1.5195, Acc_digits: 82/128 (64%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [5/64],Loss: 1.5966, Acc_digits: 81/128 (63%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [8/25],Step [6/64],Loss: 1.6399, Acc_digits: 78/128 (61%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [7/64],Loss: 1.5131, Acc_digits: 89/128 (70%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [8/64],Loss: 1.5613, Acc_digits: 72/128 (56%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [9/64],Loss: 1.4663, Acc_digits: 86/128 (67%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [10/64],Loss: 1.5146, Acc_digits: 84/128 (66%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [11/64],Loss: 1.6643, Acc_digits: 81/128 (63%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [12/64],Loss: 1.3199, Acc_digits: 92/128 (72%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [13/64],Loss: 1.6655, Acc_digits: 79/128 (62%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [8/25],Step [14/64],Loss: 1.6324, Acc_digits: 83/128 (65%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [8/25],Step [15/64],Loss: 1.4903, Acc_digits: 87/128 (68%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [16/40],Loss: 1.1530, Acc_digits: 55/80 (69%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2275944029679522\n",
      "\n",
      "Test set: Loss: 1.0888,Acc_digits: 1654/2000 (83%),Acc_labels: 761/1000 76%\n",
      "Epoch [9/25],Step [1/64],Loss: 1.7031, Acc_digits: 77/128 (60%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [9/25],Step [2/64],Loss: 1.6102, Acc_digits: 76/128 (59%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [3/64],Loss: 1.6322, Acc_digits: 72/128 (56%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [9/25],Step [4/64],Loss: 1.4767, Acc_digits: 89/128 (70%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [9/25],Step [5/64],Loss: 1.4219, Acc_digits: 86/128 (67%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [6/64],Loss: 1.6266, Acc_digits: 71/128 (55%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [7/64],Loss: 1.7540, Acc_digits: 75/128 (59%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [9/25],Step [8/64],Loss: 1.3720, Acc_digits: 83/128 (65%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [9/64],Loss: 1.4160, Acc_digits: 89/128 (70%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [10/64],Loss: 1.7705, Acc_digits: 76/128 (59%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [9/25],Step [11/64],Loss: 1.8916, Acc_digits: 77/128 (60%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [9/25],Step [12/64],Loss: 1.3457, Acc_digits: 86/128 (67%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [13/64],Loss: 1.4810, Acc_digits: 85/128 (66%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [14/64],Loss: 1.6379, Acc_digits: 76/128 (59%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [9/25],Step [15/64],Loss: 1.3057, Acc_digits: 103/128 (80%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.9694, Acc_digits: 66/80 (82%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22314221097622067\n",
      "\n",
      "Test set: Loss: 1.0331,Acc_digits: 1692/2000 (85%),Acc_labels: 770/1000 77%\n",
      "Epoch [10/25],Step [1/64],Loss: 1.4194, Acc_digits: 85/128 (66%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [10/25],Step [2/64],Loss: 1.6190, Acc_digits: 83/128 (65%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [3/64],Loss: 1.7328, Acc_digits: 82/128 (64%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [10/25],Step [4/64],Loss: 1.6961, Acc_digits: 84/128 (66%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.5090, Acc_digits: 83/128 (65%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [10/25],Step [6/64],Loss: 1.6973, Acc_digits: 79/128 (62%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [10/25],Step [7/64],Loss: 1.5867, Acc_digits: 90/128 (70%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [8/64],Loss: 1.3812, Acc_digits: 83/128 (65%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [9/64],Loss: 1.3921, Acc_digits: 83/128 (65%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [10/64],Loss: 1.5911, Acc_digits: 79/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [11/64],Loss: 1.4857, Acc_digits: 88/128 (69%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [12/64],Loss: 1.3216, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [13/64],Loss: 1.4886, Acc_digits: 82/128 (64%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [14/64],Loss: 1.4185, Acc_digits: 85/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [15/64],Loss: 1.3481, Acc_digits: 93/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [16/40],Loss: 1.2148, Acc_digits: 56/80 (70%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.251661992049776\n",
      "\n",
      "Test set: Loss: 0.9776,Acc_digits: 1700/2000 (85%),Acc_labels: 789/1000 79%\n",
      "Epoch [11/25],Step [1/64],Loss: 1.6168, Acc_digits: 82/128 (64%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [2/64],Loss: 1.4289, Acc_digits: 83/128 (65%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [3/64],Loss: 1.5098, Acc_digits: 82/128 (64%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [11/25],Step [4/64],Loss: 1.2499, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [5/64],Loss: 1.5484, Acc_digits: 79/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [6/64],Loss: 1.5465, Acc_digits: 81/128 (63%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [7/64],Loss: 1.5741, Acc_digits: 81/128 (63%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.4693, Acc_digits: 79/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [9/64],Loss: 1.2442, Acc_digits: 88/128 (69%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [10/64],Loss: 1.4764, Acc_digits: 81/128 (63%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [11/64],Loss: 1.6238, Acc_digits: 88/128 (69%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [11/25],Step [12/64],Loss: 1.2350, Acc_digits: 89/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [13/64],Loss: 1.6911, Acc_digits: 80/128 (62%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [11/25],Step [14/64],Loss: 1.6031, Acc_digits: 80/128 (62%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [15/64],Loss: 1.5235, Acc_digits: 85/128 (66%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [16/40],Loss: 1.2949, Acc_digits: 55/80 (69%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22470252297353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.0050,Acc_digits: 1705/2000 (85%),Acc_labels: 794/1000 79%\n",
      "Epoch [12/25],Step [1/64],Loss: 1.5874, Acc_digits: 86/128 (67%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [2/64],Loss: 1.4968, Acc_digits: 88/128 (69%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [3/64],Loss: 1.4755, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [4/64],Loss: 1.5216, Acc_digits: 86/128 (67%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [12/25],Step [5/64],Loss: 1.5040, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [6/64],Loss: 1.6957, Acc_digits: 79/128 (62%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [12/25],Step [7/64],Loss: 1.5071, Acc_digits: 86/128 (67%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [12/25],Step [8/64],Loss: 1.5374, Acc_digits: 80/128 (62%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [12/25],Step [9/64],Loss: 1.3007, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [10/64],Loss: 1.6238, Acc_digits: 80/128 (62%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [11/64],Loss: 1.5424, Acc_digits: 83/128 (65%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [12/25],Step [12/64],Loss: 1.1519, Acc_digits: 92/128 (72%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [13/64],Loss: 1.4609, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [14/64],Loss: 1.3718, Acc_digits: 85/128 (66%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [15/64],Loss: 1.3928, Acc_digits: 84/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [16/40],Loss: 1.4014, Acc_digits: 55/80 (69%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24013738392386585\n",
      "\n",
      "Test set: Loss: 0.9522,Acc_digits: 1714/2000 (86%),Acc_labels: 783/1000 78%\n",
      "Epoch [13/25],Step [1/64],Loss: 1.3898, Acc_digits: 89/128 (70%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [2/64],Loss: 1.7101, Acc_digits: 74/128 (58%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [3/64],Loss: 1.4804, Acc_digits: 90/128 (70%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [4/64],Loss: 1.4655, Acc_digits: 85/128 (66%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [13/25],Step [5/64],Loss: 1.5848, Acc_digits: 81/128 (63%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [13/25],Step [6/64],Loss: 1.7211, Acc_digits: 81/128 (63%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [13/25],Step [7/64],Loss: 1.4747, Acc_digits: 87/128 (68%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [13/25],Step [8/64],Loss: 1.5877, Acc_digits: 81/128 (63%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [9/64],Loss: 1.3604, Acc_digits: 88/128 (69%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [10/64],Loss: 1.4451, Acc_digits: 88/128 (69%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [13/25],Step [11/64],Loss: 1.4224, Acc_digits: 81/128 (63%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [12/64],Loss: 1.3386, Acc_digits: 89/128 (70%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [13/64],Loss: 1.3961, Acc_digits: 90/128 (70%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [13/25],Step [14/64],Loss: 1.2671, Acc_digits: 88/128 (69%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [15/64],Loss: 1.3467, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [16/40],Loss: 1.0353, Acc_digits: 66/80 (82%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2870031000347808\n",
      "\n",
      "Test set: Loss: 0.9175,Acc_digits: 1724/2000 (86%),Acc_labels: 789/1000 79%\n",
      "Epoch [14/25],Step [1/64],Loss: 1.3623, Acc_digits: 86/128 (67%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [2/64],Loss: 1.4686, Acc_digits: 82/128 (64%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [3/64],Loss: 1.5399, Acc_digits: 89/128 (70%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [14/25],Step [4/64],Loss: 1.5286, Acc_digits: 84/128 (66%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [14/25],Step [5/64],Loss: 1.3478, Acc_digits: 87/128 (68%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [14/25],Step [6/64],Loss: 1.6235, Acc_digits: 75/128 (59%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [14/25],Step [7/64],Loss: 1.7636, Acc_digits: 78/128 (61%) ,Acc_labels: 38/64 (59%)\n",
      "Epoch [14/25],Step [8/64],Loss: 1.4314, Acc_digits: 84/128 (66%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [14/25],Step [9/64],Loss: 1.2702, Acc_digits: 89/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [10/64],Loss: 1.5337, Acc_digits: 82/128 (64%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [11/64],Loss: 1.5964, Acc_digits: 77/128 (60%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [14/25],Step [12/64],Loss: 1.2061, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [13/64],Loss: 1.3996, Acc_digits: 89/128 (70%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [14/64],Loss: 1.4432, Acc_digits: 85/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [15/64],Loss: 1.2945, Acc_digits: 86/128 (67%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.9688, Acc_digits: 63/80 (79%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2485164919635281\n",
      "\n",
      "Test set: Loss: 0.8910,Acc_digits: 1735/2000 (87%),Acc_labels: 821/1000 82%\n",
      "Epoch [15/25],Step [1/64],Loss: 1.3220, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [2/64],Loss: 1.4353, Acc_digits: 91/128 (71%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [15/25],Step [3/64],Loss: 1.4798, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [4/64],Loss: 1.3189, Acc_digits: 94/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [15/25],Step [5/64],Loss: 1.4257, Acc_digits: 85/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [15/25],Step [6/64],Loss: 1.5140, Acc_digits: 88/128 (69%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [15/25],Step [7/64],Loss: 1.7051, Acc_digits: 81/128 (63%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [15/25],Step [8/64],Loss: 1.5970, Acc_digits: 81/128 (63%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [9/64],Loss: 1.4366, Acc_digits: 84/128 (66%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [10/64],Loss: 1.4377, Acc_digits: 84/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [11/64],Loss: 1.5339, Acc_digits: 91/128 (71%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [15/25],Step [12/64],Loss: 1.2712, Acc_digits: 86/128 (67%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [13/64],Loss: 1.6359, Acc_digits: 81/128 (63%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [14/64],Loss: 1.3608, Acc_digits: 87/128 (68%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [15/25],Step [15/64],Loss: 1.3701, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [16/40],Loss: 1.2106, Acc_digits: 63/80 (79%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.23054950404912233\n",
      "\n",
      "Test set: Loss: 0.9106,Acc_digits: 1740/2000 (87%),Acc_labels: 803/1000 80%\n",
      "Epoch [16/25],Step [1/64],Loss: 1.3620, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [2/64],Loss: 1.3666, Acc_digits: 86/128 (67%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [3/64],Loss: 1.3684, Acc_digits: 94/128 (73%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [16/25],Step [4/64],Loss: 1.3984, Acc_digits: 87/128 (68%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [16/25],Step [5/64],Loss: 1.3371, Acc_digits: 86/128 (67%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [6/64],Loss: 1.5100, Acc_digits: 85/128 (66%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [16/25],Step [7/64],Loss: 1.3005, Acc_digits: 91/128 (71%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [16/25],Step [8/64],Loss: 1.3033, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [9/64],Loss: 1.4165, Acc_digits: 86/128 (67%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [10/64],Loss: 1.4011, Acc_digits: 86/128 (67%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [11/64],Loss: 1.5824, Acc_digits: 86/128 (67%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [12/64],Loss: 1.1651, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [13/64],Loss: 1.4251, Acc_digits: 91/128 (71%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [14/64],Loss: 1.3727, Acc_digits: 91/128 (71%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [15/64],Loss: 1.3521, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [16/40],Loss: 1.2266, Acc_digits: 55/80 (69%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23414578405208886\n",
      "\n",
      "Test set: Loss: 0.9041,Acc_digits: 1729/2000 (86%),Acc_labels: 822/1000 82%\n",
      "Epoch [17/25],Step [1/64],Loss: 1.5099, Acc_digits: 87/128 (68%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [2/64],Loss: 1.4370, Acc_digits: 84/128 (66%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [17/25],Step [3/64],Loss: 1.3102, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [4/64],Loss: 1.3820, Acc_digits: 93/128 (73%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [17/25],Step [5/64],Loss: 1.6147, Acc_digits: 80/128 (62%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [17/25],Step [6/64],Loss: 1.3470, Acc_digits: 84/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [7/64],Loss: 1.4254, Acc_digits: 86/128 (67%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [17/25],Step [8/64],Loss: 1.2760, Acc_digits: 95/128 (74%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [17/25],Step [9/64],Loss: 1.4011, Acc_digits: 83/128 (65%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [10/64],Loss: 1.4680, Acc_digits: 89/128 (70%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [11/64],Loss: 1.6479, Acc_digits: 82/128 (64%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [12/64],Loss: 1.1248, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [13/64],Loss: 1.5251, Acc_digits: 86/128 (67%) ,Acc_labels: 51/64 (80%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25],Step [14/64],Loss: 1.2240, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [15/64],Loss: 1.3171, Acc_digits: 86/128 (67%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [16/40],Loss: 1.2514, Acc_digits: 57/80 (71%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2334209280088544\n",
      "\n",
      "Test set: Loss: 0.9013,Acc_digits: 1733/2000 (87%),Acc_labels: 817/1000 82%\n",
      "Epoch [18/25],Step [1/64],Loss: 1.5047, Acc_digits: 88/128 (69%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [18/25],Step [2/64],Loss: 1.5029, Acc_digits: 79/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [3/64],Loss: 1.4389, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [4/64],Loss: 1.2403, Acc_digits: 101/128 (79%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [18/25],Step [5/64],Loss: 1.2122, Acc_digits: 90/128 (70%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [6/64],Loss: 1.4160, Acc_digits: 91/128 (71%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [7/64],Loss: 1.4697, Acc_digits: 87/128 (68%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [8/64],Loss: 1.4648, Acc_digits: 88/128 (69%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [18/25],Step [9/64],Loss: 1.4275, Acc_digits: 82/128 (64%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [10/64],Loss: 1.4016, Acc_digits: 89/128 (70%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [11/64],Loss: 1.4597, Acc_digits: 87/128 (68%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [12/64],Loss: 1.2391, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [13/64],Loss: 1.4601, Acc_digits: 82/128 (64%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [14/64],Loss: 1.3676, Acc_digits: 79/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [15/64],Loss: 1.3625, Acc_digits: 89/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [16/40],Loss: 1.1044, Acc_digits: 59/80 (74%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.23622091906145215\n",
      "\n",
      "Test set: Loss: 0.8845,Acc_digits: 1735/2000 (87%),Acc_labels: 802/1000 80%\n",
      "Epoch [19/25],Step [1/64],Loss: 1.7262, Acc_digits: 87/128 (68%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [2/64],Loss: 1.4158, Acc_digits: 83/128 (65%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [3/64],Loss: 1.4101, Acc_digits: 89/128 (70%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [4/64],Loss: 1.2255, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [5/64],Loss: 1.5035, Acc_digits: 78/128 (61%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [6/64],Loss: 1.4295, Acc_digits: 85/128 (66%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [7/64],Loss: 1.5029, Acc_digits: 93/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [8/64],Loss: 1.3643, Acc_digits: 87/128 (68%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [9/64],Loss: 1.1193, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [10/64],Loss: 1.5822, Acc_digits: 79/128 (62%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [19/25],Step [11/64],Loss: 1.7232, Acc_digits: 86/128 (67%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [12/64],Loss: 1.1326, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [13/64],Loss: 1.5555, Acc_digits: 76/128 (59%) ,Acc_labels: 42/64 (66%)\n",
      "Epoch [19/25],Step [14/64],Loss: 1.5368, Acc_digits: 87/128 (68%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [15/64],Loss: 1.4345, Acc_digits: 87/128 (68%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [16/40],Loss: 1.1568, Acc_digits: 63/80 (79%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.24962293705902994\n",
      "\n",
      "Test set: Loss: 0.8834,Acc_digits: 1773/2000 (89%),Acc_labels: 804/1000 80%\n",
      "Epoch [20/25],Step [1/64],Loss: 1.4807, Acc_digits: 91/128 (71%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [20/25],Step [2/64],Loss: 1.5145, Acc_digits: 83/128 (65%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [3/64],Loss: 1.3510, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [4/64],Loss: 1.3191, Acc_digits: 88/128 (69%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [5/64],Loss: 1.4568, Acc_digits: 88/128 (69%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.3535, Acc_digits: 83/128 (65%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [7/64],Loss: 1.4240, Acc_digits: 89/128 (70%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [20/25],Step [8/64],Loss: 1.4372, Acc_digits: 79/128 (62%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [20/25],Step [9/64],Loss: 1.2724, Acc_digits: 88/128 (69%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [10/64],Loss: 1.2821, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [11/64],Loss: 1.5303, Acc_digits: 85/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [12/64],Loss: 1.0652, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [13/64],Loss: 1.4861, Acc_digits: 91/128 (71%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [20/25],Step [14/64],Loss: 1.2572, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [15/64],Loss: 1.3078, Acc_digits: 91/128 (71%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [16/40],Loss: 1.3300, Acc_digits: 53/80 (66%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.21958388795610517\n",
      "\n",
      "Test set: Loss: 0.8766,Acc_digits: 1764/2000 (88%),Acc_labels: 811/1000 81%\n",
      "Epoch [21/25],Step [1/64],Loss: 1.2854, Acc_digits: 91/128 (71%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [2/64],Loss: 1.5252, Acc_digits: 82/128 (64%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [3/64],Loss: 1.4496, Acc_digits: 86/128 (67%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [21/25],Step [4/64],Loss: 1.4450, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.5167, Acc_digits: 88/128 (69%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [6/64],Loss: 1.4904, Acc_digits: 83/128 (65%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [7/64],Loss: 1.4184, Acc_digits: 87/128 (68%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [21/25],Step [8/64],Loss: 1.2412, Acc_digits: 96/128 (75%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [21/25],Step [9/64],Loss: 1.1245, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [10/64],Loss: 1.5615, Acc_digits: 86/128 (67%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [21/25],Step [11/64],Loss: 1.2509, Acc_digits: 90/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [12/64],Loss: 1.0268, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [13/64],Loss: 1.4609, Acc_digits: 84/128 (66%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [21/25],Step [14/64],Loss: 1.2547, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [15/64],Loss: 1.1374, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [16/40],Loss: 1.1435, Acc_digits: 60/80 (75%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22426392999477684\n",
      "\n",
      "Test set: Loss: 0.8318,Acc_digits: 1755/2000 (88%),Acc_labels: 809/1000 81%\n",
      "Epoch [22/25],Step [1/64],Loss: 1.3659, Acc_digits: 95/128 (74%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [22/25],Step [2/64],Loss: 1.3836, Acc_digits: 85/128 (66%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [3/64],Loss: 1.5033, Acc_digits: 86/128 (67%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [4/64],Loss: 1.4101, Acc_digits: 89/128 (70%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [22/25],Step [5/64],Loss: 1.4231, Acc_digits: 85/128 (66%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [6/64],Loss: 1.3987, Acc_digits: 87/128 (68%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [7/64],Loss: 1.3069, Acc_digits: 87/128 (68%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [22/25],Step [8/64],Loss: 1.5225, Acc_digits: 81/128 (63%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [22/25],Step [9/64],Loss: 1.1168, Acc_digits: 95/128 (74%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [10/64],Loss: 1.2564, Acc_digits: 90/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [11/64],Loss: 1.3332, Acc_digits: 90/128 (70%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [12/64],Loss: 1.2694, Acc_digits: 97/128 (76%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [13/64],Loss: 1.3512, Acc_digits: 84/128 (66%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [22/25],Step [14/64],Loss: 1.2143, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [22/25],Step [15/64],Loss: 1.4015, Acc_digits: 84/128 (66%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [22/25],Step [16/40],Loss: 1.1350, Acc_digits: 61/80 (76%) ,Acc_labels: 29/40 (72%)\n",
      "Time needed to train  0.22317279898561537\n",
      "\n",
      "Test set: Loss: 0.8062,Acc_digits: 1775/2000 (89%),Acc_labels: 815/1000 82%\n",
      "Epoch [23/25],Step [1/64],Loss: 1.1476, Acc_digits: 96/128 (75%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [23/25],Step [2/64],Loss: 1.5265, Acc_digits: 83/128 (65%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [3/64],Loss: 1.1030, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [4/64],Loss: 1.3618, Acc_digits: 89/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.4220, Acc_digits: 89/128 (70%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [6/64],Loss: 1.3733, Acc_digits: 90/128 (70%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [23/25],Step [7/64],Loss: 1.2507, Acc_digits: 89/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [8/64],Loss: 1.2604, Acc_digits: 89/128 (70%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [23/25],Step [9/64],Loss: 1.1805, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [10/64],Loss: 1.4225, Acc_digits: 83/128 (65%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [11/64],Loss: 1.3999, Acc_digits: 89/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [12/64],Loss: 1.1435, Acc_digits: 94/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [13/64],Loss: 1.5428, Acc_digits: 88/128 (69%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [14/64],Loss: 1.1564, Acc_digits: 92/128 (72%) ,Acc_labels: 49/64 (77%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25],Step [15/64],Loss: 1.2625, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [16/40],Loss: 1.1178, Acc_digits: 61/80 (76%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2133769509382546\n",
      "\n",
      "Test set: Loss: 0.8295,Acc_digits: 1777/2000 (89%),Acc_labels: 818/1000 82%\n",
      "Epoch [24/25],Step [1/64],Loss: 1.3515, Acc_digits: 88/128 (69%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [24/25],Step [2/64],Loss: 1.4552, Acc_digits: 85/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [3/64],Loss: 1.3112, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [4/64],Loss: 1.2821, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [5/64],Loss: 1.4250, Acc_digits: 86/128 (67%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [6/64],Loss: 1.4032, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [7/64],Loss: 1.5631, Acc_digits: 87/128 (68%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [24/25],Step [8/64],Loss: 1.3237, Acc_digits: 90/128 (70%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [24/25],Step [9/64],Loss: 1.1426, Acc_digits: 93/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [10/64],Loss: 1.3638, Acc_digits: 85/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [11/64],Loss: 1.4474, Acc_digits: 84/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [12/64],Loss: 1.1466, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [24/25],Step [13/64],Loss: 1.4963, Acc_digits: 88/128 (69%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [24/25],Step [14/64],Loss: 1.2656, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [15/64],Loss: 1.2074, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.8290, Acc_digits: 67/80 (84%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2285967320203781\n",
      "\n",
      "Test set: Loss: 0.8269,Acc_digits: 1765/2000 (88%),Acc_labels: 819/1000 82%\n",
      "Epoch [25/25],Step [1/64],Loss: 1.2624, Acc_digits: 94/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [2/64],Loss: 1.2966, Acc_digits: 80/128 (62%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [3/64],Loss: 1.5148, Acc_digits: 84/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [4/64],Loss: 1.2997, Acc_digits: 91/128 (71%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [25/25],Step [5/64],Loss: 1.3087, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.6807, Acc_digits: 80/128 (62%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [25/25],Step [7/64],Loss: 1.4498, Acc_digits: 92/128 (72%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.3404, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [25/25],Step [9/64],Loss: 1.0762, Acc_digits: 94/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [10/64],Loss: 1.4089, Acc_digits: 89/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [11/64],Loss: 1.2773, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [12/64],Loss: 1.1216, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [13/64],Loss: 1.3792, Acc_digits: 83/128 (65%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [14/64],Loss: 1.2595, Acc_digits: 91/128 (71%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [25/25],Step [15/64],Loss: 1.1257, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.8697, Acc_digits: 66/80 (82%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24078286299481988\n",
      "\n",
      "Test set: Loss: 0.8044,Acc_digits: 1777/2000 (89%),Acc_labels: 810/1000 81%\n",
      "Epoch [26/25],Step [1/64],Loss: 1.2586, Acc_digits: 97/128 (76%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [26/25],Step [2/64],Loss: 1.1799, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [3/64],Loss: 1.3401, Acc_digits: 88/128 (69%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [26/25],Step [4/64],Loss: 1.1331, Acc_digits: 96/128 (75%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.3748, Acc_digits: 84/128 (66%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [26/25],Step [6/64],Loss: 1.3461, Acc_digits: 87/128 (68%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [26/25],Step [7/64],Loss: 1.4672, Acc_digits: 81/128 (63%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [26/25],Step [8/64],Loss: 1.2922, Acc_digits: 82/128 (64%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [9/64],Loss: 1.3100, Acc_digits: 90/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [10/64],Loss: 1.2844, Acc_digits: 85/128 (66%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [11/64],Loss: 1.2319, Acc_digits: 91/128 (71%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [12/64],Loss: 1.3602, Acc_digits: 92/128 (72%) ,Acc_labels: 43/64 (67%)\n",
      "Epoch [26/25],Step [13/64],Loss: 1.2432, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [14/64],Loss: 1.1271, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [15/64],Loss: 1.3546, Acc_digits: 87/128 (68%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.8966, Acc_digits: 62/80 (78%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22327250498346984\n",
      "\n",
      "Test set: Loss: 0.8011,Acc_digits: 1761/2000 (88%),Acc_labels: 820/1000 82%\n",
      "Epoch [2/25],Step [1/64],Loss: 1.2618, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [2/25],Step [2/64],Loss: 1.1943, Acc_digits: 93/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [3/64],Loss: 1.5474, Acc_digits: 88/128 (69%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [2/25],Step [4/64],Loss: 1.1393, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [5/64],Loss: 1.4330, Acc_digits: 87/128 (68%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [6/64],Loss: 1.4679, Acc_digits: 86/128 (67%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [7/64],Loss: 1.2937, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [8/64],Loss: 1.2582, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [9/64],Loss: 1.1038, Acc_digits: 96/128 (75%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [10/64],Loss: 1.3423, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [11/64],Loss: 1.3310, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.9373, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [13/64],Loss: 1.3232, Acc_digits: 92/128 (72%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [2/25],Step [14/64],Loss: 1.3825, Acc_digits: 94/128 (73%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [2/25],Step [15/64],Loss: 1.0776, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [16/40],Loss: 1.0524, Acc_digits: 61/80 (76%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.226751578040421\n",
      "\n",
      "Test set: Loss: 0.7860,Acc_digits: 1768/2000 (88%),Acc_labels: 822/1000 82%\n",
      "Epoch [3/25],Step [1/64],Loss: 1.3162, Acc_digits: 87/128 (68%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [3/25],Step [2/64],Loss: 1.5444, Acc_digits: 84/128 (66%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [3/64],Loss: 1.3019, Acc_digits: 87/128 (68%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [4/64],Loss: 1.2724, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [5/64],Loss: 1.2349, Acc_digits: 92/128 (72%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [6/64],Loss: 1.4426, Acc_digits: 85/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [7/64],Loss: 1.2819, Acc_digits: 92/128 (72%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [8/64],Loss: 1.4022, Acc_digits: 88/128 (69%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.9878, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [10/64],Loss: 1.3172, Acc_digits: 88/128 (69%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [11/64],Loss: 1.3346, Acc_digits: 90/128 (70%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [3/25],Step [12/64],Loss: 1.0036, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [13/64],Loss: 1.2127, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [14/64],Loss: 1.3420, Acc_digits: 94/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [15/64],Loss: 1.0948, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [16/40],Loss: 1.1102, Acc_digits: 58/80 (72%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.23002527200151235\n",
      "\n",
      "Test set: Loss: 0.7901,Acc_digits: 1782/2000 (89%),Acc_labels: 821/1000 82%\n",
      "Epoch [4/25],Step [1/64],Loss: 1.2917, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [2/64],Loss: 1.1852, Acc_digits: 89/128 (70%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [3/64],Loss: 1.2355, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [4/64],Loss: 1.6492, Acc_digits: 88/128 (69%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [5/64],Loss: 1.3267, Acc_digits: 91/128 (71%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [6/64],Loss: 1.2803, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [7/64],Loss: 1.3851, Acc_digits: 95/128 (74%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.2165, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [9/64],Loss: 1.1146, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [10/64],Loss: 1.0976, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [11/64],Loss: 1.3004, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [12/64],Loss: 1.0472, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [13/64],Loss: 1.2788, Acc_digits: 91/128 (71%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [14/64],Loss: 1.2384, Acc_digits: 92/128 (72%) ,Acc_labels: 52/64 (81%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25],Step [15/64],Loss: 1.2283, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [16/40],Loss: 1.1362, Acc_digits: 55/80 (69%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.2214396019699052\n",
      "\n",
      "Test set: Loss: 0.7931,Acc_digits: 1785/2000 (89%),Acc_labels: 810/1000 81%\n",
      "Epoch [5/25],Step [1/64],Loss: 1.3803, Acc_digits: 95/128 (74%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [2/64],Loss: 1.2708, Acc_digits: 92/128 (72%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [5/25],Step [3/64],Loss: 1.1775, Acc_digits: 91/128 (71%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [4/64],Loss: 1.3092, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [5/64],Loss: 1.4546, Acc_digits: 91/128 (71%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [5/25],Step [6/64],Loss: 1.2247, Acc_digits: 94/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [5/25],Step [7/64],Loss: 1.4769, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [8/64],Loss: 1.2847, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [9/64],Loss: 1.0959, Acc_digits: 97/128 (76%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [5/25],Step [10/64],Loss: 1.3089, Acc_digits: 89/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [11/64],Loss: 1.1240, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [12/64],Loss: 1.0980, Acc_digits: 101/128 (79%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [13/64],Loss: 1.3668, Acc_digits: 90/128 (70%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [14/64],Loss: 1.2372, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [15/64],Loss: 1.3058, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [16/40],Loss: 1.3347, Acc_digits: 53/80 (66%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.23241118504665792\n",
      "\n",
      "Test set: Loss: 0.7872,Acc_digits: 1790/2000 (90%),Acc_labels: 823/1000 82%\n",
      "Epoch [6/25],Step [1/64],Loss: 1.3262, Acc_digits: 95/128 (74%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [6/25],Step [2/64],Loss: 1.3401, Acc_digits: 84/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [3/64],Loss: 1.1600, Acc_digits: 89/128 (70%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [4/64],Loss: 1.2536, Acc_digits: 93/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [5/64],Loss: 1.3171, Acc_digits: 90/128 (70%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [6/64],Loss: 1.1718, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [7/64],Loss: 1.3513, Acc_digits: 91/128 (71%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.3023, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [9/64],Loss: 1.0306, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [10/64],Loss: 1.3213, Acc_digits: 84/128 (66%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [6/25],Step [11/64],Loss: 1.2182, Acc_digits: 91/128 (71%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [12/64],Loss: 1.0620, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [13/64],Loss: 1.1521, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [14/64],Loss: 1.2336, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [15/64],Loss: 1.0584, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [16/40],Loss: 1.0257, Acc_digits: 63/80 (79%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.22758290392812341\n",
      "\n",
      "Test set: Loss: 0.7816,Acc_digits: 1791/2000 (90%),Acc_labels: 808/1000 81%\n",
      "Epoch [7/25],Step [1/64],Loss: 1.2457, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [2/64],Loss: 1.3550, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [3/64],Loss: 1.0758, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [4/64],Loss: 1.2755, Acc_digits: 102/128 (80%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [7/25],Step [5/64],Loss: 1.3008, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [6/64],Loss: 1.4279, Acc_digits: 83/128 (65%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [7/64],Loss: 1.3435, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [7/25],Step [8/64],Loss: 1.1410, Acc_digits: 97/128 (76%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [9/64],Loss: 1.1174, Acc_digits: 90/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [10/64],Loss: 1.0363, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [11/64],Loss: 1.2191, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.9643, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [13/64],Loss: 1.3343, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [14/64],Loss: 1.2737, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [15/64],Loss: 1.0998, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.8898, Acc_digits: 67/80 (84%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22784012695774436\n",
      "\n",
      "Test set: Loss: 0.7620,Acc_digits: 1776/2000 (89%),Acc_labels: 811/1000 81%\n",
      "Epoch [8/25],Step [1/64],Loss: 1.3228, Acc_digits: 91/128 (71%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [2/64],Loss: 1.3197, Acc_digits: 88/128 (69%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [3/64],Loss: 1.2264, Acc_digits: 98/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [4/64],Loss: 1.0717, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [5/64],Loss: 1.4991, Acc_digits: 87/128 (68%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [6/64],Loss: 1.1793, Acc_digits: 86/128 (67%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [7/64],Loss: 1.1841, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [8/64],Loss: 1.3377, Acc_digits: 86/128 (67%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [9/64],Loss: 1.1441, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [10/64],Loss: 1.3066, Acc_digits: 97/128 (76%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [11/64],Loss: 1.4120, Acc_digits: 89/128 (70%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [12/64],Loss: 1.1526, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [13/64],Loss: 1.2674, Acc_digits: 89/128 (70%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [14/64],Loss: 1.0524, Acc_digits: 98/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [15/64],Loss: 1.2076, Acc_digits: 93/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.7807, Acc_digits: 64/80 (80%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.24067598499823362\n",
      "\n",
      "Test set: Loss: 0.7580,Acc_digits: 1789/2000 (89%),Acc_labels: 813/1000 81%\n",
      "Epoch [9/25],Step [1/64],Loss: 1.1926, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [2/64],Loss: 1.3206, Acc_digits: 92/128 (72%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [3/64],Loss: 1.0835, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [4/64],Loss: 1.1427, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [9/25],Step [5/64],Loss: 1.1224, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [6/64],Loss: 1.2704, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [7/64],Loss: 1.2302, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [8/64],Loss: 1.2507, Acc_digits: 85/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [9/64],Loss: 1.1237, Acc_digits: 94/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [10/64],Loss: 1.2608, Acc_digits: 89/128 (70%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [11/64],Loss: 1.1848, Acc_digits: 91/128 (71%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [12/64],Loss: 1.2301, Acc_digits: 94/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [13/64],Loss: 1.2544, Acc_digits: 93/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [14/64],Loss: 1.0696, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [15/64],Loss: 1.2841, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [16/40],Loss: 1.1376, Acc_digits: 62/80 (78%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.2302158799720928\n",
      "\n",
      "Test set: Loss: 0.7395,Acc_digits: 1800/2000 (90%),Acc_labels: 835/1000 84%\n",
      "Epoch [10/25],Step [1/64],Loss: 1.1640, Acc_digits: 97/128 (76%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [10/25],Step [2/64],Loss: 1.2539, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [3/64],Loss: 1.2226, Acc_digits: 91/128 (71%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [4/64],Loss: 1.2040, Acc_digits: 98/128 (77%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.4643, Acc_digits: 87/128 (68%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [6/64],Loss: 1.4249, Acc_digits: 93/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [7/64],Loss: 1.3890, Acc_digits: 89/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [8/64],Loss: 1.4189, Acc_digits: 86/128 (67%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [10/25],Step [9/64],Loss: 1.1451, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [10/64],Loss: 1.2884, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [11/64],Loss: 1.3135, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [10/25],Step [12/64],Loss: 1.1495, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [10/25],Step [13/64],Loss: 1.1806, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25],Step [14/64],Loss: 1.2686, Acc_digits: 86/128 (67%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [15/64],Loss: 1.0018, Acc_digits: 101/128 (79%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [16/40],Loss: 1.0829, Acc_digits: 64/80 (80%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23159333306830376\n",
      "\n",
      "Test set: Loss: 0.7631,Acc_digits: 1794/2000 (90%),Acc_labels: 821/1000 82%\n",
      "Epoch [11/25],Step [1/64],Loss: 1.2622, Acc_digits: 95/128 (74%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [11/25],Step [2/64],Loss: 1.0995, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [3/64],Loss: 1.2613, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [4/64],Loss: 1.2789, Acc_digits: 97/128 (76%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [11/25],Step [5/64],Loss: 1.3437, Acc_digits: 89/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [6/64],Loss: 1.3392, Acc_digits: 92/128 (72%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [7/64],Loss: 1.2102, Acc_digits: 93/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.2557, Acc_digits: 95/128 (74%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.9378, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [10/64],Loss: 1.1779, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [11/25],Step [11/64],Loss: 1.2540, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [11/25],Step [12/64],Loss: 1.0811, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [13/64],Loss: 1.3336, Acc_digits: 87/128 (68%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [11/25],Step [14/64],Loss: 1.3091, Acc_digits: 93/128 (73%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [15/64],Loss: 1.0419, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [16/40],Loss: 1.0070, Acc_digits: 63/80 (79%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2263029869645834\n",
      "\n",
      "Test set: Loss: 0.7447,Acc_digits: 1812/2000 (91%),Acc_labels: 820/1000 82%\n",
      "Epoch [12/25],Step [1/64],Loss: 1.2538, Acc_digits: 95/128 (74%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [2/64],Loss: 1.3117, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [3/64],Loss: 1.2136, Acc_digits: 95/128 (74%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [4/64],Loss: 1.2251, Acc_digits: 92/128 (72%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [12/25],Step [5/64],Loss: 1.3996, Acc_digits: 91/128 (71%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [6/64],Loss: 1.3668, Acc_digits: 88/128 (69%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [7/64],Loss: 1.2955, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [8/64],Loss: 1.3312, Acc_digits: 96/128 (75%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [12/25],Step [9/64],Loss: 1.1332, Acc_digits: 90/128 (70%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [10/64],Loss: 1.1221, Acc_digits: 88/128 (69%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [11/64],Loss: 1.2865, Acc_digits: 92/128 (72%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.9549, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [13/64],Loss: 1.2200, Acc_digits: 93/128 (73%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [12/25],Step [14/64],Loss: 1.0727, Acc_digits: 91/128 (71%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [15/64],Loss: 1.4170, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.9883, Acc_digits: 65/80 (81%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.23472444096114486\n",
      "\n",
      "Test set: Loss: 0.7591,Acc_digits: 1784/2000 (89%),Acc_labels: 815/1000 82%\n",
      "Epoch [13/25],Step [1/64],Loss: 1.2499, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [2/64],Loss: 1.2618, Acc_digits: 92/128 (72%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [3/64],Loss: 1.1463, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [4/64],Loss: 1.1686, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [5/64],Loss: 1.1781, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [6/64],Loss: 1.4930, Acc_digits: 87/128 (68%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [13/25],Step [7/64],Loss: 1.1947, Acc_digits: 92/128 (72%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [8/64],Loss: 1.1187, Acc_digits: 90/128 (70%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [9/64],Loss: 1.0389, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [10/64],Loss: 1.3753, Acc_digits: 93/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [11/64],Loss: 1.1925, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.8818, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [13/64],Loss: 1.2535, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [14/64],Loss: 1.1451, Acc_digits: 92/128 (72%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [15/64],Loss: 1.2404, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [16/40],Loss: 1.0287, Acc_digits: 63/80 (79%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.22854848694987595\n",
      "\n",
      "Test set: Loss: 0.7459,Acc_digits: 1789/2000 (89%),Acc_labels: 813/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 1.2276, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [2/64],Loss: 1.1185, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [3/64],Loss: 1.0171, Acc_digits: 96/128 (75%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [4/64],Loss: 1.1055, Acc_digits: 95/128 (74%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [14/25],Step [5/64],Loss: 1.3088, Acc_digits: 92/128 (72%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [14/25],Step [6/64],Loss: 1.3675, Acc_digits: 92/128 (72%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [14/25],Step [7/64],Loss: 1.3681, Acc_digits: 88/128 (69%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [14/25],Step [8/64],Loss: 1.3895, Acc_digits: 87/128 (68%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [14/25],Step [9/64],Loss: 1.0842, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.9659, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [11/64],Loss: 1.1963, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [12/64],Loss: 1.0186, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [13/64],Loss: 1.1374, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [14/64],Loss: 1.0731, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [15/64],Loss: 1.0497, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [16/40],Loss: 1.0949, Acc_digits: 61/80 (76%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2265383180929348\n",
      "\n",
      "Test set: Loss: 0.7280,Acc_digits: 1809/2000 (90%),Acc_labels: 817/1000 82%\n",
      "Epoch [15/25],Step [1/64],Loss: 1.1536, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [2/64],Loss: 1.2243, Acc_digits: 89/128 (70%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [3/64],Loss: 1.1607, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [4/64],Loss: 1.1693, Acc_digits: 100/128 (78%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [15/25],Step [5/64],Loss: 1.2038, Acc_digits: 98/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [15/25],Step [6/64],Loss: 1.3533, Acc_digits: 86/128 (67%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [15/25],Step [7/64],Loss: 1.4515, Acc_digits: 87/128 (68%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [8/64],Loss: 1.0812, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.9820, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [10/64],Loss: 1.1530, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [11/64],Loss: 1.1825, Acc_digits: 95/128 (74%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [12/64],Loss: 1.0149, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [13/64],Loss: 1.4044, Acc_digits: 93/128 (73%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [15/25],Step [14/64],Loss: 1.0953, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [15/64],Loss: 1.3022, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.7775, Acc_digits: 71/80 (89%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2323574359761551\n",
      "\n",
      "Test set: Loss: 0.7566,Acc_digits: 1783/2000 (89%),Acc_labels: 822/1000 82%\n",
      "Epoch [16/25],Step [1/64],Loss: 1.3286, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [2/64],Loss: 1.3746, Acc_digits: 82/128 (64%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [3/64],Loss: 1.0090, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [4/64],Loss: 1.1138, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [5/64],Loss: 1.2310, Acc_digits: 95/128 (74%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [16/25],Step [6/64],Loss: 1.2962, Acc_digits: 89/128 (70%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [7/64],Loss: 1.1392, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [8/64],Loss: 1.3788, Acc_digits: 86/128 (67%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [16/25],Step [9/64],Loss: 1.1313, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [10/64],Loss: 1.2152, Acc_digits: 90/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [11/64],Loss: 1.1887, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [12/64],Loss: 1.0140, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [13/64],Loss: 1.3017, Acc_digits: 93/128 (73%) ,Acc_labels: 48/64 (75%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25],Step [14/64],Loss: 1.1778, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [15/64],Loss: 1.2477, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [16/40],Loss: 1.0905, Acc_digits: 64/80 (80%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.23102975101210177\n",
      "\n",
      "Test set: Loss: 0.7203,Acc_digits: 1815/2000 (91%),Acc_labels: 822/1000 82%\n",
      "Epoch [17/25],Step [1/64],Loss: 1.1505, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [2/64],Loss: 1.1381, Acc_digits: 92/128 (72%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [3/64],Loss: 1.0837, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [4/64],Loss: 1.1180, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [5/64],Loss: 1.2541, Acc_digits: 95/128 (74%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [6/64],Loss: 1.2965, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [7/64],Loss: 1.2130, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [8/64],Loss: 1.3120, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [9/64],Loss: 1.0972, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [10/64],Loss: 1.1758, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [11/64],Loss: 1.0746, Acc_digits: 93/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.8369, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [13/64],Loss: 1.1795, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [14/64],Loss: 1.2252, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.9905, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.7596, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.24318691808730364\n",
      "\n",
      "Test set: Loss: 0.7054,Acc_digits: 1824/2000 (91%),Acc_labels: 820/1000 82%\n",
      "Epoch [18/25],Step [1/64],Loss: 1.1934, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [2/64],Loss: 1.1677, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [3/64],Loss: 1.2794, Acc_digits: 98/128 (77%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [18/25],Step [4/64],Loss: 1.0781, Acc_digits: 102/128 (80%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.9964, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [6/64],Loss: 1.1138, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [7/64],Loss: 1.1016, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [8/64],Loss: 1.0605, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [9/64],Loss: 1.2205, Acc_digits: 90/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [10/64],Loss: 1.1931, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.9960, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.8738, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [13/64],Loss: 1.1836, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.9473, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [15/64],Loss: 1.0009, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.9676, Acc_digits: 60/80 (75%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.24419341096654534\n",
      "\n",
      "Test set: Loss: 0.6870,Acc_digits: 1831/2000 (92%),Acc_labels: 826/1000 83%\n",
      "Epoch [19/25],Step [1/64],Loss: 1.2170, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [19/25],Step [2/64],Loss: 1.2008, Acc_digits: 96/128 (75%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [3/64],Loss: 1.0636, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.9936, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [5/64],Loss: 1.3339, Acc_digits: 89/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [6/64],Loss: 1.3478, Acc_digits: 92/128 (72%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [7/64],Loss: 1.3731, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [19/25],Step [8/64],Loss: 1.2816, Acc_digits: 88/128 (69%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [9/64],Loss: 1.0644, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [10/64],Loss: 1.3179, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [19/25],Step [11/64],Loss: 1.2033, Acc_digits: 88/128 (69%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [12/64],Loss: 1.0716, Acc_digits: 97/128 (76%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [13/64],Loss: 1.1154, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [14/64],Loss: 1.1373, Acc_digits: 97/128 (76%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [15/64],Loss: 1.0866, Acc_digits: 106/128 (83%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.7996, Acc_digits: 65/80 (81%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.25489926093723625\n",
      "\n",
      "Test set: Loss: 0.6830,Acc_digits: 1833/2000 (92%),Acc_labels: 834/1000 83%\n",
      "Epoch [20/25],Step [1/64],Loss: 1.1856, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [2/64],Loss: 1.0846, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [3/64],Loss: 1.2424, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [4/64],Loss: 1.0149, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [5/64],Loss: 1.2135, Acc_digits: 90/128 (70%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.0299, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [7/64],Loss: 1.1767, Acc_digits: 94/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [8/64],Loss: 1.2290, Acc_digits: 98/128 (77%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [20/25],Step [9/64],Loss: 1.2589, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [10/64],Loss: 1.0284, Acc_digits: 93/128 (73%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [11/64],Loss: 1.2052, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.9393, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [13/64],Loss: 1.2622, Acc_digits: 87/128 (68%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [14/64],Loss: 1.0952, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.8645, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.8617, Acc_digits: 65/80 (81%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.28770741005428135\n",
      "\n",
      "Test set: Loss: 0.7044,Acc_digits: 1818/2000 (91%),Acc_labels: 816/1000 82%\n",
      "Epoch [21/25],Step [1/64],Loss: 1.1487, Acc_digits: 92/128 (72%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [21/25],Step [2/64],Loss: 1.1242, Acc_digits: 93/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.9862, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [4/64],Loss: 1.2746, Acc_digits: 100/128 (78%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.4615, Acc_digits: 88/128 (69%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [21/25],Step [6/64],Loss: 1.3701, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [21/25],Step [7/64],Loss: 1.2044, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [8/64],Loss: 1.1697, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [21/25],Step [9/64],Loss: 1.3209, Acc_digits: 86/128 (67%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [10/64],Loss: 1.1773, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [11/64],Loss: 1.0113, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [12/64],Loss: 1.1045, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [13/64],Loss: 1.0888, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [14/64],Loss: 1.1409, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [21/25],Step [15/64],Loss: 1.1039, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.9595, Acc_digits: 70/80 (88%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.24710007896646857\n",
      "\n",
      "Test set: Loss: 0.7038,Acc_digits: 1811/2000 (91%),Acc_labels: 825/1000 82%\n",
      "Epoch [22/25],Step [1/64],Loss: 1.1675, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [2/64],Loss: 1.1554, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [3/64],Loss: 1.0566, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.9961, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25],Step [5/64],Loss: 1.0438, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [6/64],Loss: 1.1308, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [7/64],Loss: 1.3839, Acc_digits: 92/128 (72%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [22/25],Step [8/64],Loss: 1.2491, Acc_digits: 92/128 (72%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [22/25],Step [9/64],Loss: 1.1207, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [22/25],Step [10/64],Loss: 1.2090, Acc_digits: 88/128 (69%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [11/64],Loss: 1.0791, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.7717, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [13/64],Loss: 1.2645, Acc_digits: 92/128 (72%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [14/64],Loss: 1.1409, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [15/64],Loss: 1.0035, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [16/40],Loss: 1.0465, Acc_digits: 63/80 (79%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.2705982920015231\n",
      "\n",
      "Test set: Loss: 0.6919,Acc_digits: 1817/2000 (91%),Acc_labels: 818/1000 82%\n",
      "Epoch [23/25],Step [1/64],Loss: 1.0852, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [2/64],Loss: 1.2574, Acc_digits: 85/128 (66%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [3/64],Loss: 1.3987, Acc_digits: 83/128 (65%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [4/64],Loss: 1.0742, Acc_digits: 99/128 (77%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.1730, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [6/64],Loss: 1.2932, Acc_digits: 91/128 (71%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [7/64],Loss: 1.2131, Acc_digits: 98/128 (77%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [23/25],Step [8/64],Loss: 1.1533, Acc_digits: 97/128 (76%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [9/64],Loss: 1.0233, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [10/64],Loss: 1.1665, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [11/64],Loss: 1.0367, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.9008, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [13/64],Loss: 1.1738, Acc_digits: 95/128 (74%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [23/25],Step [14/64],Loss: 1.0047, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [15/64],Loss: 1.1601, Acc_digits: 100/128 (78%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.6211, Acc_digits: 72/80 (90%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.228107230970636\n",
      "\n",
      "Test set: Loss: 0.6730,Acc_digits: 1840/2000 (92%),Acc_labels: 822/1000 82%\n",
      "Epoch [24/25],Step [1/64],Loss: 1.1038, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [2/64],Loss: 1.0991, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.9355, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.9817, Acc_digits: 108/128 (84%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [24/25],Step [5/64],Loss: 1.1025, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [24/25],Step [6/64],Loss: 1.1248, Acc_digits: 103/128 (80%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [24/25],Step [7/64],Loss: 1.3374, Acc_digits: 94/128 (73%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [24/25],Step [8/64],Loss: 1.0154, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [9/64],Loss: 1.1248, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [10/64],Loss: 1.1459, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [11/64],Loss: 1.2709, Acc_digits: 91/128 (71%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [12/64],Loss: 1.0482, Acc_digits: 103/128 (80%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.9854, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [14/64],Loss: 1.0637, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.9783, Acc_digits: 106/128 (83%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.8720, Acc_digits: 65/80 (81%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.21992182498797774\n",
      "\n",
      "Test set: Loss: 0.6652,Acc_digits: 1831/2000 (92%),Acc_labels: 826/1000 83%\n",
      "Epoch [25/25],Step [1/64],Loss: 1.1599, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [2/64],Loss: 1.4005, Acc_digits: 80/128 (62%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [3/64],Loss: 1.1514, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [4/64],Loss: 1.1724, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [5/64],Loss: 1.0845, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.2247, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [7/64],Loss: 1.1898, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.1705, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [25/25],Step [9/64],Loss: 1.1198, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.9747, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [11/64],Loss: 1.2679, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.9975, Acc_digits: 101/128 (79%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [13/64],Loss: 1.0769, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.9873, Acc_digits: 105/128 (82%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.9473, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.9169, Acc_digits: 62/80 (78%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23216410097666085\n",
      "\n",
      "Test set: Loss: 0.7071,Acc_digits: 1830/2000 (92%),Acc_labels: 806/1000 81%\n",
      "Epoch [26/25],Step [1/64],Loss: 1.0181, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [2/64],Loss: 1.2594, Acc_digits: 87/128 (68%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [3/64],Loss: 1.1411, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [4/64],Loss: 1.0793, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.2072, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [6/64],Loss: 1.1877, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [7/64],Loss: 1.3118, Acc_digits: 97/128 (76%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.9920, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [9/64],Loss: 1.3012, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [10/64],Loss: 1.0212, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [11/64],Loss: 1.0834, Acc_digits: 95/128 (74%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.9122, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [13/64],Loss: 1.3594, Acc_digits: 87/128 (68%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [14/64],Loss: 1.0720, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.9729, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.9401, Acc_digits: 64/80 (80%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.2550621209666133\n",
      "\n",
      "Test set: Loss: 0.6726,Acc_digits: 1840/2000 (92%),Acc_labels: 822/1000 82%\n",
      "Epoch [2/25],Step [1/64],Loss: 1.0717, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [2/64],Loss: 1.1624, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.9730, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [4/64],Loss: 1.1095, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [2/25],Step [5/64],Loss: 1.1987, Acc_digits: 105/128 (82%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [6/64],Loss: 1.1071, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [7/64],Loss: 1.1836, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [8/64],Loss: 1.0419, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [2/25],Step [9/64],Loss: 1.0352, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [10/64],Loss: 1.0466, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25],Step [11/64],Loss: 1.1874, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.8686, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [13/64],Loss: 1.0539, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [14/64],Loss: 1.0984, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.9421, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.7947, Acc_digits: 68/80 (85%) ,Acc_labels: 32/40 (80%)\n",
      "Time needed to train  0.28090781904757023\n",
      "\n",
      "Test set: Loss: 0.6592,Acc_digits: 1836/2000 (92%),Acc_labels: 821/1000 82%\n",
      "Epoch [3/25],Step [1/64],Loss: 1.0738, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [3/25],Step [2/64],Loss: 1.1425, Acc_digits: 89/128 (70%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.9695, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.9802, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [5/64],Loss: 1.1203, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [6/64],Loss: 1.2093, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [7/64],Loss: 1.0767, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [8/64],Loss: 1.0123, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.9956, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [10/64],Loss: 1.0389, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [11/64],Loss: 1.1379, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.9939, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [13/64],Loss: 1.1816, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [14/64],Loss: 1.0520, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [15/64],Loss: 1.1771, Acc_digits: 102/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.9351, Acc_digits: 61/80 (76%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2595815929817036\n",
      "\n",
      "Test set: Loss: 0.6691,Acc_digits: 1837/2000 (92%),Acc_labels: 810/1000 81%\n",
      "Epoch [4/25],Step [1/64],Loss: 1.1299, Acc_digits: 91/128 (71%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [2/64],Loss: 1.2243, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [3/64],Loss: 1.1240, Acc_digits: 93/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [4/64],Loss: 1.2194, Acc_digits: 92/128 (72%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [5/64],Loss: 1.2505, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [6/64],Loss: 1.4575, Acc_digits: 87/128 (68%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.9828, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.2785, Acc_digits: 94/128 (73%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [4/25],Step [9/64],Loss: 1.0513, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [10/64],Loss: 1.1323, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.9441, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.8905, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [13/64],Loss: 1.2632, Acc_digits: 86/128 (67%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [14/64],Loss: 1.0426, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.9902, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.9020, Acc_digits: 67/80 (84%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22422374202869833\n",
      "\n",
      "Test set: Loss: 0.6719,Acc_digits: 1821/2000 (91%),Acc_labels: 819/1000 82%\n",
      "Epoch [5/25],Step [1/64],Loss: 1.1144, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [2/64],Loss: 1.2454, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.8208, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.9966, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [5/64],Loss: 1.1007, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [6/64],Loss: 1.1470, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [7/64],Loss: 1.2233, Acc_digits: 92/128 (72%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [8/64],Loss: 1.3671, Acc_digits: 87/128 (68%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.9705, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [10/64],Loss: 1.2667, Acc_digits: 88/128 (69%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [11/64],Loss: 1.1995, Acc_digits: 98/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.9039, Acc_digits: 95/128 (74%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.9428, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [14/64],Loss: 1.0710, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [15/64],Loss: 1.0183, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.7684, Acc_digits: 64/80 (80%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.25795530795585364\n",
      "\n",
      "Test set: Loss: 0.6442,Acc_digits: 1839/2000 (92%),Acc_labels: 838/1000 84%\n",
      "Epoch [6/25],Step [1/64],Loss: 1.0245, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [2/64],Loss: 1.1077, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [3/64],Loss: 1.0171, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.9992, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [5/64],Loss: 1.0599, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.9877, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [7/64],Loss: 1.2090, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.2309, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.9771, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.9489, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.9775, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.8879, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [13/64],Loss: 1.2458, Acc_digits: 92/128 (72%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [14/64],Loss: 1.0740, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [15/64],Loss: 1.0003, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.7630, Acc_digits: 67/80 (84%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.2571904599899426\n",
      "\n",
      "Test set: Loss: 0.6547,Acc_digits: 1845/2000 (92%),Acc_labels: 830/1000 83%\n",
      "Epoch [7/25],Step [1/64],Loss: 1.0997, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [2/64],Loss: 1.2016, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [3/64],Loss: 1.0942, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [4/64],Loss: 1.0027, Acc_digits: 103/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.9640, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [6/64],Loss: 1.1514, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [7/64],Loss: 1.2932, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [8/64],Loss: 1.0176, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.9364, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [10/64],Loss: 1.2182, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [11/64],Loss: 1.2024, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.8644, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [13/64],Loss: 1.1997, Acc_digits: 98/128 (77%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [7/25],Step [14/64],Loss: 1.0091, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [15/64],Loss: 1.0147, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.6737, Acc_digits: 72/80 (90%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24083808599971235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.6322,Acc_digits: 1849/2000 (92%),Acc_labels: 830/1000 83%\n",
      "Epoch [8/25],Step [1/64],Loss: 1.1940, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [2/64],Loss: 1.1521, Acc_digits: 91/128 (71%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.9418, Acc_digits: 95/128 (74%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.9724, Acc_digits: 106/128 (83%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [5/64],Loss: 1.1713, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [6/64],Loss: 1.3329, Acc_digits: 92/128 (72%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [7/64],Loss: 1.1885, Acc_digits: 102/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [8/64],Loss: 1.2752, Acc_digits: 96/128 (75%) ,Acc_labels: 41/64 (64%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.9906, Acc_digits: 97/128 (76%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [10/64],Loss: 1.1291, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [11/64],Loss: 1.2348, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.9296, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.9493, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.9883, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.9372, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.9998, Acc_digits: 62/80 (78%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23675794899463654\n",
      "\n",
      "Test set: Loss: 0.6264,Acc_digits: 1849/2000 (92%),Acc_labels: 827/1000 83%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.8994, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [2/64],Loss: 1.0819, Acc_digits: 91/128 (71%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.8219, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [4/64],Loss: 1.0039, Acc_digits: 101/128 (79%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [5/64],Loss: 1.1398, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [6/64],Loss: 1.0755, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [7/64],Loss: 1.1522, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [8/64],Loss: 1.0250, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [9/64],Loss: 1.1474, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [10/64],Loss: 1.0887, Acc_digits: 91/128 (71%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [11/64],Loss: 1.0602, Acc_digits: 97/128 (76%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.9140, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [13/64],Loss: 1.0926, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.9795, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.9355, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.7961, Acc_digits: 65/80 (81%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23778992297593504\n",
      "\n",
      "Test set: Loss: 0.6179,Acc_digits: 1860/2000 (93%),Acc_labels: 835/1000 84%\n",
      "Epoch [10/25],Step [1/64],Loss: 1.1834, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [2/64],Loss: 1.0491, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.8419, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [4/64],Loss: 1.0943, Acc_digits: 105/128 (82%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.1959, Acc_digits: 103/128 (80%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [10/25],Step [6/64],Loss: 1.1097, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [7/64],Loss: 1.0962, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [8/64],Loss: 1.2630, Acc_digits: 95/128 (74%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [9/64],Loss: 1.0950, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [10/64],Loss: 1.0635, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [11/64],Loss: 1.1908, Acc_digits: 91/128 (71%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.9171, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [13/64],Loss: 1.1772, Acc_digits: 94/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.8864, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.8602, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.5847, Acc_digits: 74/80 (92%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.24190731800626963\n",
      "\n",
      "Test set: Loss: 0.6491,Acc_digits: 1844/2000 (92%),Acc_labels: 832/1000 83%\n",
      "Epoch [11/25],Step [1/64],Loss: 1.1002, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [2/64],Loss: 1.1951, Acc_digits: 91/128 (71%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [3/64],Loss: 1.0335, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [4/64],Loss: 1.2065, Acc_digits: 101/128 (79%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [11/25],Step [5/64],Loss: 1.0294, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [6/64],Loss: 1.0481, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [7/64],Loss: 1.2952, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.2538, Acc_digits: 98/128 (77%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [9/64],Loss: 1.0378, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [10/64],Loss: 1.0642, Acc_digits: 94/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [11/64],Loss: 1.1814, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.8715, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [13/64],Loss: 1.0197, Acc_digits: 101/128 (79%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [11/25],Step [14/64],Loss: 1.1455, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.7506, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.9965, Acc_digits: 65/80 (81%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.261162101989612\n",
      "\n",
      "Test set: Loss: 0.6427,Acc_digits: 1837/2000 (92%),Acc_labels: 824/1000 82%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.9759, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [2/64],Loss: 1.2152, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.9596, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [4/64],Loss: 1.0688, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [5/64],Loss: 1.2596, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [6/64],Loss: 1.3324, Acc_digits: 90/128 (70%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [7/64],Loss: 1.1686, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [8/64],Loss: 1.0810, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.9967, Acc_digits: 97/128 (76%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [10/64],Loss: 1.0562, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [11/64],Loss: 1.0876, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.8207, Acc_digits: 106/128 (83%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [13/64],Loss: 1.0811, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.9004, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.8143, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [16/40],Loss: 1.0933, Acc_digits: 61/80 (76%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.22969838592689484\n",
      "\n",
      "Test set: Loss: 0.6472,Acc_digits: 1841/2000 (92%),Acc_labels: 822/1000 82%\n",
      "Epoch [13/25],Step [1/64],Loss: 1.1519, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [2/64],Loss: 1.1646, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.9013, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [4/64],Loss: 1.0228, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25],Step [5/64],Loss: 1.2317, Acc_digits: 97/128 (76%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [6/64],Loss: 1.0382, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [7/64],Loss: 1.0828, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [8/64],Loss: 1.2205, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.9640, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [10/64],Loss: 1.1266, Acc_digits: 95/128 (74%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [11/64],Loss: 1.0889, Acc_digits: 94/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.8498, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [13/64],Loss: 1.1098, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.9869, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [15/64],Loss: 1.1439, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.7020, Acc_digits: 69/80 (86%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24942367093171924\n",
      "\n",
      "Test set: Loss: 0.6375,Acc_digits: 1835/2000 (92%),Acc_labels: 838/1000 84%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.9543, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [2/64],Loss: 1.0487, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [3/64],Loss: 1.1110, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.9581, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.8290, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [6/64],Loss: 1.1747, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [7/64],Loss: 1.0699, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [8/64],Loss: 1.1893, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.9515, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.9683, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.9351, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.7122, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [13/64],Loss: 1.0800, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.7304, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.9855, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.6562, Acc_digits: 71/80 (89%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.23470627807546407\n",
      "\n",
      "Test set: Loss: 0.6297,Acc_digits: 1844/2000 (92%),Acc_labels: 834/1000 83%\n",
      "Epoch [15/25],Step [1/64],Loss: 1.0419, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [15/25],Step [2/64],Loss: 1.2195, Acc_digits: 90/128 (70%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.9087, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.8705, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [5/64],Loss: 1.1679, Acc_digits: 89/128 (70%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [6/64],Loss: 1.0650, Acc_digits: 109/128 (85%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [15/25],Step [7/64],Loss: 1.1051, Acc_digits: 98/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [8/64],Loss: 1.1859, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [9/64],Loss: 1.0522, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [10/64],Loss: 1.0797, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.9184, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.8937, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [13/64],Loss: 1.0080, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [14/64],Loss: 1.0164, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.7770, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.9221, Acc_digits: 64/80 (80%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.26695327600464225\n",
      "\n",
      "Test set: Loss: 0.6327,Acc_digits: 1835/2000 (92%),Acc_labels: 830/1000 83%\n",
      "Epoch [16/25],Step [1/64],Loss: 1.0690, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.9970, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [3/64],Loss: 1.1174, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.9770, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.9566, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [6/64],Loss: 1.2321, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [7/64],Loss: 1.1980, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [8/64],Loss: 1.1481, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [9/64],Loss: 1.0118, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [10/64],Loss: 1.0186, Acc_digits: 92/128 (72%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [11/64],Loss: 1.0333, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.7904, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [13/64],Loss: 1.0437, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.9392, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.9963, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.7328, Acc_digits: 69/80 (86%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.2904060559812933\n",
      "\n",
      "Test set: Loss: 0.6234,Acc_digits: 1846/2000 (92%),Acc_labels: 819/1000 82%\n",
      "Epoch [17/25],Step [1/64],Loss: 1.2890, Acc_digits: 93/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [2/64],Loss: 1.0733, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.8253, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [4/64],Loss: 1.0352, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [17/25],Step [5/64],Loss: 1.0966, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [6/64],Loss: 1.0805, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [7/64],Loss: 1.2244, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [8/64],Loss: 1.3171, Acc_digits: 88/128 (69%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.8787, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [10/64],Loss: 1.1154, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [11/64],Loss: 1.1149, Acc_digits: 94/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.8646, Acc_digits: 108/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [13/64],Loss: 1.1469, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.9430, Acc_digits: 98/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.9026, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.7408, Acc_digits: 66/80 (82%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.3221900858916342\n",
      "\n",
      "Test set: Loss: 0.6203,Acc_digits: 1868/2000 (93%),Acc_labels: 813/1000 81%\n",
      "Epoch [18/25],Step [1/64],Loss: 1.1539, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [2/64],Loss: 1.1850, Acc_digits: 87/128 (68%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [3/64],Loss: 1.1702, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [4/64],Loss: 1.1348, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.9564, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.9865, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [7/64],Loss: 1.1003, Acc_digits: 107/128 (84%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [8/64],Loss: 1.0062, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [9/64],Loss: 1.0230, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.8460, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.9399, Acc_digits: 103/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.8532, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25],Step [13/64],Loss: 0.9534, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [14/64],Loss: 1.0477, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.8487, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.7129, Acc_digits: 72/80 (90%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.26138092996552587\n",
      "\n",
      "Test set: Loss: 0.5977,Acc_digits: 1862/2000 (93%),Acc_labels: 834/1000 83%\n",
      "Epoch [19/25],Step [1/64],Loss: 1.0699, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [2/64],Loss: 1.1473, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [3/64],Loss: 1.0548, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.8738, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [5/64],Loss: 1.2342, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [6/64],Loss: 1.1524, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [7/64],Loss: 1.0896, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [8/64],Loss: 1.3191, Acc_digits: 91/128 (71%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [9/64],Loss: 1.1385, Acc_digits: 93/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [10/64],Loss: 1.0747, Acc_digits: 95/128 (74%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [11/64],Loss: 1.1112, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [12/64],Loss: 1.0030, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.9715, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.9634, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [15/64],Loss: 1.0489, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.8916, Acc_digits: 63/80 (79%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.26906489999964833\n",
      "\n",
      "Test set: Loss: 0.6390,Acc_digits: 1863/2000 (93%),Acc_labels: 823/1000 82%\n",
      "Epoch [20/25],Step [1/64],Loss: 1.0732, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [2/64],Loss: 1.0631, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [3/64],Loss: 1.0518, Acc_digits: 101/128 (79%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [4/64],Loss: 1.1143, Acc_digits: 101/128 (79%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [5/64],Loss: 1.0428, Acc_digits: 96/128 (75%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.2245, Acc_digits: 98/128 (77%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [20/25],Step [7/64],Loss: 1.0808, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [8/64],Loss: 1.2372, Acc_digits: 92/128 (72%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.9657, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [10/64],Loss: 1.0612, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [11/64],Loss: 1.0008, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.9466, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [13/64],Loss: 1.1218, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.9139, Acc_digits: 111/128 (87%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.9427, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.7251, Acc_digits: 68/80 (85%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.25569339701905847\n",
      "\n",
      "Test set: Loss: 0.6153,Acc_digits: 1836/2000 (92%),Acc_labels: 840/1000 84%\n",
      "Epoch [21/25],Step [1/64],Loss: 1.1305, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [2/64],Loss: 1.2198, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.9153, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [4/64],Loss: 1.0792, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.2779, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [6/64],Loss: 1.0456, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [7/64],Loss: 1.1677, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [8/64],Loss: 1.1145, Acc_digits: 97/128 (76%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [21/25],Step [9/64],Loss: 1.0492, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.8874, Acc_digits: 101/128 (79%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [11/64],Loss: 1.0288, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.9681, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.9137, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [14/64],Loss: 1.0075, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.9289, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.7771, Acc_digits: 68/80 (85%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24507579603232443\n",
      "\n",
      "Test set: Loss: 0.6047,Acc_digits: 1855/2000 (93%),Acc_labels: 833/1000 83%\n",
      "Epoch [22/25],Step [1/64],Loss: 1.0270, Acc_digits: 106/128 (83%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.8435, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.8312, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [4/64],Loss: 1.1034, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.8522, Acc_digits: 103/128 (80%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [22/25],Step [6/64],Loss: 1.0219, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [7/64],Loss: 1.0327, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [8/64],Loss: 1.0712, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [9/64],Loss: 1.0146, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.9146, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [11/64],Loss: 1.0479, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.8636, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.8778, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.8837, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.8688, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.8087, Acc_digits: 65/80 (81%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22419526788871735\n",
      "\n",
      "Test set: Loss: 0.6035,Acc_digits: 1848/2000 (92%),Acc_labels: 825/1000 82%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.9656, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [2/64],Loss: 1.1492, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.8649, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.9799, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.1448, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [6/64],Loss: 1.0847, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [7/64],Loss: 1.0976, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.9929, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [9/64],Loss: 1.0169, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [10/64],Loss: 1.1704, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.9765, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.9757, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [13/64],Loss: 1.0130, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.9458, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.8083, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.8600, Acc_digits: 64/80 (80%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.22480387601535767\n",
      "\n",
      "Test set: Loss: 0.6157,Acc_digits: 1859/2000 (93%),Acc_labels: 816/1000 82%\n",
      "Epoch [24/25],Step [1/64],Loss: 1.0861, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [2/64],Loss: 1.0506, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [3/64],Loss: 1.0898, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.9944, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25],Step [5/64],Loss: 1.2551, Acc_digits: 93/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [6/64],Loss: 1.0448, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [7/64],Loss: 1.0630, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [8/64],Loss: 1.0003, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.8269, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.9926, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [11/64],Loss: 1.1519, Acc_digits: 87/128 (68%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.8909, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [13/64],Loss: 1.1410, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.8323, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.8781, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.7062, Acc_digits: 70/80 (88%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.22630399698391557\n",
      "\n",
      "Test set: Loss: 0.6158,Acc_digits: 1840/2000 (92%),Acc_labels: 834/1000 83%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.8306, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [2/64],Loss: 1.0598, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [3/64],Loss: 1.0114, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.8102, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [5/64],Loss: 1.0614, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.0113, Acc_digits: 104/128 (81%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.9113, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.2373, Acc_digits: 97/128 (76%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [9/64],Loss: 1.1312, Acc_digits: 98/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [10/64],Loss: 1.0772, Acc_digits: 93/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [11/64],Loss: 1.1002, Acc_digits: 97/128 (76%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.8121, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [13/64],Loss: 1.2021, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.9996, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.9988, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.9123, Acc_digits: 64/80 (80%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24632818810641766\n",
      "\n",
      "Test set: Loss: 0.6156,Acc_digits: 1847/2000 (92%),Acc_labels: 837/1000 84%\n",
      "Epoch [26/25],Step [1/64],Loss: 1.1158, Acc_digits: 98/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.8358, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.8881, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [4/64],Loss: 1.0727, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.0440, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [6/64],Loss: 1.1901, Acc_digits: 91/128 (71%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [7/64],Loss: 1.3252, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [8/64],Loss: 1.1150, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.8870, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.9147, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [11/64],Loss: 1.0984, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.9242, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [13/64],Loss: 1.0335, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.9439, Acc_digits: 97/128 (76%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.8921, Acc_digits: 109/128 (85%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.8702, Acc_digits: 63/80 (79%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.26010352896992117\n",
      "\n",
      "Test set: Loss: 0.6175,Acc_digits: 1852/2000 (93%),Acc_labels: 822/1000 82%\n",
      "Epoch [2/25],Step [1/64],Loss: 1.0271, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [2/64],Loss: 1.1208, Acc_digits: 97/128 (76%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.8975, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.9468, Acc_digits: 102/128 (80%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [2/25],Step [5/64],Loss: 1.3146, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.8408, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [7/64],Loss: 1.0779, Acc_digits: 94/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [8/64],Loss: 1.0995, Acc_digits: 95/128 (74%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.8138, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [10/64],Loss: 1.1646, Acc_digits: 91/128 (71%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [11/64],Loss: 1.0843, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.8878, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.9961, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.8803, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.8918, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.5838, Acc_digits: 68/80 (85%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2554092339705676\n",
      "\n",
      "Test set: Loss: 0.5983,Acc_digits: 1864/2000 (93%),Acc_labels: 836/1000 84%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.9903, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.9873, Acc_digits: 98/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [3/64],Loss: 1.0677, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [4/64],Loss: 1.1154, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [3/25],Step [5/64],Loss: 1.0472, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [6/64],Loss: 1.2122, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.9408, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [8/64],Loss: 1.1707, Acc_digits: 97/128 (76%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [3/25],Step [9/64],Loss: 1.0868, Acc_digits: 92/128 (72%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.9361, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.9383, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.8088, Acc_digits: 114/128 (89%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [13/64],Loss: 1.1211, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.8645, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.8738, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.6720, Acc_digits: 68/80 (85%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.24655457597691566\n",
      "\n",
      "Test set: Loss: 0.6194,Acc_digits: 1844/2000 (92%),Acc_labels: 831/1000 83%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.9826, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [2/64],Loss: 1.1953, Acc_digits: 96/128 (75%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.9894, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [4/64],Loss: 1.0751, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [5/64],Loss: 1.0594, Acc_digits: 103/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [4/25],Step [6/64],Loss: 1.3309, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [7/64],Loss: 1.1655, Acc_digits: 97/128 (76%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.0458, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.9383, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [10/64],Loss: 1.0851, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.7706, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [12/64],Loss: 1.0298, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25],Step [13/64],Loss: 1.0677, Acc_digits: 98/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.9920, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.8885, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.7671, Acc_digits: 67/80 (84%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24301369092427194\n",
      "\n",
      "Test set: Loss: 0.6175,Acc_digits: 1851/2000 (93%),Acc_labels: 833/1000 83%\n",
      "Epoch [5/25],Step [1/64],Loss: 1.1356, Acc_digits: 94/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [2/64],Loss: 1.1707, Acc_digits: 93/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.9287, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.8211, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.9196, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [6/64],Loss: 1.1202, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [7/64],Loss: 1.0918, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [8/64],Loss: 1.0332, Acc_digits: 108/128 (84%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.8853, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.9029, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [5/25],Step [11/64],Loss: 1.1647, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [12/64],Loss: 1.0619, Acc_digits: 96/128 (75%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.9171, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.9510, Acc_digits: 94/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.8661, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.6724, Acc_digits: 69/80 (86%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2245820650132373\n",
      "\n",
      "Test set: Loss: 0.5907,Acc_digits: 1863/2000 (93%),Acc_labels: 822/1000 82%\n",
      "Epoch [6/25],Step [1/64],Loss: 1.0162, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [2/64],Loss: 1.1235, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [3/64],Loss: 1.1193, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.8901, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.9915, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [6/64],Loss: 1.1256, Acc_digits: 96/128 (75%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [7/64],Loss: 1.1310, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.0718, Acc_digits: 95/128 (74%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.9763, Acc_digits: 106/128 (83%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [10/64],Loss: 1.0248, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [11/64],Loss: 1.1802, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.8225, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [13/64],Loss: 1.0106, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [6/25],Step [14/64],Loss: 1.0278, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.8311, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.5528, Acc_digits: 70/80 (88%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.23111361905466765\n",
      "\n",
      "Test set: Loss: 0.5840,Acc_digits: 1864/2000 (93%),Acc_labels: 831/1000 83%\n",
      "Epoch [7/25],Step [1/64],Loss: 1.0233, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [2/64],Loss: 1.0698, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.8875, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.9508, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [5/64],Loss: 1.2275, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [6/64],Loss: 1.1689, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.9827, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [8/64],Loss: 1.1588, Acc_digits: 93/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.8621, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.9712, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.8639, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.8599, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [13/64],Loss: 1.1113, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [14/64],Loss: 1.0499, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.7190, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.7870, Acc_digits: 66/80 (82%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22670884802937508\n",
      "\n",
      "Test set: Loss: 0.6040,Acc_digits: 1857/2000 (93%),Acc_labels: 833/1000 83%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.9280, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [2/64],Loss: 1.1577, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.8524, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.9714, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.9606, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [6/64],Loss: 1.1021, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.9834, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [8/64],Loss: 1.1178, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.9430, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.9631, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [11/64],Loss: 1.2577, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.8375, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [13/64],Loss: 1.0425, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [8/25],Step [14/64],Loss: 1.1303, Acc_digits: 101/128 (79%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.9048, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.8283, Acc_digits: 66/80 (82%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.22681636398192495\n",
      "\n",
      "Test set: Loss: 0.5842,Acc_digits: 1873/2000 (94%),Acc_labels: 824/1000 82%\n",
      "Epoch [9/25],Step [1/64],Loss: 1.0119, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.8953, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.9639, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.9065, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [5/64],Loss: 1.1613, Acc_digits: 92/128 (72%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [6/64],Loss: 1.0381, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [7/64],Loss: 1.1594, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [8/64],Loss: 1.1337, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.8996, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.9767, Acc_digits: 94/128 (73%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.8555, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.8179, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [13/64],Loss: 1.0330, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.9721, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.7984, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.6464, Acc_digits: 67/80 (84%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22750875900965184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.6085,Acc_digits: 1851/2000 (93%),Acc_labels: 824/1000 82%\n",
      "Epoch [10/25],Step [1/64],Loss: 1.2621, Acc_digits: 97/128 (76%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.9465, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [3/64],Loss: 1.1777, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.8582, Acc_digits: 111/128 (87%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.0616, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [6/64],Loss: 1.0273, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [7/64],Loss: 1.0405, Acc_digits: 96/128 (75%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [8/64],Loss: 1.0237, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [9/64],Loss: 1.0127, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.9704, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.8708, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.8852, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [13/64],Loss: 1.1797, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [14/64],Loss: 1.1893, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.8494, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.7936, Acc_digits: 65/80 (81%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2334295139880851\n",
      "\n",
      "Test set: Loss: 0.6020,Acc_digits: 1858/2000 (93%),Acc_labels: 842/1000 84%\n",
      "Epoch [11/25],Step [1/64],Loss: 1.1637, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.9167, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.8619, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [4/64],Loss: 1.1545, Acc_digits: 99/128 (77%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.9091, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.9312, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [7/64],Loss: 1.0084, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.0105, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.9759, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [10/64],Loss: 1.0106, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.9028, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.8794, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.7631, Acc_digits: 105/128 (82%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.9132, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [15/64],Loss: 1.0215, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.7665, Acc_digits: 64/80 (80%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22152949403971434\n",
      "\n",
      "Test set: Loss: 0.5998,Acc_digits: 1859/2000 (93%),Acc_labels: 828/1000 83%\n",
      "Epoch [12/25],Step [1/64],Loss: 1.0630, Acc_digits: 102/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [12/25],Step [2/64],Loss: 1.0677, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.8899, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.9804, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [5/64],Loss: 1.0144, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.9886, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [7/64],Loss: 1.0210, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.9423, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.9096, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.9927, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.8794, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.9177, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.9374, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.8484, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.8434, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.6853, Acc_digits: 68/80 (85%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22403527505230159\n",
      "\n",
      "Test set: Loss: 0.5569,Acc_digits: 1859/2000 (93%),Acc_labels: 848/1000 85%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.9964, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.8688, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.8128, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.9760, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.9597, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [6/64],Loss: 1.0027, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.9515, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [8/64],Loss: 1.0752, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [9/64],Loss: 1.1639, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [10/64],Loss: 1.0703, Acc_digits: 95/128 (74%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [13/25],Step [11/64],Loss: 1.1260, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.8588, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.9007, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.9844, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.9482, Acc_digits: 110/128 (86%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.7007, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22633331699762493\n",
      "\n",
      "Test set: Loss: 0.6040,Acc_digits: 1856/2000 (93%),Acc_labels: 842/1000 84%\n",
      "Epoch [14/25],Step [1/64],Loss: 1.1289, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.9708, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.9769, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.8762, Acc_digits: 110/128 (86%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.9371, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.9474, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [7/64],Loss: 1.0794, Acc_digits: 93/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.9695, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.9728, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.8684, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.8881, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.8162, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [13/64],Loss: 1.0663, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.9339, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.8719, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.8763, Acc_digits: 68/80 (85%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.21548624290153384\n",
      "\n",
      "Test set: Loss: 0.5878,Acc_digits: 1870/2000 (94%),Acc_labels: 834/1000 83%\n",
      "Epoch [15/25],Step [1/64],Loss: 1.0226, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.8918, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.9838, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25],Step [4/64],Loss: 0.8020, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.8810, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [6/64],Loss: 1.2203, Acc_digits: 91/128 (71%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [7/64],Loss: 1.1182, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.9466, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.9571, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.8036, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.9589, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.8131, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [13/64],Loss: 1.1650, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.7619, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.6961, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.6623, Acc_digits: 68/80 (85%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.21696741704363376\n",
      "\n",
      "Test set: Loss: 0.5981,Acc_digits: 1854/2000 (93%),Acc_labels: 832/1000 83%\n",
      "Epoch [16/25],Step [1/64],Loss: 1.3034, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [16/25],Step [2/64],Loss: 1.0412, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.7461, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [4/64],Loss: 1.0488, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.9996, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.9644, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.9709, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.9837, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.8208, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.9467, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.8996, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.9223, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.9518, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.9503, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.8973, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.8403, Acc_digits: 68/80 (85%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2440785630606115\n",
      "\n",
      "Test set: Loss: 0.5841,Acc_digits: 1862/2000 (93%),Acc_labels: 834/1000 83%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.8083, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.9708, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.7671, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.9217, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.9177, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.9554, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [7/64],Loss: 1.1873, Acc_digits: 95/128 (74%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.8938, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [9/64],Loss: 1.0426, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.8470, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.9062, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.8326, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [13/64],Loss: 1.0483, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.7118, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [17/25],Step [15/64],Loss: 1.0016, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.8647, Acc_digits: 65/80 (81%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.23541920201387256\n",
      "\n",
      "Test set: Loss: 0.5552,Acc_digits: 1877/2000 (94%),Acc_labels: 840/1000 84%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.9713, Acc_digits: 106/128 (83%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [2/64],Loss: 1.0168, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [3/64],Loss: 1.0970, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [4/64],Loss: 1.0101, Acc_digits: 101/128 (79%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.9387, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.9428, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [7/64],Loss: 1.0519, Acc_digits: 94/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [8/64],Loss: 1.1734, Acc_digits: 97/128 (76%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.7957, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [10/64],Loss: 1.0638, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.8406, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.7403, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.8378, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.9901, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.9301, Acc_digits: 99/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.9881, Acc_digits: 66/80 (82%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.2531119950581342\n",
      "\n",
      "Test set: Loss: 0.5999,Acc_digits: 1853/2000 (93%),Acc_labels: 828/1000 83%\n",
      "Epoch [19/25],Step [1/64],Loss: 1.0878, Acc_digits: 107/128 (84%) ,Acc_labels: 46/64 (72%)\n",
      "Epoch [19/25],Step [2/64],Loss: 1.0560, Acc_digits: 93/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.9501, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.8084, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [5/64],Loss: 1.1940, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [6/64],Loss: 1.0096, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.9902, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [8/64],Loss: 1.2523, Acc_digits: 97/128 (76%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.8573, Acc_digits: 110/128 (86%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.7584, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [11/64],Loss: 1.0167, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [12/64],Loss: 1.0548, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.7885, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.7408, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.9055, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.7021, Acc_digits: 66/80 (82%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.328258675057441\n",
      "\n",
      "Test set: Loss: 0.5724,Acc_digits: 1862/2000 (93%),Acc_labels: 823/1000 82%\n",
      "Epoch [20/25],Step [1/64],Loss: 1.0377, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.9487, Acc_digits: 95/128 (74%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.8001, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.9884, Acc_digits: 106/128 (83%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [20/25],Step [5/64],Loss: 1.1590, Acc_digits: 93/128 (73%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.0512, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [7/64],Loss: 1.1521, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.9902, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.7713, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.9757, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.7639, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.8781, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.8588, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25],Step [14/64],Loss: 0.7268, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.8873, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.8069, Acc_digits: 68/80 (85%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2468494459753856\n",
      "\n",
      "Test set: Loss: 0.5894,Acc_digits: 1858/2000 (93%),Acc_labels: 836/1000 84%\n",
      "Epoch [21/25],Step [1/64],Loss: 1.1614, Acc_digits: 98/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [21/25],Step [2/64],Loss: 1.1763, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [3/64],Loss: 1.1402, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [4/64],Loss: 1.0209, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.0235, Acc_digits: 104/128 (81%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.9286, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [7/64],Loss: 1.0253, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [8/64],Loss: 1.0012, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.7573, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.8905, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.8901, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.7770, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.9498, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.8341, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.9073, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.6424, Acc_digits: 70/80 (88%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2340885290177539\n",
      "\n",
      "Test set: Loss: 0.5990,Acc_digits: 1873/2000 (94%),Acc_labels: 816/1000 82%\n",
      "Epoch [22/25],Step [1/64],Loss: 1.0693, Acc_digits: 93/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.9612, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.9893, Acc_digits: 93/128 (73%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.8927, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [5/64],Loss: 1.1359, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [6/64],Loss: 1.0582, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.9905, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.9353, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.8242, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.9160, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.9797, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.9929, Acc_digits: 105/128 (82%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.9992, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.9968, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [15/64],Loss: 1.0759, Acc_digits: 99/128 (77%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.6302, Acc_digits: 70/80 (88%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.24593380303122103\n",
      "\n",
      "Test set: Loss: 0.5955,Acc_digits: 1858/2000 (93%),Acc_labels: 838/1000 84%\n",
      "Epoch [23/25],Step [1/64],Loss: 1.0516, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.9398, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.7345, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.9990, Acc_digits: 104/128 (81%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.0715, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.8335, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [7/64],Loss: 1.0816, Acc_digits: 94/128 (73%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.9890, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.8798, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.9010, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.8861, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.7389, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.9943, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.8361, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.7178, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.7851, Acc_digits: 66/80 (82%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.24334471893962473\n",
      "\n",
      "Test set: Loss: 0.5567,Acc_digits: 1878/2000 (94%),Acc_labels: 841/1000 84%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.8022, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.9168, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.8437, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [4/64],Loss: 1.0494, Acc_digits: 104/128 (81%) ,Acc_labels: 44/64 (69%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.9375, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.9477, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [7/64],Loss: 1.0267, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.9030, Acc_digits: 109/128 (85%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.8395, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.9040, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.9862, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.9378, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [13/64],Loss: 1.0032, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.8849, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.8349, Acc_digits: 109/128 (85%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.4921, Acc_digits: 76/80 (95%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.27841856807935983\n",
      "\n",
      "Test set: Loss: 0.5691,Acc_digits: 1880/2000 (94%),Acc_labels: 834/1000 83%\n",
      "Epoch [25/25],Step [1/64],Loss: 1.0115, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [2/64],Loss: 1.1017, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.8209, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.8802, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.9682, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.0539, Acc_digits: 96/128 (75%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [7/64],Loss: 1.0183, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.1322, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.8925, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [10/64],Loss: 1.0323, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [11/64],Loss: 1.1754, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.9743, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.8241, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.7492, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.8354, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.6971, Acc_digits: 68/80 (85%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2886766840238124\n",
      "\n",
      "Test set: Loss: 0.5543,Acc_digits: 1878/2000 (94%),Acc_labels: 847/1000 85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/25],Step [1/64],Loss: 1.0173, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.9744, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.7732, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.9701, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.0994, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.9551, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.8277, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [8/64],Loss: 1.0030, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.7476, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [26/25],Step [10/64],Loss: 1.0370, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.7916, Acc_digits: 112/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.9176, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.8944, Acc_digits: 96/128 (75%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.9267, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.6881, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.6058, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2710364100057632\n",
      "\n",
      "Test set: Loss: 0.5687,Acc_digits: 1861/2000 (93%),Acc_labels: 847/1000 85%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.9495, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.9430, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.7751, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.8833, Acc_digits: 111/128 (87%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.9658, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.9129, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [7/64],Loss: 1.2241, Acc_digits: 89/128 (70%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.9130, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.9550, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.8706, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.9758, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.6261, Acc_digits: 106/128 (83%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [2/25],Step [13/64],Loss: 1.0839, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.9036, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.8088, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.5625, Acc_digits: 73/80 (91%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.27281162794679403\n",
      "\n",
      "Test set: Loss: 0.5546,Acc_digits: 1874/2000 (94%),Acc_labels: 840/1000 84%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.8723, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.8516, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [3/64],Loss: 1.0787, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.9450, Acc_digits: 110/128 (86%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [5/64],Loss: 1.2206, Acc_digits: 95/128 (74%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [6/64],Loss: 1.1233, Acc_digits: 97/128 (76%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.9423, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [8/64],Loss: 1.1484, Acc_digits: 101/128 (79%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.9759, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [10/64],Loss: 1.0584, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.9984, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.8527, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [13/64],Loss: 1.0234, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [14/64],Loss: 1.0454, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.9058, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.6130, Acc_digits: 70/80 (88%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2637513510417193\n",
      "\n",
      "Test set: Loss: 0.5575,Acc_digits: 1872/2000 (94%),Acc_labels: 834/1000 83%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.8745, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [2/64],Loss: 1.1010, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.8316, Acc_digits: 102/128 (80%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.9332, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.8649, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [6/64],Loss: 1.0069, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.9850, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.0950, Acc_digits: 93/128 (73%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.8767, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.8212, Acc_digits: 99/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.6784, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.7247, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.9071, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.8370, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [15/64],Loss: 1.0383, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.4744, Acc_digits: 73/80 (91%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2325254559982568\n",
      "\n",
      "Test set: Loss: 0.5379,Acc_digits: 1879/2000 (94%),Acc_labels: 846/1000 85%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.9640, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.8677, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.6796, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.9123, Acc_digits: 106/128 (83%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.9729, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [6/64],Loss: 1.2868, Acc_digits: 90/128 (70%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.9057, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [8/64],Loss: 1.1114, Acc_digits: 97/128 (76%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.9353, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [10/64],Loss: 1.0082, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.8714, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.8594, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.8194, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.7519, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.8239, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.7388, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.30634179897606373\n",
      "\n",
      "Test set: Loss: 0.5609,Acc_digits: 1858/2000 (93%),Acc_labels: 835/1000 84%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.9479, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.9019, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [3/64],Loss: 1.0220, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.8620, Acc_digits: 110/128 (86%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [5/64],Loss: 1.0800, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [6/64],Loss: 1.0835, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.9826, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.1132, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.8642, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.6735, Acc_digits: 102/128 (80%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [11/64],Loss: 1.0011, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25],Step [12/64],Loss: 0.8692, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [6/25],Step [13/64],Loss: 1.0764, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.8224, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.9587, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.6077, Acc_digits: 70/80 (88%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.2807130789151415\n",
      "\n",
      "Test set: Loss: 0.5419,Acc_digits: 1873/2000 (94%),Acc_labels: 858/1000 86%\n",
      "Epoch [7/25],Step [1/64],Loss: 1.0189, Acc_digits: 102/128 (80%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.9377, Acc_digits: 99/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.8742, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.9123, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [5/64],Loss: 1.0761, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [6/64],Loss: 1.0774, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [7/64],Loss: 1.0258, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [8/64],Loss: 1.1252, Acc_digits: 96/128 (75%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.8367, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.9138, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [11/64],Loss: 1.1049, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.9097, Acc_digits: 110/128 (86%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [13/64],Loss: 1.0231, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.9875, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.8460, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.8617, Acc_digits: 66/80 (82%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24023810494691133\n",
      "\n",
      "Test set: Loss: 0.5469,Acc_digits: 1882/2000 (94%),Acc_labels: 852/1000 85%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.8235, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [2/64],Loss: 1.0141, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.8156, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.9313, Acc_digits: 105/128 (82%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.9092, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.8395, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [7/64],Loss: 1.1843, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [8/64],Loss: 1.1094, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.9099, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.7972, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.9006, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.8164, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.8515, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.8279, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.7920, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.7511, Acc_digits: 65/80 (81%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.2298986769746989\n",
      "\n",
      "Test set: Loss: 0.5469,Acc_digits: 1874/2000 (94%),Acc_labels: 861/1000 86%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.9095, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.9719, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [3/64],Loss: 1.0074, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.7038, Acc_digits: 114/128 (89%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.9596, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [6/64],Loss: 1.0985, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.6860, Acc_digits: 112/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [8/64],Loss: 1.1152, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.7486, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.8422, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.9419, Acc_digits: 98/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.8730, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.9309, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.7646, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.8287, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.7811, Acc_digits: 67/80 (84%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22986226493958384\n",
      "\n",
      "Test set: Loss: 0.5380,Acc_digits: 1872/2000 (94%),Acc_labels: 855/1000 86%\n",
      "Epoch [10/25],Step [1/64],Loss: 1.0017, Acc_digits: 106/128 (83%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.9499, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.8284, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [4/64],Loss: 1.0362, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.0454, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [6/64],Loss: 1.0588, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.7353, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.9411, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.8276, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.9541, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.7750, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.5846, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [13/64],Loss: 1.2279, Acc_digits: 101/128 (79%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.9159, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.7939, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.7289, Acc_digits: 67/80 (84%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23345031903591007\n",
      "\n",
      "Test set: Loss: 0.5529,Acc_digits: 1871/2000 (94%),Acc_labels: 848/1000 85%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.7766, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.8990, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.9303, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.9446, Acc_digits: 109/128 (85%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.9162, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [6/64],Loss: 1.2332, Acc_digits: 94/128 (73%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.9789, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.1069, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.7284, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.9123, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.9697, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.8333, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.9785, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.8211, Acc_digits: 110/128 (86%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.8386, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.7114, Acc_digits: 65/80 (81%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2568991440348327\n",
      "\n",
      "Test set: Loss: 0.5574,Acc_digits: 1877/2000 (94%),Acc_labels: 841/1000 84%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.6978, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.8752, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25],Step [3/64],Loss: 0.8858, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.7174, Acc_digits: 110/128 (86%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [5/64],Loss: 1.0912, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [6/64],Loss: 1.0815, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.9139, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [8/64],Loss: 1.0368, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.8657, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.7789, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.9181, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.9126, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [13/64],Loss: 1.0845, Acc_digits: 102/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.8433, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.8695, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.4967, Acc_digits: 71/80 (89%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.24432858603540808\n",
      "\n",
      "Test set: Loss: 0.5495,Acc_digits: 1873/2000 (94%),Acc_labels: 847/1000 85%\n",
      "Epoch [13/25],Step [1/64],Loss: 1.0054, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.8043, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [3/64],Loss: 1.1628, Acc_digits: 96/128 (75%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.8734, Acc_digits: 107/128 (84%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.7772, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.9910, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [7/64],Loss: 1.1389, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.9251, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.8072, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.9993, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.6863, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.7603, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.9031, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.9871, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.9330, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.7564, Acc_digits: 67/80 (84%) ,Acc_labels: 40/40 (100%)\n",
      "Time needed to train  0.24351540801580995\n",
      "\n",
      "Test set: Loss: 0.5512,Acc_digits: 1873/2000 (94%),Acc_labels: 839/1000 84%\n",
      "Epoch [14/25],Step [1/64],Loss: 1.0406, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [2/64],Loss: 1.0600, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.8055, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.7878, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [5/64],Loss: 1.0803, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.9486, Acc_digits: 98/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.7941, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.9690, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [9/64],Loss: 1.0170, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.8916, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.9883, Acc_digits: 92/128 (72%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.9022, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [13/64],Loss: 1.1106, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.9204, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.7383, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.5661, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2588870369363576\n",
      "\n",
      "Test set: Loss: 0.5556,Acc_digits: 1879/2000 (94%),Acc_labels: 843/1000 84%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.8298, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.8972, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.7218, Acc_digits: 110/128 (86%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.8489, Acc_digits: 113/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.9162, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [6/64],Loss: 1.0248, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.9161, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.8343, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.8194, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.7470, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.8145, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.7172, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.7532, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.7104, Acc_digits: 114/128 (89%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.7485, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.4675, Acc_digits: 73/80 (91%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2259228229522705\n",
      "\n",
      "Test set: Loss: 0.5512,Acc_digits: 1874/2000 (94%),Acc_labels: 840/1000 84%\n",
      "Epoch [16/25],Step [1/64],Loss: 1.0982, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.8308, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.8275, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.7929, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [5/64],Loss: 1.0676, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.9524, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.9802, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [8/64],Loss: 1.0173, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.7072, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.8740, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.9285, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.6200, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.9553, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.6725, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.8639, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.7889, Acc_digits: 63/80 (79%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.24671943101566285\n",
      "\n",
      "Test set: Loss: 0.5480,Acc_digits: 1862/2000 (93%),Acc_labels: 845/1000 84%\n",
      "Epoch [17/25],Step [1/64],Loss: 1.0107, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [2/64],Loss: 1.1252, Acc_digits: 91/128 (71%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.7111, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.8521, Acc_digits: 113/128 (88%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.9020, Acc_digits: 99/128 (77%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.9467, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.8653, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [8/64],Loss: 1.0749, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.9862, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.9032, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.9690, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.8114, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.9166, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25],Step [14/64],Loss: 0.7125, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.7264, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.4886, Acc_digits: 72/80 (90%) ,Acc_labels: 40/40 (100%)\n",
      "Time needed to train  0.2451030679512769\n",
      "\n",
      "Test set: Loss: 0.5587,Acc_digits: 1875/2000 (94%),Acc_labels: 833/1000 83%\n",
      "Epoch [18/25],Step [1/64],Loss: 1.0189, Acc_digits: 93/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.8444, Acc_digits: 103/128 (80%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.8309, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.8398, Acc_digits: 113/128 (88%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.9328, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [6/64],Loss: 1.0689, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.9540, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [8/64],Loss: 1.0534, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.8675, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.6414, Acc_digits: 109/128 (85%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.7399, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.7992, Acc_digits: 98/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [13/64],Loss: 1.0059, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.7733, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.7666, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.9019, Acc_digits: 70/80 (88%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23827563994564116\n",
      "\n",
      "Test set: Loss: 0.5560,Acc_digits: 1879/2000 (94%),Acc_labels: 843/1000 84%\n",
      "Epoch [19/25],Step [1/64],Loss: 1.0244, Acc_digits: 103/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.8527, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.8294, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.7454, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [5/64],Loss: 1.0153, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.9592, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.8172, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [8/64],Loss: 1.1089, Acc_digits: 101/128 (79%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.8256, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [10/64],Loss: 1.0056, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.8415, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.7818, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [13/64],Loss: 1.0495, Acc_digits: 104/128 (81%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.6820, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.7470, Acc_digits: 114/128 (89%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.6111, Acc_digits: 71/80 (89%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2475652559660375\n",
      "\n",
      "Test set: Loss: 0.5275,Acc_digits: 1884/2000 (94%),Acc_labels: 851/1000 85%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.9361, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.9895, Acc_digits: 97/128 (76%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.8285, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.8168, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [5/64],Loss: 1.1375, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.1509, Acc_digits: 98/128 (77%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.8843, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [8/64],Loss: 1.1032, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.8017, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.9363, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.8195, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.5723, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.9456, Acc_digits: 111/128 (87%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.8951, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.7354, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.5761, Acc_digits: 66/80 (82%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.23728914896491915\n",
      "\n",
      "Test set: Loss: 0.5529,Acc_digits: 1873/2000 (94%),Acc_labels: 852/1000 85%\n",
      "Epoch [21/25],Step [1/64],Loss: 1.0592, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.9184, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.8729, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.8596, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.2274, Acc_digits: 96/128 (75%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.9772, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [7/64],Loss: 1.1345, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [8/64],Loss: 1.0409, Acc_digits: 99/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.9705, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.8195, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.7179, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.8352, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.7847, Acc_digits: 99/128 (77%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.8915, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.8291, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.5018, Acc_digits: 70/80 (88%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.226893455022946\n",
      "\n",
      "Test set: Loss: 0.5378,Acc_digits: 1870/2000 (94%),Acc_labels: 849/1000 85%\n",
      "Epoch [22/25],Step [1/64],Loss: 1.0616, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.7184, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.9607, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.8899, Acc_digits: 106/128 (83%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [22/25],Step [5/64],Loss: 1.0704, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.8316, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.9075, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.9058, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.6444, Acc_digits: 116/128 (91%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.9489, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.8429, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.7422, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.9169, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.8698, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.6494, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.7215, Acc_digits: 68/80 (85%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22937582503072917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.5460,Acc_digits: 1879/2000 (94%),Acc_labels: 837/1000 84%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.9557, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [2/64],Loss: 1.0761, Acc_digits: 97/128 (76%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [3/64],Loss: 1.0079, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.9644, Acc_digits: 107/128 (84%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.0231, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [6/64],Loss: 1.0947, Acc_digits: 95/128 (74%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.9496, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.9968, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.8660, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.9000, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.8342, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.8590, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.9251, Acc_digits: 105/128 (82%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.7181, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.6196, Acc_digits: 116/128 (91%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.6608, Acc_digits: 71/80 (89%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2302654410013929\n",
      "\n",
      "Test set: Loss: 0.5436,Acc_digits: 1878/2000 (94%),Acc_labels: 840/1000 84%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.7848, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.9758, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.8096, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.8453, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.8103, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.8806, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.9703, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.8655, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.7154, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.9042, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.8820, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.8079, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [13/64],Loss: 1.0240, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.8102, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.6831, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.6706, Acc_digits: 68/80 (85%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22910285007674247\n",
      "\n",
      "Test set: Loss: 0.5571,Acc_digits: 1880/2000 (94%),Acc_labels: 827/1000 83%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.8623, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.9915, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [3/64],Loss: 1.0590, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.8775, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.9716, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.1444, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [7/64],Loss: 1.1344, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.0814, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.8089, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.9254, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.8176, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.6986, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.8183, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.7602, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.7060, Acc_digits: 114/128 (89%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.7209, Acc_digits: 68/80 (85%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.23332637001294643\n",
      "\n",
      "Test set: Loss: 0.5345,Acc_digits: 1887/2000 (94%),Acc_labels: 842/1000 84%\n",
      "Epoch [26/25],Step [1/64],Loss: 1.0937, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.9337, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.8874, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.7915, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.1057, Acc_digits: 97/128 (76%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.9087, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.7969, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.8617, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.8374, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.9620, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.9849, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.7434, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.8579, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.7273, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.7591, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.6533, Acc_digits: 71/80 (89%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22232585994061083\n",
      "\n",
      "Test set: Loss: 0.5342,Acc_digits: 1879/2000 (94%),Acc_labels: 849/1000 85%\n",
      "Epoch [2/25],Step [1/64],Loss: 1.0071, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.8921, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.7622, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.6616, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.7727, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [6/64],Loss: 1.0841, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.7437, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.9750, Acc_digits: 104/128 (81%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.7517, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.7173, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.8258, Acc_digits: 99/128 (77%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.8501, Acc_digits: 109/128 (85%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.8607, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.8546, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.8559, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.6776, Acc_digits: 71/80 (89%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.2279121520696208\n",
      "\n",
      "Test set: Loss: 0.5501,Acc_digits: 1870/2000 (94%),Acc_labels: 831/1000 83%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.7231, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.8498, Acc_digits: 98/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.8483, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.8102, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [5/64],Loss: 1.1937, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.9609, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [3/25],Step [7/64],Loss: 1.0676, Acc_digits: 93/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.9478, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.6710, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.9449, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.9464, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.8028, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.7240, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.7510, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.8535, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.7112, Acc_digits: 69/80 (86%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.23834421602077782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.5265,Acc_digits: 1879/2000 (94%),Acc_labels: 854/1000 85%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.9884, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.9172, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.7218, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.9897, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.9999, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.8741, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.8092, Acc_digits: 103/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.0753, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.5516, Acc_digits: 117/128 (91%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.9239, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.8888, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.7690, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.8992, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.7878, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.8113, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.6566, Acc_digits: 67/80 (84%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.23378685698844492\n",
      "\n",
      "Test set: Loss: 0.5515,Acc_digits: 1872/2000 (94%),Acc_labels: 833/1000 83%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.9409, Acc_digits: 105/128 (82%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.8920, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.8503, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.8996, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.9183, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.9071, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.8846, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.9910, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.7265, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.7942, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.9075, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.6026, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.9256, Acc_digits: 97/128 (76%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.7658, Acc_digits: 112/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.8075, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.8648, Acc_digits: 66/80 (82%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.22971303493250161\n",
      "\n",
      "Test set: Loss: 0.5214,Acc_digits: 1882/2000 (94%),Acc_labels: 850/1000 85%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.8046, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.7829, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.7414, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.7952, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.8362, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.8896, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.7302, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.0152, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.6255, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [10/64],Loss: 1.0174, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [11/64],Loss: 1.0341, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.6543, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.7465, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.9719, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.7780, Acc_digits: 103/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.4214, Acc_digits: 73/80 (91%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.23896889598108828\n",
      "\n",
      "Test set: Loss: 0.5582,Acc_digits: 1868/2000 (93%),Acc_labels: 843/1000 84%\n",
      "Epoch [7/25],Step [1/64],Loss: 1.0126, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [2/64],Loss: 1.0663, Acc_digits: 94/128 (73%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [3/64],Loss: 1.0082, Acc_digits: 108/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.9889, Acc_digits: 101/128 (79%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.9425, Acc_digits: 105/128 (82%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.8692, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.8114, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [8/64],Loss: 1.0412, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.8528, Acc_digits: 97/128 (76%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.8476, Acc_digits: 101/128 (79%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.7146, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.7005, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.8757, Acc_digits: 103/128 (80%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.9077, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.6903, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.9007, Acc_digits: 66/80 (82%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.23523064597975463\n",
      "\n",
      "Test set: Loss: 0.5302,Acc_digits: 1881/2000 (94%),Acc_labels: 844/1000 84%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.9100, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.8186, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.8818, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.7286, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [5/64],Loss: 1.1356, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.9910, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.7659, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.9912, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.7270, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.6759, Acc_digits: 111/128 (87%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.8445, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.6842, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.8456, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.6615, Acc_digits: 109/128 (85%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.8363, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.6153, Acc_digits: 71/80 (89%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.22755617904476821\n",
      "\n",
      "Test set: Loss: 0.5064,Acc_digits: 1892/2000 (95%),Acc_labels: 867/1000 87%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.9098, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.9882, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.8734, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.9504, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.9666, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.8352, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.9587, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.8948, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.7505, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.6595, Acc_digits: 111/128 (87%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.8878, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.8271, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.8755, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.6347, Acc_digits: 114/128 (89%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.9646, Acc_digits: 112/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.9000, Acc_digits: 66/80 (82%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22589487594086677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.4990,Acc_digits: 1884/2000 (94%),Acc_labels: 867/1000 87%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.7693, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.7444, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.7682, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.8594, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.0587, Acc_digits: 90/128 (70%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.8727, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [7/64],Loss: 1.0143, Acc_digits: 93/128 (73%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [8/64],Loss: 1.0672, Acc_digits: 102/128 (80%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.8729, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.8466, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.7344, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.8269, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.7567, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.7959, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.9490, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.5251, Acc_digits: 71/80 (89%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22161131293978542\n",
      "\n",
      "Test set: Loss: 0.5059,Acc_digits: 1883/2000 (94%),Acc_labels: 859/1000 86%\n",
      "Epoch [11/25],Step [1/64],Loss: 1.0589, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.8453, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.8212, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.9283, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.9525, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.7537, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.9289, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.2055, Acc_digits: 92/128 (72%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.8080, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.9175, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.8940, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.8243, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.8069, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.6906, Acc_digits: 112/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.7174, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.7917, Acc_digits: 63/80 (79%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22365794703364372\n",
      "\n",
      "Test set: Loss: 0.5254,Acc_digits: 1885/2000 (94%),Acc_labels: 847/1000 85%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.8843, Acc_digits: 109/128 (85%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [12/25],Step [2/64],Loss: 1.0661, Acc_digits: 91/128 (71%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.8106, Acc_digits: 100/128 (78%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.7710, Acc_digits: 112/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.8822, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.8657, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.8836, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [12/25],Step [8/64],Loss: 1.1766, Acc_digits: 96/128 (75%) ,Acc_labels: 45/64 (70%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.7252, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.9626, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.9209, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.6917, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.8048, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.7462, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.8521, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.7311, Acc_digits: 60/80 (75%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22498773701954633\n",
      "\n",
      "Test set: Loss: 0.5221,Acc_digits: 1878/2000 (94%),Acc_labels: 855/1000 86%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.7454, Acc_digits: 103/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.8143, Acc_digits: 99/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.6433, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.9345, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.8982, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [6/64],Loss: 1.1710, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.7711, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.9000, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.8861, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.8565, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.8752, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.6806, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.9155, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.9093, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.6870, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.7105, Acc_digits: 64/80 (80%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22524779301602393\n",
      "\n",
      "Test set: Loss: 0.5208,Acc_digits: 1886/2000 (94%),Acc_labels: 848/1000 85%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.8088, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.9636, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.6856, Acc_digits: 100/128 (78%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.7955, Acc_digits: 115/128 (90%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.8323, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.9439, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.8520, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.8059, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.7865, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.7192, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.7417, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.8533, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.8679, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.9020, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.7082, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.6875, Acc_digits: 67/80 (84%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22070020192768425\n",
      "\n",
      "Test set: Loss: 0.5021,Acc_digits: 1879/2000 (94%),Acc_labels: 862/1000 86%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.8838, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.7437, Acc_digits: 103/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.9901, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.7716, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25],Step [5/64],Loss: 1.0586, Acc_digits: 106/128 (83%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.7955, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.9841, Acc_digits: 94/128 (73%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.9386, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.8955, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.9445, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.7038, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.6138, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.8302, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.8050, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.7437, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.5959, Acc_digits: 70/80 (88%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.23110486404038966\n",
      "\n",
      "Test set: Loss: 0.5021,Acc_digits: 1881/2000 (94%),Acc_labels: 848/1000 85%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.7886, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [2/64],Loss: 1.0243, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.8044, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.8565, Acc_digits: 109/128 (85%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [16/25],Step [5/64],Loss: 1.0021, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [6/64],Loss: 1.0636, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.9386, Acc_digits: 100/128 (78%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.9488, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.7137, Acc_digits: 109/128 (85%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.8824, Acc_digits: 97/128 (76%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.8400, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.9552, Acc_digits: 107/128 (84%) ,Acc_labels: 48/64 (75%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.7118, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.8107, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.6528, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.5853, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2380990319652483\n",
      "\n",
      "Test set: Loss: 0.5065,Acc_digits: 1883/2000 (94%),Acc_labels: 854/1000 85%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.9479, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.8810, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.8406, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.7907, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.9827, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.7066, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.8293, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.8751, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.8647, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.6595, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.8370, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.5991, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.7271, Acc_digits: 105/128 (82%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.8100, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.7399, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.7957, Acc_digits: 63/80 (79%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22070505504962057\n",
      "\n",
      "Test set: Loss: 0.5021,Acc_digits: 1877/2000 (94%),Acc_labels: 867/1000 87%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.8007, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.9624, Acc_digits: 100/128 (78%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.7785, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.9653, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.9148, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [6/64],Loss: 1.1605, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.8889, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.7904, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.7317, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [10/64],Loss: 1.1095, Acc_digits: 96/128 (75%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.8414, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.7847, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.7374, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.8761, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.6185, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.4404, Acc_digits: 75/80 (94%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.21585817798040807\n",
      "\n",
      "Test set: Loss: 0.5579,Acc_digits: 1870/2000 (94%),Acc_labels: 833/1000 83%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.9144, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.9236, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.8266, Acc_digits: 99/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.7070, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.7707, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.7361, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.8996, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.8583, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.6865, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.7898, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [11/64],Loss: 1.0287, Acc_digits: 96/128 (75%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.7986, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.9302, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.7807, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.8394, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.5284, Acc_digits: 71/80 (89%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.23259674198925495\n",
      "\n",
      "Test set: Loss: 0.5101,Acc_digits: 1880/2000 (94%),Acc_labels: 854/1000 85%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.8311, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.9572, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.6435, Acc_digits: 114/128 (89%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.8066, Acc_digits: 116/128 (91%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.7488, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [6/64],Loss: 1.0443, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.9242, Acc_digits: 111/128 (87%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.9049, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.7399, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.7314, Acc_digits: 103/128 (80%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.8158, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.6559, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.7212, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25],Step [14/64],Loss: 0.6535, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.8413, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.6913, Acc_digits: 67/80 (84%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.226751992944628\n",
      "\n",
      "Test set: Loss: 0.5087,Acc_digits: 1886/2000 (94%),Acc_labels: 856/1000 86%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.9344, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.8009, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.9750, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.7710, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [5/64],Loss: 1.0275, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.8100, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.7783, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.9137, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [9/64],Loss: 1.0731, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.8546, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.8737, Acc_digits: 100/128 (78%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.7326, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.7882, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.6756, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.7632, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.4876, Acc_digits: 72/80 (90%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22133451502304524\n",
      "\n",
      "Test set: Loss: 0.5109,Acc_digits: 1872/2000 (94%),Acc_labels: 875/1000 88%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.8462, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [2/64],Loss: 1.0317, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.8074, Acc_digits: 100/128 (78%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.7112, Acc_digits: 115/128 (90%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.9021, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.9354, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.8441, Acc_digits: 103/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.8134, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.8148, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.9564, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [11/64],Loss: 1.0193, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.8188, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.9890, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.6960, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.7744, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.6865, Acc_digits: 69/80 (86%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2271392900729552\n",
      "\n",
      "Test set: Loss: 0.5368,Acc_digits: 1879/2000 (94%),Acc_labels: 846/1000 85%\n",
      "Epoch [23/25],Step [1/64],Loss: 1.0460, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.9512, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.8320, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.7497, Acc_digits: 114/128 (89%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [5/64],Loss: 1.0651, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.9500, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.9918, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.9559, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.7451, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.9307, Acc_digits: 96/128 (75%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.9021, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.6265, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.9486, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.7559, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.7110, Acc_digits: 110/128 (86%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.7169, Acc_digits: 67/80 (84%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.25984772003721446\n",
      "\n",
      "Test set: Loss: 0.5229,Acc_digits: 1879/2000 (94%),Acc_labels: 854/1000 85%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.6939, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.8987, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.7452, Acc_digits: 106/128 (83%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.9110, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.8015, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.8086, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.8484, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.9040, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.8876, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.8392, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.7989, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.7444, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.8432, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.5791, Acc_digits: 115/128 (90%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.6883, Acc_digits: 113/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.6263, Acc_digits: 74/80 (92%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.22408175095915794\n",
      "\n",
      "Test set: Loss: 0.5584,Acc_digits: 1868/2000 (93%),Acc_labels: 833/1000 83%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.8955, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.8513, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.7134, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.8841, Acc_digits: 117/128 (91%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [5/64],Loss: 1.0166, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [6/64],Loss: 1.0949, Acc_digits: 99/128 (77%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.8754, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [8/64],Loss: 1.0707, Acc_digits: 104/128 (81%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.7353, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.8039, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.6641, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.8780, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.7986, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.7845, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.6896, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.6523, Acc_digits: 71/80 (89%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22175801696721464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.5154,Acc_digits: 1877/2000 (94%),Acc_labels: 860/1000 86%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.8251, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.9582, Acc_digits: 99/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.7023, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [4/64],Loss: 1.0357, Acc_digits: 100/128 (78%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.1115, Acc_digits: 94/128 (73%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.9350, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.7647, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.8668, Acc_digits: 101/128 (79%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.7112, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.7619, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.7701, Acc_digits: 114/128 (89%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.5722, Acc_digits: 117/128 (91%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.7185, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.7378, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.8338, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.6314, Acc_digits: 71/80 (89%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24961708101909608\n",
      "\n",
      "Test set: Loss: 0.5024,Acc_digits: 1886/2000 (94%),Acc_labels: 865/1000 86%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.6602, Acc_digits: 112/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.8258, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.7982, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [4/64],Loss: 1.0430, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.9652, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.6953, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [7/64],Loss: 1.0091, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [8/64],Loss: 1.0562, Acc_digits: 102/128 (80%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.6185, Acc_digits: 110/128 (86%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [2/25],Step [10/64],Loss: 1.1708, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.9352, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.8671, Acc_digits: 113/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.7491, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.6759, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.6640, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.4971, Acc_digits: 71/80 (89%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22786099405493587\n",
      "\n",
      "Test set: Loss: 0.5097,Acc_digits: 1883/2000 (94%),Acc_labels: 860/1000 86%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.7634, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.9233, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.7177, Acc_digits: 105/128 (82%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.8532, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.7528, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.9197, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.8429, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.7976, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.6280, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.8218, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.8964, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.8357, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.6091, Acc_digits: 112/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.7461, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.7421, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.7372, Acc_digits: 63/80 (79%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.21576543396804482\n",
      "\n",
      "Test set: Loss: 0.5187,Acc_digits: 1883/2000 (94%),Acc_labels: 858/1000 86%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.7899, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.5753, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.7908, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.5692, Acc_digits: 115/128 (90%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.9357, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [6/64],Loss: 1.0220, Acc_digits: 101/128 (79%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [7/64],Loss: 1.0004, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.8426, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.9566, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.8131, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.9317, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.6180, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.9280, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.7257, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.8381, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.5952, Acc_digits: 72/80 (90%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22923772397916764\n",
      "\n",
      "Test set: Loss: 0.4986,Acc_digits: 1883/2000 (94%),Acc_labels: 872/1000 87%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.8912, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.8627, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.7098, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.7248, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.5939, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.8065, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.9660, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [8/64],Loss: 1.0321, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.6193, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.7305, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.9426, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.6956, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.9293, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.9166, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.5522, Acc_digits: 114/128 (89%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.5062, Acc_digits: 70/80 (88%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2257721379864961\n",
      "\n",
      "Test set: Loss: 0.5209,Acc_digits: 1875/2000 (94%),Acc_labels: 859/1000 86%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.8953, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.7590, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.9072, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.7509, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.9680, Acc_digits: 105/128 (82%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.9866, Acc_digits: 103/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.6990, Acc_digits: 106/128 (83%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.1548, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.8307, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.7266, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.7911, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.8125, Acc_digits: 112/128 (88%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.8787, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.8871, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.7751, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.6624, Acc_digits: 70/80 (88%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22947506001219153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.5139,Acc_digits: 1879/2000 (94%),Acc_labels: 856/1000 86%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.8284, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.6317, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.6696, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [7/25],Step [4/64],Loss: 1.0180, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.9866, Acc_digits: 105/128 (82%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.8199, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.8162, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.9481, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.8483, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.7586, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.9399, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.8593, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.7174, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.7353, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.8652, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.5639, Acc_digits: 72/80 (90%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.2267331249313429\n",
      "\n",
      "Test set: Loss: 0.5066,Acc_digits: 1882/2000 (94%),Acc_labels: 852/1000 85%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.9473, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.9076, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.7320, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.8522, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.9310, Acc_digits: 105/128 (82%) ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.8054, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.9536, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.8273, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.8942, Acc_digits: 100/128 (78%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.8692, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.5642, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.6824, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.8459, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.8512, Acc_digits: 115/128 (90%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.5745, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.6636, Acc_digits: 74/80 (92%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2231098710326478\n",
      "\n",
      "Test set: Loss: 0.4913,Acc_digits: 1881/2000 (94%),Acc_labels: 865/1000 86%\n",
      "Epoch [9/25],Step [1/64],Loss: 1.0447, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.8682, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.8046, Acc_digits: 107/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.7897, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.8460, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.8470, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.7826, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.9362, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.6306, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.8574, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.8763, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.6668, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.9980, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.6977, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.7619, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.7076, Acc_digits: 63/80 (79%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22595802799332887\n",
      "\n",
      "Test set: Loss: 0.4892,Acc_digits: 1894/2000 (95%),Acc_labels: 860/1000 86%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.7084, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.7957, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.8998, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.6640, Acc_digits: 114/128 (89%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.8153, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.9547, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.9179, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.9516, Acc_digits: 98/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.6664, Acc_digits: 118/128 (92%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.6867, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.9982, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.6066, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.7922, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.8795, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.7067, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.5509, Acc_digits: 70/80 (88%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.23141651798505336\n",
      "\n",
      "Test set: Loss: 0.5104,Acc_digits: 1872/2000 (94%),Acc_labels: 857/1000 86%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.9403, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.9994, Acc_digits: 95/128 (74%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.5774, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.7747, Acc_digits: 112/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.9029, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.8765, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.7416, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [8/64],Loss: 1.1174, Acc_digits: 100/128 (78%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.8383, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.8309, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.9417, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.8596, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.6687, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.7955, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.6994, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.6319, Acc_digits: 70/80 (88%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2292334649246186\n",
      "\n",
      "Test set: Loss: 0.5245,Acc_digits: 1880/2000 (94%),Acc_labels: 854/1000 85%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.8914, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.8328, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [3/64],Loss: 1.0599, Acc_digits: 93/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.6607, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25],Step [5/64],Loss: 0.9006, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.8076, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.7927, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.7770, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.7563, Acc_digits: 113/128 (88%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.8352, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [11/64],Loss: 1.1111, Acc_digits: 93/128 (73%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.8046, Acc_digits: 108/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.8276, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.7587, Acc_digits: 101/128 (79%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.8115, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.6275, Acc_digits: 70/80 (88%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.23500258300919086\n",
      "\n",
      "Test set: Loss: 0.4986,Acc_digits: 1883/2000 (94%),Acc_labels: 855/1000 86%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.8339, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.9038, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.8758, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.9190, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.9777, Acc_digits: 97/128 (76%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.9344, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.8906, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.8931, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.6805, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.9302, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.7822, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.7151, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.7565, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.5879, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.9763, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.5933, Acc_digits: 72/80 (90%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.2341217000503093\n",
      "\n",
      "Test set: Loss: 0.4832,Acc_digits: 1899/2000 (95%),Acc_labels: 867/1000 87%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.9677, Acc_digits: 105/128 (82%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.8510, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.9818, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.8805, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.9220, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.8316, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.9208, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [8/64],Loss: 1.0007, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.8201, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.8162, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.7978, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.6655, Acc_digits: 115/128 (90%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.7191, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.5423, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.6164, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.4370, Acc_digits: 73/80 (91%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.23369163600727916\n",
      "\n",
      "Test set: Loss: 0.4939,Acc_digits: 1881/2000 (94%),Acc_labels: 864/1000 86%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.6708, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.7652, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.7298, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.6324, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.8682, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.8915, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.8870, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.7934, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.7669, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.8476, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.7146, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.5971, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.9745, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.9602, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.5928, Acc_digits: 115/128 (90%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.7313, Acc_digits: 70/80 (88%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22839966893661767\n",
      "\n",
      "Test set: Loss: 0.4923,Acc_digits: 1871/2000 (94%),Acc_labels: 874/1000 87%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.8422, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [2/64],Loss: 1.0956, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.8551, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.7913, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.8575, Acc_digits: 95/128 (74%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.9511, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.8568, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.7718, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.8731, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.7491, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.7887, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.6964, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.5459, Acc_digits: 116/128 (91%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.7719, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.7481, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.4509, Acc_digits: 71/80 (89%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.23353497998323292\n",
      "\n",
      "Test set: Loss: 0.4752,Acc_digits: 1888/2000 (94%),Acc_labels: 869/1000 87%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.9054, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.7905, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.6488, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.7379, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.7595, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.8521, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.7701, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.8398, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.6963, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.8064, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.7030, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.9037, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.8381, Acc_digits: 96/128 (75%) ,Acc_labels: 57/64 (89%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25],Step [14/64],Loss: 0.6478, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.7347, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.5719, Acc_digits: 69/80 (86%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.24096975894644856\n",
      "\n",
      "Test set: Loss: 0.4853,Acc_digits: 1884/2000 (94%),Acc_labels: 867/1000 87%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.9305, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.9086, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.6714, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.7949, Acc_digits: 111/128 (87%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.8542, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.7109, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.7836, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.8913, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.6386, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.6897, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.9978, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.7802, Acc_digits: 102/128 (80%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.6846, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.7423, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.8780, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.6843, Acc_digits: 67/80 (84%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.22597580589354038\n",
      "\n",
      "Test set: Loss: 0.5116,Acc_digits: 1869/2000 (93%),Acc_labels: 863/1000 86%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.9574, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.8668, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.8978, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.6113, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.7460, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.8723, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.7404, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.8112, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.7287, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.8055, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.7367, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.8282, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.8734, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.8207, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.5392, Acc_digits: 115/128 (90%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.6531, Acc_digits: 67/80 (84%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.223689206992276\n",
      "\n",
      "Test set: Loss: 0.4657,Acc_digits: 1885/2000 (94%),Acc_labels: 869/1000 87%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.7464, Acc_digits: 110/128 (86%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.8608, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.7429, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.7605, Acc_digits: 116/128 (91%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [5/64],Loss: 1.0656, Acc_digits: 99/128 (77%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.8199, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.6443, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.6877, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.6818, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.7879, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.7412, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.7538, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.7771, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.7544, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.7471, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.6432, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22298433107789606\n",
      "\n",
      "Test set: Loss: 0.4858,Acc_digits: 1885/2000 (94%),Acc_labels: 869/1000 87%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.6643, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.8075, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.6796, Acc_digits: 111/128 (87%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.6093, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.9274, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.8837, Acc_digits: 98/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.8371, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.8940, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.8506, Acc_digits: 94/128 (73%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.8094, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.8300, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.5777, Acc_digits: 115/128 (90%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.8097, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.8097, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.5163, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.3535, Acc_digits: 73/80 (91%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2537240650271997\n",
      "\n",
      "Test set: Loss: 0.4908,Acc_digits: 1876/2000 (94%),Acc_labels: 862/1000 86%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.7181, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.8360, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.7393, Acc_digits: 107/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.6170, Acc_digits: 118/128 (92%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.8609, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.7576, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.8156, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.9485, Acc_digits: 103/128 (80%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.8056, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.7226, Acc_digits: 111/128 (87%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.7885, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.6655, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.9077, Acc_digits: 106/128 (83%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.7264, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.7025, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.7434, Acc_digits: 71/80 (89%) ,Acc_labels: 31/40 (78%)\n",
      "Time needed to train  0.24683565006125718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.5083,Acc_digits: 1889/2000 (94%),Acc_labels: 854/1000 85%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.8579, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.8028, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.8342, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.6545, Acc_digits: 110/128 (86%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.7866, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.9765, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.7679, Acc_digits: 103/128 (80%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.9968, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.7436, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.8549, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.7770, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.6250, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.5486, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.6476, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.7704, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.4948, Acc_digits: 76/80 (95%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.3225937020033598\n",
      "\n",
      "Test set: Loss: 0.4928,Acc_digits: 1880/2000 (94%),Acc_labels: 866/1000 87%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.8675, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.7348, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.7644, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.6857, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.8142, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.8481, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.6114, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.7752, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.6398, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [10/64],Loss: 1.0060, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.7707, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.6643, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.8218, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.6841, Acc_digits: 114/128 (89%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.8259, Acc_digits: 110/128 (86%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.7133, Acc_digits: 70/80 (88%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.25806902709882706\n",
      "\n",
      "Test set: Loss: 0.4923,Acc_digits: 1883/2000 (94%),Acc_labels: 868/1000 87%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.7905, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.8295, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.6875, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.7342, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.7698, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.8238, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.9248, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.7417, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.7601, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.8891, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.6644, Acc_digits: 111/128 (87%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.5966, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.8234, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.4853, Acc_digits: 115/128 (90%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.5903, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.5365, Acc_digits: 71/80 (89%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.324420205084607\n",
      "\n",
      "Test set: Loss: 0.4871,Acc_digits: 1891/2000 (95%),Acc_labels: 860/1000 86%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.8369, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.8304, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.5512, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.5891, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [5/64],Loss: 1.0476, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.9329, Acc_digits: 97/128 (76%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.8162, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.7839, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.6064, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.7962, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.5274, Acc_digits: 114/128 (89%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.8485, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.6773, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.6640, Acc_digits: 105/128 (82%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.7341, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.6820, Acc_digits: 68/80 (85%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.24224787892308086\n",
      "\n",
      "Test set: Loss: 0.5038,Acc_digits: 1895/2000 (95%),Acc_labels: 857/1000 86%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.6808, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.9425, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.7309, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.6033, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.8247, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.8876, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.8257, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [8/64],Loss: 1.0936, Acc_digits: 95/128 (74%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.8315, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.9138, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.9000, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.6815, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.7559, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.7330, Acc_digits: 109/128 (85%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.6585, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.6657, Acc_digits: 65/80 (81%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.25663382990751415\n",
      "\n",
      "Test set: Loss: 0.4823,Acc_digits: 1881/2000 (94%),Acc_labels: 872/1000 87%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.8535, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.8392, Acc_digits: 101/128 (79%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.8593, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.7027, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.8078, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.9299, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.8135, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.9146, Acc_digits: 105/128 (82%) ,Acc_labels: 52/64 (81%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25],Step [9/64],Loss: 0.6315, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.6137, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.8682, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.6404, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.7115, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.7612, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.7543, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.4920, Acc_digits: 72/80 (90%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.3461822579847649\n",
      "\n",
      "Test set: Loss: 0.4718,Acc_digits: 1897/2000 (95%),Acc_labels: 867/1000 87%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.7025, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.8208, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.7028, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.8152, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.6562, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.8066, Acc_digits: 98/128 (77%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.7414, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [8/64],Loss: 1.0184, Acc_digits: 104/128 (81%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.7493, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.7906, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.9167, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.7086, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.6960, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.8023, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.7848, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.6105, Acc_digits: 68/80 (85%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22332809993531555\n",
      "\n",
      "Test set: Loss: 0.4907,Acc_digits: 1890/2000 (94%),Acc_labels: 868/1000 87%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.7642, Acc_digits: 106/128 (83%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.6963, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [5/25],Step [3/64],Loss: 1.0277, Acc_digits: 102/128 (80%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.8142, Acc_digits: 111/128 (87%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.8675, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.6994, Acc_digits: 104/128 (81%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.8635, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.9469, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.7873, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.8321, Acc_digits: 107/128 (84%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [5/25],Step [11/64],Loss: 1.0646, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.8266, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.8369, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.7853, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.7262, Acc_digits: 113/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.5740, Acc_digits: 70/80 (88%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22699222899973392\n",
      "\n",
      "Test set: Loss: 0.4944,Acc_digits: 1895/2000 (95%),Acc_labels: 865/1000 86%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.9482, Acc_digits: 106/128 (83%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.8354, Acc_digits: 103/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.6132, Acc_digits: 106/128 (83%) ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.6300, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.8511, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.8543, Acc_digits: 110/128 (86%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.8201, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.8983, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.8604, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.8789, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.7608, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.6093, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.6858, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.7107, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.7274, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.5887, Acc_digits: 73/80 (91%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22749487194232643\n",
      "\n",
      "Test set: Loss: 0.4951,Acc_digits: 1867/2000 (93%),Acc_labels: 874/1000 87%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.8711, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.9721, Acc_digits: 96/128 (75%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.8083, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.6853, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [5/64],Loss: 1.0081, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.8439, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.8512, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.8305, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.7436, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.6779, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.8067, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.6728, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.6338, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.7540, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.9210, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.5329, Acc_digits: 70/80 (88%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.22845967602916062\n",
      "\n",
      "Test set: Loss: 0.5086,Acc_digits: 1882/2000 (94%),Acc_labels: 861/1000 86%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.7815, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.8183, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.7882, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.8396, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.7984, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.8721, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.7138, Acc_digits: 105/128 (82%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.8007, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.6470, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.7405, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.7496, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.5232, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.7623, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.5622, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25],Step [15/64],Loss: 0.7171, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.5691, Acc_digits: 67/80 (84%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22602252499200404\n",
      "\n",
      "Test set: Loss: 0.4880,Acc_digits: 1891/2000 (95%),Acc_labels: 863/1000 86%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.9277, Acc_digits: 98/128 (77%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.7110, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.6794, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.9464, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.9088, Acc_digits: 112/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.8336, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.7923, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.8044, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.8179, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.8158, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.7765, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.8438, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.7237, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.6569, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.8162, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.6865, Acc_digits: 71/80 (89%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.22706582001410425\n",
      "\n",
      "Test set: Loss: 0.5054,Acc_digits: 1888/2000 (94%),Acc_labels: 866/1000 87%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.7252, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.8924, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.7747, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.9675, Acc_digits: 107/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [5/64],Loss: 1.0037, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.8804, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.7167, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.7598, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.7208, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.6172, Acc_digits: 114/128 (89%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.8416, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.7458, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.6767, Acc_digits: 106/128 (83%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.6528, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.6654, Acc_digits: 114/128 (89%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.6107, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.21698538900818676\n",
      "\n",
      "Test set: Loss: 0.4810,Acc_digits: 1888/2000 (94%),Acc_labels: 870/1000 87%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.7940, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.7548, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.7491, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.7404, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.8524, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [6/64],Loss: 1.0412, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.7284, Acc_digits: 111/128 (87%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.7036, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.6101, Acc_digits: 114/128 (89%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.7438, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.8633, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.7191, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.6628, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.7387, Acc_digits: 100/128 (78%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.7051, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.5865, Acc_digits: 71/80 (89%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.24293517600744963\n",
      "\n",
      "Test set: Loss: 0.4710,Acc_digits: 1886/2000 (94%),Acc_labels: 863/1000 86%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.6734, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.7461, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.7190, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.7543, Acc_digits: 115/128 (90%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.7546, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.7853, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.7595, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.8390, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.7337, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.6539, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.6978, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.6633, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.8011, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.6350, Acc_digits: 110/128 (86%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.6215, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.4968, Acc_digits: 75/80 (94%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.21708452398888767\n",
      "\n",
      "Test set: Loss: 0.5017,Acc_digits: 1872/2000 (94%),Acc_labels: 864/1000 86%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.8121, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.6839, Acc_digits: 104/128 (81%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.8834, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.6928, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [5/64],Loss: 1.0712, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.7587, Acc_digits: 100/128 (78%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.7767, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.8304, Acc_digits: 94/128 (73%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.7177, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.8973, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.7492, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.6334, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.6217, Acc_digits: 107/128 (84%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.6142, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.7929, Acc_digits: 115/128 (90%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.6766, Acc_digits: 69/80 (86%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22963412397075444\n",
      "\n",
      "Test set: Loss: 0.4771,Acc_digits: 1879/2000 (94%),Acc_labels: 872/1000 87%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.9975, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.8910, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.7947, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.9229, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.7258, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.7357, Acc_digits: 113/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.6988, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.7319, Acc_digits: 111/128 (87%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.7870, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.9166, Acc_digits: 102/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [11/64],Loss: 1.0012, Acc_digits: 96/128 (75%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.6963, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.8433, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25],Step [14/64],Loss: 0.6539, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.5726, Acc_digits: 114/128 (89%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.6872, Acc_digits: 68/80 (85%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.23009316599927843\n",
      "\n",
      "Test set: Loss: 0.4885,Acc_digits: 1884/2000 (94%),Acc_labels: 864/1000 86%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.8316, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.7552, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.7651, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.7598, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.7438, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.7598, Acc_digits: 110/128 (86%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.5673, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.9397, Acc_digits: 101/128 (79%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.9397, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.6079, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.7992, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.8419, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.6912, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.6683, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.6193, Acc_digits: 114/128 (89%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.4930, Acc_digits: 74/80 (92%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22382030403241515\n",
      "\n",
      "Test set: Loss: 0.4703,Acc_digits: 1885/2000 (94%),Acc_labels: 870/1000 87%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.6038, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.9559, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.7502, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.9733, Acc_digits: 111/128 (87%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.8895, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.8681, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.6813, Acc_digits: 110/128 (86%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.7763, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.7865, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.9057, Acc_digits: 100/128 (78%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.5730, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.5494, Acc_digits: 119/128 (93%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.7803, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.7714, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.7786, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.6220, Acc_digits: 70/80 (88%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2246776050888002\n",
      "\n",
      "Test set: Loss: 0.4696,Acc_digits: 1892/2000 (95%),Acc_labels: 864/1000 86%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.7105, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.7928, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.8009, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.5727, Acc_digits: 117/128 (91%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.8626, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.8820, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.8410, Acc_digits: 104/128 (81%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.8300, Acc_digits: 108/128 (84%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.6808, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.7519, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.8513, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.9171, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.7300, Acc_digits: 114/128 (89%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.7690, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.6760, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.8136, Acc_digits: 67/80 (84%) ,Acc_labels: 34/40 (85%)\n",
      "Time needed to train  0.22945781506132334\n",
      "\n",
      "Test set: Loss: 0.4787,Acc_digits: 1881/2000 (94%),Acc_labels: 867/1000 87%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.7575, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.8904, Acc_digits: 101/128 (79%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.7296, Acc_digits: 111/128 (87%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.4868, Acc_digits: 115/128 (90%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.6574, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.9804, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.8594, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.7856, Acc_digits: 101/128 (79%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.7059, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.7117, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.7673, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.5921, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [13/64],Loss: 1.0610, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.7341, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.7773, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.4816, Acc_digits: 73/80 (91%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.21892243600450456\n",
      "\n",
      "Test set: Loss: 0.4684,Acc_digits: 1898/2000 (95%),Acc_labels: 875/1000 88%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.8006, Acc_digits: 109/128 (85%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.9337, Acc_digits: 104/128 (81%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.8854, Acc_digits: 104/128 (81%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.7150, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.8022, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.7784, Acc_digits: 111/128 (87%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.7573, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.8629, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.7338, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.9285, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.8738, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.6541, Acc_digits: 103/128 (80%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.7050, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.5988, Acc_digits: 117/128 (91%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.7388, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.5838, Acc_digits: 75/80 (94%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22159704205114394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.5055,Acc_digits: 1886/2000 (94%),Acc_labels: 865/1000 86%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.7492, Acc_digits: 102/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.8453, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.8388, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.7233, Acc_digits: 118/128 (92%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.7327, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.8917, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.6436, Acc_digits: 109/128 (85%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.7715, Acc_digits: 106/128 (83%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.5753, Acc_digits: 116/128 (91%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.8340, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.6223, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.7763, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.6453, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.7745, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.6644, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.4916, Acc_digits: 72/80 (90%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2260028449818492\n",
      "\n",
      "Test set: Loss: 0.4933,Acc_digits: 1879/2000 (94%),Acc_labels: 869/1000 87%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.6470, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.7771, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.5338, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.7358, Acc_digits: 113/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.8245, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.8259, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.6154, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.7574, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.6593, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.5997, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [11/64],Loss: 1.0667, Acc_digits: 101/128 (79%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.5107, Acc_digits: 117/128 (91%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.9749, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.8179, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.6196, Acc_digits: 114/128 (89%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.4654, Acc_digits: 72/80 (90%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.21137118805199862\n",
      "\n",
      "Test set: Loss: 0.4873,Acc_digits: 1885/2000 (94%),Acc_labels: 863/1000 86%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.8597, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.8157, Acc_digits: 104/128 (81%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.7564, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.6640, Acc_digits: 116/128 (91%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.6800, Acc_digits: 113/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.7909, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.7546, Acc_digits: 115/128 (90%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.8062, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.5745, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.9084, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.9808, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.6973, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.7779, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.6215, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.7243, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.8029, Acc_digits: 62/80 (78%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2362092250259593\n",
      "\n",
      "Test set: Loss: 0.4622,Acc_digits: 1895/2000 (95%),Acc_labels: 879/1000 88%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.7417, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.7969, Acc_digits: 100/128 (78%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.5334, Acc_digits: 110/128 (86%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.8485, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.8432, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.8714, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.8017, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.7650, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.6857, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.8489, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.7901, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.7788, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.7678, Acc_digits: 110/128 (86%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.8876, Acc_digits: 100/128 (78%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.6820, Acc_digits: 115/128 (90%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.5617, Acc_digits: 72/80 (90%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.23846729600336403\n",
      "\n",
      "Test set: Loss: 0.4742,Acc_digits: 1896/2000 (95%),Acc_labels: 870/1000 87%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.7788, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.8835, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.6268, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.6573, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.9143, Acc_digits: 99/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.7514, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.7382, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.6786, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.7608, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.8923, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.7759, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.5703, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.7656, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.5450, Acc_digits: 108/128 (84%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.7941, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.4755, Acc_digits: 71/80 (89%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22847540897782892\n",
      "\n",
      "Test set: Loss: 0.4695,Acc_digits: 1890/2000 (94%),Acc_labels: 872/1000 87%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.8798, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.8476, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.6874, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.5998, Acc_digits: 112/128 (88%) ,Acc_labels: 57/64 (89%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25],Step [5/64],Loss: 0.9691, Acc_digits: 103/128 (80%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.6778, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.6796, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.8201, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.7690, Acc_digits: 117/128 (91%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.8151, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.5437, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.5909, Acc_digits: 115/128 (90%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.6648, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.6839, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.6865, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.6323, Acc_digits: 71/80 (89%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.23241901805158705\n",
      "\n",
      "Test set: Loss: 0.4569,Acc_digits: 1892/2000 (95%),Acc_labels: 871/1000 87%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.7243, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.6768, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.7290, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.8944, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.6660, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.7253, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [7/64],Loss: 1.0315, Acc_digits: 98/128 (77%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.9630, Acc_digits: 98/128 (77%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.8039, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.7871, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.8931, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.7205, Acc_digits: 113/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.6941, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.8041, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.5757, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.7048, Acc_digits: 66/80 (82%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.23879252397455275\n",
      "\n",
      "Test set: Loss: 0.4741,Acc_digits: 1892/2000 (95%),Acc_labels: 859/1000 86%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.7269, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.7601, Acc_digits: 98/128 (77%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.7174, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.5565, Acc_digits: 118/128 (92%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.9049, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.9993, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.8593, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.8311, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.7857, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.6706, Acc_digits: 113/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.6439, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.5284, Acc_digits: 118/128 (92%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.7526, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.6803, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.7365, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.4154, Acc_digits: 75/80 (94%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2263238449813798\n",
      "\n",
      "Test set: Loss: 0.4918,Acc_digits: 1889/2000 (94%),Acc_labels: 859/1000 86%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.8930, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.6090, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.9589, Acc_digits: 96/128 (75%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.6446, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.8955, Acc_digits: 102/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.7996, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.6058, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.7936, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.7908, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.8712, Acc_digits: 102/128 (80%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.8018, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.5690, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.7166, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.6721, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.6827, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.5926, Acc_digits: 73/80 (91%) ,Acc_labels: 35/40 (88%)\n",
      "Time needed to train  0.22301621898077428\n",
      "\n",
      "Test set: Loss: 0.4927,Acc_digits: 1882/2000 (94%),Acc_labels: 865/1000 86%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.7531, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.7128, Acc_digits: 100/128 (78%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.8471, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.5939, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.7028, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.8636, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.8101, Acc_digits: 110/128 (86%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.8104, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.5867, Acc_digits: 111/128 (87%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.7515, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.7916, Acc_digits: 114/128 (89%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.6860, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.6105, Acc_digits: 117/128 (91%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.5676, Acc_digits: 114/128 (89%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.7792, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.4118, Acc_digits: 74/80 (92%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.23166656191460788\n",
      "\n",
      "Test set: Loss: 0.4660,Acc_digits: 1894/2000 (95%),Acc_labels: 876/1000 88%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.8414, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.6904, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.5066, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.6472, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.7754, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.6306, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.5769, Acc_digits: 114/128 (89%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.9120, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.8076, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.7952, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.6768, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.6078, Acc_digits: 114/128 (89%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.6569, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.7195, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25],Step [15/64],Loss: 0.6036, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.4399, Acc_digits: 75/80 (94%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.22886776993982494\n",
      "\n",
      "Test set: Loss: 0.4746,Acc_digits: 1885/2000 (94%),Acc_labels: 863/1000 86%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.6210, Acc_digits: 112/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.7163, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.6247, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.5921, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.9005, Acc_digits: 112/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.7536, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.9190, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [6/25],Step [8/64],Loss: 1.0157, Acc_digits: 104/128 (81%) ,Acc_labels: 50/64 (78%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.6571, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.7904, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.7203, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.6190, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.7107, Acc_digits: 105/128 (82%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.5797, Acc_digits: 117/128 (91%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.6130, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.4086, Acc_digits: 74/80 (92%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22145408997312188\n",
      "\n",
      "Test set: Loss: 0.4701,Acc_digits: 1892/2000 (95%),Acc_labels: 867/1000 87%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.8476, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.9109, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.6674, Acc_digits: 116/128 (91%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.5620, Acc_digits: 116/128 (91%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.7668, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.6913, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.6811, Acc_digits: 102/128 (80%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.7313, Acc_digits: 103/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.5933, Acc_digits: 110/128 (86%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.6524, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.7182, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.7217, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.3989, Acc_digits: 117/128 (91%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.7063, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.5749, Acc_digits: 117/128 (91%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.6221, Acc_digits: 68/80 (85%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22590629407204688\n",
      "\n",
      "Test set: Loss: 0.4459,Acc_digits: 1906/2000 (95%),Acc_labels: 871/1000 87%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.9350, Acc_digits: 108/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.6291, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.7313, Acc_digits: 102/128 (80%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.5244, Acc_digits: 117/128 (91%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.7339, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.7381, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.7568, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.6632, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.6955, Acc_digits: 109/128 (85%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.8524, Acc_digits: 105/128 (82%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.6806, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.5370, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.6328, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.6807, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.6042, Acc_digits: 114/128 (89%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.6497, Acc_digits: 67/80 (84%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.220089745009318\n",
      "\n",
      "Test set: Loss: 0.4769,Acc_digits: 1882/2000 (94%),Acc_labels: 874/1000 87%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.7037, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.6541, Acc_digits: 110/128 (86%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.9610, Acc_digits: 113/128 (88%) ,Acc_labels: 49/64 (77%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.6533, Acc_digits: 115/128 (90%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.7101, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.7524, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.8071, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.8251, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.8708, Acc_digits: 102/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.8972, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.7359, Acc_digits: 112/128 (88%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.5821, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.8172, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.6097, Acc_digits: 109/128 (85%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.5713, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.5894, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22762999299447984\n",
      "\n",
      "Test set: Loss: 0.4607,Acc_digits: 1900/2000 (95%),Acc_labels: 867/1000 87%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.6915, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.7935, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.6129, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.6777, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.7198, Acc_digits: 110/128 (86%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.7219, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.8245, Acc_digits: 99/128 (77%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.9457, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.8700, Acc_digits: 103/128 (80%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.9031, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.7648, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.6382, Acc_digits: 117/128 (91%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.8341, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.6809, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.5914, Acc_digits: 112/128 (88%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.3631, Acc_digits: 73/80 (91%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2687346040038392\n",
      "\n",
      "Test set: Loss: 0.4628,Acc_digits: 1896/2000 (95%),Acc_labels: 867/1000 87%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.7659, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.7371, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.6899, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.8021, Acc_digits: 113/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.6574, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25],Step [6/64],Loss: 0.8117, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.8249, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.6538, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.6377, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.6376, Acc_digits: 112/128 (88%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.6543, Acc_digits: 115/128 (90%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.5989, Acc_digits: 118/128 (92%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.7145, Acc_digits: 107/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.7263, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.6156, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.4591, Acc_digits: 73/80 (91%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22712645109277219\n",
      "\n",
      "Test set: Loss: 0.4889,Acc_digits: 1892/2000 (95%),Acc_labels: 861/1000 86%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.6335, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.9480, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.6726, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.6959, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.7740, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.6630, Acc_digits: 109/128 (85%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.7586, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.7590, Acc_digits: 105/128 (82%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.6202, Acc_digits: 109/128 (85%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.8523, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.8558, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.6776, Acc_digits: 109/128 (85%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.8122, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.6447, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.5841, Acc_digits: 115/128 (90%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.4314, Acc_digits: 73/80 (91%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2327891979366541\n",
      "\n",
      "Test set: Loss: 0.4563,Acc_digits: 1885/2000 (94%),Acc_labels: 888/1000 89%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.7197, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.8003, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.4928, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.6726, Acc_digits: 114/128 (89%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.7041, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.6844, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.6945, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.7980, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.7879, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.8453, Acc_digits: 104/128 (81%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.7535, Acc_digits: 102/128 (80%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.6925, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.6628, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.7971, Acc_digits: 110/128 (86%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.5864, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.5273, Acc_digits: 70/80 (88%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2270019130082801\n",
      "\n",
      "Test set: Loss: 0.4676,Acc_digits: 1880/2000 (94%),Acc_labels: 876/1000 88%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.7478, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.7169, Acc_digits: 107/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.5638, Acc_digits: 115/128 (90%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.6258, Acc_digits: 114/128 (89%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.7345, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.7143, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.8484, Acc_digits: 102/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.6495, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.6144, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.7898, Acc_digits: 103/128 (80%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.8033, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.5734, Acc_digits: 115/128 (90%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.6647, Acc_digits: 109/128 (85%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.7365, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.9590, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.5397, Acc_digits: 71/80 (89%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.2401111340150237\n",
      "\n",
      "Test set: Loss: 0.4488,Acc_digits: 1890/2000 (94%),Acc_labels: 875/1000 88%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.6387, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.5990, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.7063, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.6262, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.8798, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.8969, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.6450, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.6780, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.7133, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.6875, Acc_digits: 113/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [11/64],Loss: 1.0243, Acc_digits: 100/128 (78%) ,Acc_labels: 51/64 (80%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.7064, Acc_digits: 108/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.9775, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.7082, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.5771, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.5838, Acc_digits: 72/80 (90%) ,Acc_labels: 33/40 (82%)\n",
      "Time needed to train  0.2357955490006134\n",
      "\n",
      "Test set: Loss: 0.4823,Acc_digits: 1890/2000 (94%),Acc_labels: 874/1000 87%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.7195, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.7195, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.7151, Acc_digits: 105/128 (82%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.7308, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.5712, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.6065, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.8439, Acc_digits: 107/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.6341, Acc_digits: 108/128 (84%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.7468, Acc_digits: 104/128 (81%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.6391, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.6737, Acc_digits: 111/128 (87%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.5798, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.7396, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.5985, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.6507, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25],Step [16/40],Loss: 0.3659, Acc_digits: 76/80 (95%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.21180474699940532\n",
      "\n",
      "Test set: Loss: 0.4922,Acc_digits: 1877/2000 (94%),Acc_labels: 870/1000 87%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.7572, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.9564, Acc_digits: 103/128 (80%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.5922, Acc_digits: 115/128 (90%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.8296, Acc_digits: 111/128 (87%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.6689, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.6059, Acc_digits: 114/128 (89%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.9346, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.7755, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.6583, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.7880, Acc_digits: 106/128 (83%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.8794, Acc_digits: 103/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.5023, Acc_digits: 113/128 (88%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.7374, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.7047, Acc_digits: 111/128 (87%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.5036, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.5097, Acc_digits: 68/80 (85%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.2266432479955256\n",
      "\n",
      "Test set: Loss: 0.4794,Acc_digits: 1888/2000 (94%),Acc_labels: 863/1000 86%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.7569, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.7882, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.5665, Acc_digits: 112/128 (88%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.6827, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.8516, Acc_digits: 106/128 (83%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.7309, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.8814, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.7925, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.5596, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.6553, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.9213, Acc_digits: 104/128 (81%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.5482, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.7999, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.4652, Acc_digits: 111/128 (87%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.5656, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.4639, Acc_digits: 71/80 (89%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2173786120256409\n",
      "\n",
      "Test set: Loss: 0.4836,Acc_digits: 1889/2000 (94%),Acc_labels: 868/1000 87%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.8466, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.7098, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.6154, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.6354, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.9196, Acc_digits: 107/128 (84%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.6604, Acc_digits: 114/128 (89%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.7098, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.7222, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.8005, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.9027, Acc_digits: 101/128 (79%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.7160, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.5774, Acc_digits: 109/128 (85%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.6880, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.7566, Acc_digits: 102/128 (80%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.5964, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.4491, Acc_digits: 75/80 (94%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2168496970552951\n",
      "\n",
      "Test set: Loss: 0.4727,Acc_digits: 1888/2000 (94%),Acc_labels: 872/1000 87%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.7963, Acc_digits: 106/128 (83%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.6855, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.6647, Acc_digits: 111/128 (87%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.6012, Acc_digits: 115/128 (90%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.8505, Acc_digits: 100/128 (78%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.8817, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.6304, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.7119, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.7651, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.6629, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.7637, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.6430, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.5844, Acc_digits: 116/128 (91%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.7285, Acc_digits: 111/128 (87%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.7375, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.6750, Acc_digits: 68/80 (85%) ,Acc_labels: 38/40 (95%)\n",
      "Time needed to train  0.22845580393914133\n",
      "\n",
      "Test set: Loss: 0.4757,Acc_digits: 1881/2000 (94%),Acc_labels: 867/1000 87%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.8077, Acc_digits: 102/128 (80%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.7846, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.5903, Acc_digits: 115/128 (90%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.7210, Acc_digits: 112/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.9324, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.7798, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.8002, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.7889, Acc_digits: 110/128 (86%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.6774, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.7951, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.5875, Acc_digits: 111/128 (87%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.5443, Acc_digits: 114/128 (89%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.7204, Acc_digits: 112/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.6443, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.7261, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.5315, Acc_digits: 69/80 (86%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.2666803599568084\n",
      "\n",
      "Test set: Loss: 0.4391,Acc_digits: 1898/2000 (95%),Acc_labels: 878/1000 88%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.8073, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.7202, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.7593, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.7842, Acc_digits: 109/128 (85%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.7418, Acc_digits: 101/128 (79%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.6433, Acc_digits: 108/128 (84%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.7471, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [8/64],Loss: 1.0190, Acc_digits: 98/128 (77%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.8197, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.7695, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.7343, Acc_digits: 112/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.8196, Acc_digits: 111/128 (87%) ,Acc_labels: 52/64 (81%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.5440, Acc_digits: 110/128 (86%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.6857, Acc_digits: 113/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.7407, Acc_digits: 107/128 (84%) ,Acc_labels: 53/64 (83%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25],Step [16/40],Loss: 0.4566, Acc_digits: 73/80 (91%) ,Acc_labels: 36/40 (90%)\n",
      "Time needed to train  0.20904107694514096\n",
      "\n",
      "Test set: Loss: 0.4443,Acc_digits: 1881/2000 (94%),Acc_labels: 875/1000 88%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.8523, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.8294, Acc_digits: 101/128 (79%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.5971, Acc_digits: 110/128 (86%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.7883, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.8985, Acc_digits: 105/128 (82%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.8413, Acc_digits: 109/128 (85%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.7726, Acc_digits: 113/128 (88%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.8339, Acc_digits: 99/128 (77%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.5199, Acc_digits: 113/128 (88%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.8567, Acc_digits: 107/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.5707, Acc_digits: 116/128 (91%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.6069, Acc_digits: 113/128 (88%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.7279, Acc_digits: 110/128 (86%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.6618, Acc_digits: 103/128 (80%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.5573, Acc_digits: 114/128 (89%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.3901, Acc_digits: 72/80 (90%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2300393549958244\n",
      "\n",
      "Test set: Loss: 0.4608,Acc_digits: 1893/2000 (95%),Acc_labels: 871/1000 87%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.7410, Acc_digits: 104/128 (81%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.9331, Acc_digits: 105/128 (82%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.6402, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.5612, Acc_digits: 112/128 (88%) ,Acc_labels: 60/64 (94%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.7746, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.7552, Acc_digits: 106/128 (83%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.7059, Acc_digits: 108/128 (84%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.7763, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.8158, Acc_digits: 105/128 (82%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.6470, Acc_digits: 106/128 (83%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.7231, Acc_digits: 108/128 (84%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.4755, Acc_digits: 109/128 (85%) ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.7144, Acc_digits: 105/128 (82%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.7385, Acc_digits: 107/128 (84%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.6990, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.4899, Acc_digits: 66/80 (82%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2190275479806587\n",
      "\n",
      "Test set: Loss: 0.4747,Acc_digits: 1874/2000 (94%),Acc_labels: 882/1000 88%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.9599, Acc_digits: 102/128 (80%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.9025, Acc_digits: 103/128 (80%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.6131, Acc_digits: 108/128 (84%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.6051, Acc_digits: 106/128 (83%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.8372, Acc_digits: 112/128 (88%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.7051, Acc_digits: 109/128 (85%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.6794, Acc_digits: 112/128 (88%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.8288, Acc_digits: 105/128 (82%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.5527, Acc_digits: 116/128 (91%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.7712, Acc_digits: 108/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.6839, Acc_digits: 114/128 (89%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.6621, Acc_digits: 106/128 (83%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.7281, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.7128, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.5861, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.6563, Acc_digits: 68/80 (85%) ,Acc_labels: 37/40 (92%)\n",
      "Time needed to train  0.22150550899095833\n",
      "\n",
      "Test set: Loss: 0.4722,Acc_digits: 1869/2000 (93%),Acc_labels: 864/1000 86%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.6921, Acc_digits: 111/128 (87%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.6395, Acc_digits: 105/128 (82%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.9101, Acc_digits: 106/128 (83%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.6723, Acc_digits: 111/128 (87%) ,Acc_labels: 55/64 (86%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.8253, Acc_digits: 108/128 (84%) ,Acc_labels: 53/64 (83%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.6801, Acc_digits: 109/128 (85%) ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.7928, Acc_digits: 104/128 (81%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.7911, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.5589, Acc_digits: 111/128 (87%) ,Acc_labels: 61/64 (95%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.8534, Acc_digits: 95/128 (74%) ,Acc_labels: 56/64 (88%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.6935, Acc_digits: 107/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.5034, Acc_digits: 113/128 (88%) ,Acc_labels: 59/64 (92%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.7307, Acc_digits: 107/128 (84%) ,Acc_labels: 54/64 (84%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.7570, Acc_digits: 110/128 (86%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.8020, Acc_digits: 108/128 (84%) ,Acc_labels: 57/64 (89%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.4548, Acc_digits: 71/80 (89%) ,Acc_labels: 39/40 (98%)\n",
      "Time needed to train  0.2181811180198565\n",
      "\n",
      "Test set: Loss: 0.4474,Acc_digits: 1888/2000 (94%),Acc_labels: 877/1000 88%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************** Start of the Network 1 ***************************\n",
      "\n",
      " The number of parameters of the network is \n",
      "  69928\n",
      "Epoch [2/25],Step [1/64],Loss: 3.1007 ,Acc_labels: 35/64 (55%)\n",
      "Epoch [2/25],Step [2/64],Loss: 47.7944 ,Acc_labels: 42/64 (66%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.8015 ,Acc_labels: 30/64 (47%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.7339 ,Acc_labels: 29/64 (45%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.6878 ,Acc_labels: 31/64 (48%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.6830 ,Acc_labels: 38/64 (59%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.6964 ,Acc_labels: 34/64 (53%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.6734 ,Acc_labels: 39/64 (61%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.6739 ,Acc_labels: 35/64 (55%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.7002 ,Acc_labels: 35/64 (55%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.7176 ,Acc_labels: 34/64 (53%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.6641 ,Acc_labels: 38/64 (59%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.6602 ,Acc_labels: 39/64 (61%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.6425 ,Acc_labels: 42/64 (66%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.6480 ,Acc_labels: 38/64 (59%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.6217 ,Acc_labels: 27/40 (68%)\n",
      "\n",
      " Time needed to train  0.3779796049930155\n",
      "\n",
      "Test set: Loss: 0.6125,Acc_labels: 683/1000 68%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.6009 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.8580 ,Acc_labels: 37/64 (58%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.6545 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.7863 ,Acc_labels: 33/64 (52%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.7601 ,Acc_labels: 30/64 (47%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.6730 ,Acc_labels: 38/64 (59%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.6579 ,Acc_labels: 36/64 (56%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.6660 ,Acc_labels: 36/64 (56%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.6941 ,Acc_labels: 32/64 (50%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.6962 ,Acc_labels: 29/64 (45%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.7044 ,Acc_labels: 28/64 (44%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.6482 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.6564 ,Acc_labels: 42/64 (66%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.6394 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.6219 ,Acc_labels: 43/64 (67%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.6075 ,Acc_labels: 27/40 (68%)\n",
      "\n",
      " Time needed to train  0.40647383499890566\n",
      "\n",
      "Test set: Loss: 0.6149,Acc_labels: 671/1000 67%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.6179 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.6248 ,Acc_labels: 41/64 (64%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.5719 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.6467 ,Acc_labels: 38/64 (59%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.6815 ,Acc_labels: 41/64 (64%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.5908 ,Acc_labels: 40/64 (62%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.6404 ,Acc_labels: 38/64 (59%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.6167 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.6235 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.6100 ,Acc_labels: 41/64 (64%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.6516 ,Acc_labels: 42/64 (66%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.6274 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.6059 ,Acc_labels: 41/64 (64%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.6454 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.5647 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.5344 ,Acc_labels: 31/40 (78%)\n",
      "\n",
      " Time needed to train  0.41381475201342255\n",
      "\n",
      "Test set: Loss: 0.5698,Acc_labels: 718/1000 72%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.5881 ,Acc_labels: 43/64 (67%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.5773 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.5362 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.6213 ,Acc_labels: 41/64 (64%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.6534 ,Acc_labels: 39/64 (61%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.5701 ,Acc_labels: 43/64 (67%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.5885 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.5739 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.5528 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.5465 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.5936 ,Acc_labels: 43/64 (67%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.5113 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.5794 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.5960 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.5150 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.4884 ,Acc_labels: 31/40 (78%)\n",
      "\n",
      " Time needed to train  0.37844797095749527\n",
      "\n",
      "Test set: Loss: 0.5318,Acc_labels: 743/1000 74%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.5224 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.7694 ,Acc_labels: 42/64 (66%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.5837 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.6213 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.6337 ,Acc_labels: 36/64 (56%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.6572 ,Acc_labels: 43/64 (67%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.6711 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.5599 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.5961 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.6185 ,Acc_labels: 40/64 (62%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.6064 ,Acc_labels: 39/64 (61%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.5353 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.6278 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.5931 ,Acc_labels: 43/64 (67%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.5802 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.5230 ,Acc_labels: 31/40 (78%)\n",
      "\n",
      " Time needed to train  0.4129275659797713\n",
      "\n",
      "Test set: Loss: 0.5614,Acc_labels: 719/1000 72%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.6073 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.5402 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.5656 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.5367 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.5397 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.6229 ,Acc_labels: 42/64 (66%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.5196 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.5732 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.4931 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.5546 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.4877 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.5902 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.5263 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.5327 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.4824 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.4751 ,Acc_labels: 32/40 (80%)\n",
      "\n",
      " Time needed to train  0.39939995401073247\n",
      "\n",
      "Test set: Loss: 0.5324,Acc_labels: 724/1000 72%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.5308 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.5107 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.5625 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.5303 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.4653 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.5028 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.4604 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.5880 ,Acc_labels: 45/64 (70%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.4827 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.5092 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.4338 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.4539 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.4787 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.5303 ,Acc_labels: 44/64 (69%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.3982 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.4249 ,Acc_labels: 33/40 (82%)\n",
      "\n",
      " Time needed to train  0.41492094192653894\n",
      "\n",
      "Test set: Loss: 0.5172,Acc_labels: 749/1000 75%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.4420 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.5145 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.5868 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.5189 ,Acc_labels: 47/64 (73%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.4665 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.4619 ,Acc_labels: 47/64 (73%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25],Step [7/64],Loss: 0.4840 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.4931 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.4424 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.4467 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.4659 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.3243 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.4101 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.5235 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.3954 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.3615 ,Acc_labels: 34/40 (85%)\n",
      "\n",
      " Time needed to train  0.419320919085294\n",
      "\n",
      "Test set: Loss: 0.4987,Acc_labels: 759/1000 76%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.4309 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.4537 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.4675 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.4639 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.3912 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.4450 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.4162 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.4198 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.4636 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.4380 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.4322 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.3139 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.4262 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.4328 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.3064 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.2647 ,Acc_labels: 37/40 (92%)\n",
      "\n",
      " Time needed to train  0.3957425949629396\n",
      "\n",
      "Test set: Loss: 0.5160,Acc_labels: 753/1000 75%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.4409 ,Acc_labels: 48/64 (75%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.4004 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.4267 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.4090 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.3432 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.4033 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.3561 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.4867 ,Acc_labels: 49/64 (77%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.3987 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.3910 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.4098 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.2619 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.3301 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.4452 ,Acc_labels: 46/64 (72%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.3538 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.2272 ,Acc_labels: 38/40 (95%)\n",
      "\n",
      " Time needed to train  0.36226800200529397\n",
      "\n",
      "Test set: Loss: 0.5126,Acc_labels: 768/1000 77%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.4174 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.3537 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.3363 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.3422 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.3032 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.3413 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.2618 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.4016 ,Acc_labels: 50/64 (78%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.3667 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.4781 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.4086 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.2679 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.3900 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.3361 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.2295 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.2139 ,Acc_labels: 37/40 (92%)\n",
      "\n",
      " Time needed to train  0.36056034197099507\n",
      "\n",
      "Test set: Loss: 0.5358,Acc_labels: 770/1000 77%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.3539 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.3657 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.3366 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.3601 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.2573 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.3615 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.2337 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.2937 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.3935 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.3203 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.4491 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.3565 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.3307 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.3444 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.2196 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.2453 ,Acc_labels: 37/40 (92%)\n",
      "\n",
      " Time needed to train  0.41089882201049477\n",
      "\n",
      "Test set: Loss: 0.5103,Acc_labels: 764/1000 76%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.3105 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.2693 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.3023 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.3744 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.2662 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.2851 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.1741 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.3235 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.2561 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.2806 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.4053 ,Acc_labels: 52/64 (81%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.2922 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.2400 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.3341 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.2453 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.1375 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.3854710579616949\n",
      "\n",
      "Test set: Loss: 0.4903,Acc_labels: 772/1000 77%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.2205 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.2527 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.2953 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.2657 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.2462 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.2629 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.1719 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.3052 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.3044 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.2679 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.3540 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.2328 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.2792 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.2617 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.2006 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.1967 ,Acc_labels: 37/40 (92%)\n",
      "\n",
      " Time needed to train  0.37763082794845104\n",
      "\n",
      "Test set: Loss: 0.4753,Acc_labels: 783/1000 78%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.2499 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.3529 ,Acc_labels: 51/64 (80%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.2119 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.2488 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.2221 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.3162 ,Acc_labels: 55/64 (86%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.2019 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.2457 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.3029 ,Acc_labels: 54/64 (84%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.3087 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.2922 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.1432 ,Acc_labels: 62/64 (97%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25],Step [13/64],Loss: 0.2348 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.2367 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.2128 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.1507 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.5516691609518602\n",
      "\n",
      "Test set: Loss: 0.4934,Acc_labels: 776/1000 78%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.2012 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.2225 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.1716 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.2370 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.2136 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.3127 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.1867 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.1937 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.2457 ,Acc_labels: 53/64 (83%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.2075 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.2387 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.2581 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.1425 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.2162 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.1401 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0960 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.4036106129642576\n",
      "\n",
      "Test set: Loss: 0.5148,Acc_labels: 783/1000 78%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.2195 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.2559 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.1597 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.1784 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.1769 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.1837 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.1304 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.2054 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.1704 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.2683 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.1875 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.1892 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.1762 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.1580 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.1430 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0966 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37277990696020424\n",
      "\n",
      "Test set: Loss: 0.5207,Acc_labels: 776/1000 78%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.1742 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.2245 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.1779 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.3272 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.1109 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.1552 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.1013 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.1310 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.1955 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.2865 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.2800 ,Acc_labels: 56/64 (88%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.1447 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.1986 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.1518 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0979 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.1427 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.41345765092410147\n",
      "\n",
      "Test set: Loss: 0.5658,Acc_labels: 786/1000 79%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.2044 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.1870 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.1191 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.1334 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.1108 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.1581 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.1561 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.1320 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0914 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.1148 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.1221 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.1711 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.1729 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.1940 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.1450 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0601 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4019952469971031\n",
      "\n",
      "Test set: Loss: 0.5647,Acc_labels: 771/1000 77%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.1041 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.1924 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.1252 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.1991 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0817 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.1687 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0523 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0680 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.2344 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.1739 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.2420 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.1657 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.1010 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.1889 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.1142 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.1249 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.41202626505400985\n",
      "\n",
      "Test set: Loss: 0.5620,Acc_labels: 778/1000 78%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.1529 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.1311 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0808 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.1610 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.1333 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.1608 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0755 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.1000 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0778 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.1249 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.2563 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0517 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.1334 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0836 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.1081 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0619 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.4135660179890692\n",
      "\n",
      "Test set: Loss: 0.6414,Acc_labels: 775/1000 78%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.1295 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0811 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.1150 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.1097 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.1052 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.1616 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0528 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.1074 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.1473 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.1089 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.1689 ,Acc_labels: 57/64 (89%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0837 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0942 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.1851 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.1224 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0977 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.3995992870768532\n",
      "\n",
      "Test set: Loss: 0.6103,Acc_labels: 775/1000 78%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.1032 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.1407 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.1266 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.1333 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0438 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0664 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0570 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.1166 ,Acc_labels: 61/64 (95%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25],Step [9/64],Loss: 0.0802 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.1390 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0904 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.1218 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0870 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.1568 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.1036 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0235 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37678000004962087\n",
      "\n",
      "Test set: Loss: 0.7403,Acc_labels: 773/1000 77%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.2264 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0853 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0781 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.1485 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.1835 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.1793 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0442 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.1441 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0662 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.1077 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.1119 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0459 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0660 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.1851 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.1128 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0387 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39059165492653847\n",
      "\n",
      "Test set: Loss: 0.7412,Acc_labels: 772/1000 77%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.2026 ,Acc_labels: 58/64 (91%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0712 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.2282 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0981 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0529 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0981 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0990 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.1048 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.1521 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.1212 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.1939 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0611 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.1482 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0422 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0759 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0307 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39188997296150774\n",
      "\n",
      "Test set: Loss: 0.6692,Acc_labels: 789/1000 79%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.1232 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.1263 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0962 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0661 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0769 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.1481 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0396 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.1509 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0677 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0912 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0798 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0461 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0760 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0371 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0626 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.1435 ,Acc_labels: 38/40 (95%)\n",
      "\n",
      " Time needed to train  0.38851309090387076\n",
      "\n",
      "Test set: Loss: 0.7371,Acc_labels: 773/1000 77%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.1097 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.1353 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0386 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0944 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0492 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.1225 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0219 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0436 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0504 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.2084 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0810 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.1135 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0723 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0508 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0897 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0303 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.404855360975489\n",
      "\n",
      "Test set: Loss: 0.7544,Acc_labels: 772/1000 77%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0882 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0670 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0811 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0346 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0480 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0686 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0884 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0633 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.0785 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.1175 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0597 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0692 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0266 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0523 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0569 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0612 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.43381013604812324\n",
      "\n",
      "Test set: Loss: 0.8356,Acc_labels: 759/1000 76%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0745 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.1033 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0181 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0885 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0232 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0837 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0529 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0473 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0117 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.1019 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.1226 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.1560 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0925 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0568 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0684 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0308 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4122456950135529\n",
      "\n",
      "Test set: Loss: 0.6720,Acc_labels: 784/1000 78%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0555 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0384 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0890 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0530 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0613 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0578 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0511 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0946 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0382 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0571 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.1001 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0939 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0320 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.1650 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0433 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.42908360494766384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.7205,Acc_labels: 787/1000 79%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0614 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0685 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0546 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0545 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0624 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0822 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0173 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0628 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.1142 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0511 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.1158 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0593 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0348 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0467 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.1444 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0237 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.382396528031677\n",
      "\n",
      "Test set: Loss: 1.0534,Acc_labels: 739/1000 74%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.1393 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0295 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0115 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.1016 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0425 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0940 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0493 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0324 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0564 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0977 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0577 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0427 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0321 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.1015 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0606 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0231 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4173403549939394\n",
      "\n",
      "Test set: Loss: 0.8221,Acc_labels: 796/1000 80%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0180 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.1070 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0385 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.1272 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0453 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0385 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0215 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0427 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.1254 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0636 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0512 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0151 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0379 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0511 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0231 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0051 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38072872208431363\n",
      "\n",
      "Test set: Loss: 0.8251,Acc_labels: 790/1000 79%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0906 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0854 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0403 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0675 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0685 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0341 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0548 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0516 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0902 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0631 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0437 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0239 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0426 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0289 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0269 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36706384795252234\n",
      "\n",
      "Test set: Loss: 0.8378,Acc_labels: 794/1000 79%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0414 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0877 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0376 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0445 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0314 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0474 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0775 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0493 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0135 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0683 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0846 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0407 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.2179 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0380 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0402 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0105 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36896650097332895\n",
      "\n",
      "Test set: Loss: 0.7798,Acc_labels: 772/1000 77%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0518 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0575 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0933 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0372 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0207 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0214 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0874 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0587 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0409 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0883 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0264 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0289 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0846 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0859 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0258 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0559 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.5264365270268172\n",
      "\n",
      "Test set: Loss: 0.8153,Acc_labels: 780/1000 78%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0279 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0249 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0073 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0564 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0434 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0877 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0432 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0996 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0119 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0383 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0523 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.1300 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0321 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0088 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0282 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0051 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4706326889572665\n",
      "\n",
      "Test set: Loss: 0.8274,Acc_labels: 791/1000 79%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0664 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0407 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.1313 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0169 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0355 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0356 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0343 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25],Step [9/64],Loss: 0.0183 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0525 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0214 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0130 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0548 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0149 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0188 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0266 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38374444690998644\n",
      "\n",
      "Test set: Loss: 0.8458,Acc_labels: 810/1000 81%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0320 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0154 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0372 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0608 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0120 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0104 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0178 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0353 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0286 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0240 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0450 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.3686523089418188\n",
      "\n",
      "Test set: Loss: 0.8338,Acc_labels: 796/1000 80%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0310 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0088 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0180 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0360 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0128 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0393 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0106 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0623 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0975 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0086 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0585 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0279 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0501 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0242 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0575 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.39204453106503934\n",
      "\n",
      "Test set: Loss: 0.9106,Acc_labels: 792/1000 79%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0809 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0233 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0192 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0145 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0137 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0680 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0992 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0119 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0793 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0588 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0348 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0133 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0253 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0371 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.37319160206243396\n",
      "\n",
      "Test set: Loss: 0.8689,Acc_labels: 791/1000 79%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0464 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0271 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0233 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.1493 ,Acc_labels: 59/64 (92%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.1085 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0330 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0544 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0176 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0944 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0261 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0328 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0566 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0292 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0798 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0675 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0255 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.36373755999375135\n",
      "\n",
      "Test set: Loss: 0.7693,Acc_labels: 800/1000 80%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0147 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.1431 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0393 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0236 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0243 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.1466 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0143 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0326 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.0293 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0349 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.1141 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0633 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0169 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0158 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0029 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37456246407236904\n",
      "\n",
      "Test set: Loss: 0.9070,Acc_labels: 798/1000 80%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0261 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0842 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0293 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0318 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0513 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0253 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0105 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0452 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0520 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0329 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0166 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0109 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0108 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0073 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0036 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.389568152022548\n",
      "\n",
      "Test set: Loss: 0.8869,Acc_labels: 800/1000 80%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0293 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0202 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0450 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0427 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0318 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0461 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0301 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0130 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0405 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0139 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0663 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0478 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0319 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0320 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0795 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0022 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3688908510375768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.9346,Acc_labels: 782/1000 78%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0787 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0123 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0245 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0194 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0469 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0468 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0099 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0346 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0098 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0152 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0170 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.1330 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0376 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0189 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0175 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3601904599927366\n",
      "\n",
      "Test set: Loss: 0.9047,Acc_labels: 786/1000 79%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0364 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0527 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0253 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0525 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0517 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0112 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0092 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0344 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0504 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0467 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0284 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0081 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3726879310561344\n",
      "\n",
      "Test set: Loss: 0.8917,Acc_labels: 799/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0270 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0368 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0116 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0096 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0299 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0726 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0085 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0311 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0197 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0968 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0389 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0135 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3775248460005969\n",
      "\n",
      "Test set: Loss: 0.9139,Acc_labels: 804/1000 80%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0424 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0359 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0545 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0174 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0296 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0115 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0440 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0301 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0170 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0187 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0240 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0116 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3719258230412379\n",
      "\n",
      "Test set: Loss: 1.0148,Acc_labels: 799/1000 80%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0523 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0131 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0195 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0143 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0133 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0275 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0476 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0312 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0121 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0186 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0160 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0158 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0025 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3814862701110542\n",
      "\n",
      "Test set: Loss: 1.0054,Acc_labels: 788/1000 79%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0096 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0147 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0153 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0582 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0201 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0398 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0101 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0173 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37311468191910535\n",
      "\n",
      "Test set: Loss: 0.9325,Acc_labels: 797/1000 80%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0153 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0616 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0334 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0262 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0341 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0207 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0284 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.3669428639113903\n",
      "\n",
      "Test set: Loss: 1.0018,Acc_labels: 807/1000 81%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0096 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0525 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0094 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0256 ,Acc_labels: 63/64 (98%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25],Step [9/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0554 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0146 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0096 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0134 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0183 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3675669670337811\n",
      "\n",
      "Test set: Loss: 0.9678,Acc_labels: 802/1000 80%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0136 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0318 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0093 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0157 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0684 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0143 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0140 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0105 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0196 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4389924379065633\n",
      "\n",
      "Test set: Loss: 0.8833,Acc_labels: 806/1000 81%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0265 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0157 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0134 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0322 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0569 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0350 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0700 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0528 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0094 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4172775959596038\n",
      "\n",
      "Test set: Loss: 0.8765,Acc_labels: 810/1000 81%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0254 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0118 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0216 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0144 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37458900397177786\n",
      "\n",
      "Test set: Loss: 0.9689,Acc_labels: 812/1000 81%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0128 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0414 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0126 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0982 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0209 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0938 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0015 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4731460449984297\n",
      "\n",
      "Test set: Loss: 0.8795,Acc_labels: 801/1000 80%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0225 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0073 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0091 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0108 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.0658 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0306 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0131 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0170 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0020 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3963303250493482\n",
      "\n",
      "Test set: Loss: 0.9756,Acc_labels: 792/1000 79%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0134 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0120 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0234 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0249 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0474 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0221 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0816 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0088 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0118 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3863119459711015\n",
      "\n",
      "Test set: Loss: 0.9535,Acc_labels: 800/1000 80%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0137 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0194 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0471 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0216 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0264 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0146 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0447 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.1218 ,Acc_labels: 62/64 (97%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25],Step [16/40],Loss: 0.0021 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41232164890971035\n",
      "\n",
      "Test set: Loss: 1.0711,Acc_labels: 811/1000 81%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0474 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.1103 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0306 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0357 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0107 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0129 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0349 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0187 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0250 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0621 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0363 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0168 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0006 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38210446794983\n",
      "\n",
      "Test set: Loss: 0.9172,Acc_labels: 808/1000 81%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0278 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0471 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0351 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0114 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.1088 ,Acc_labels: 60/64 (94%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0136 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0677 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0369 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0164 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0702 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.38555028301198035\n",
      "\n",
      "Test set: Loss: 1.0082,Acc_labels: 806/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0421 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0808 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0117 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.1018 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0509 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0112 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0152 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.1221 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0628 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0560 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0333 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0768 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0016 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38732481899205595\n",
      "\n",
      "Test set: Loss: 0.9847,Acc_labels: 788/1000 79%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0260 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0363 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0213 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0400 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0160 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0149 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0134 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0172 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0183 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0149 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0257 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0053 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0218 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0184 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0008 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39155102299991995\n",
      "\n",
      "Test set: Loss: 1.0380,Acc_labels: 807/1000 81%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0053 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.1130 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0176 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0221 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0228 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0840 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0226 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0204 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0599 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0079 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0682 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0016 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38347654102835804\n",
      "\n",
      "Test set: Loss: 0.9332,Acc_labels: 809/1000 81%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0317 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0103 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0144 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0377 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0220 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0184 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0103 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0848 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0094 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0549 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0120 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0609 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0032 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3758893079357222\n",
      "\n",
      "Test set: Loss: 0.9946,Acc_labels: 803/1000 80%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0084 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0202 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0105 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0585 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0700 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0397 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.1201 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0140 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0082 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4295979799935594\n",
      "\n",
      "Test set: Loss: 0.9992,Acc_labels: 790/1000 79%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.1900 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0266 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0474 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0236 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0700 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0497 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0229 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0556 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.1220 ,Acc_labels: 62/64 (97%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25],Step [10/64],Loss: 0.0224 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0175 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0183 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0233 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0247 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0036 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3878503249725327\n",
      "\n",
      "Test set: Loss: 0.9090,Acc_labels: 791/1000 79%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0152 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0380 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0138 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0530 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0730 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.4645485560176894\n",
      "\n",
      "Test set: Loss: 0.9273,Acc_labels: 805/1000 80%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0108 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0133 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0678 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0223 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0141 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0680 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0377 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0720 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0970 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0142 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4143195989308879\n",
      "\n",
      "Test set: Loss: 0.9738,Acc_labels: 796/1000 80%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0202 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0333 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0506 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0511 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0375 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0255 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0218 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0555 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0163 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0129 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0250 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0036 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37431211199145764\n",
      "\n",
      "Test set: Loss: 0.9608,Acc_labels: 808/1000 81%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0454 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0157 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0130 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0269 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0116 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0154 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0483 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0124 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0171 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.429624292999506\n",
      "\n",
      "Test set: Loss: 1.0166,Acc_labels: 795/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0141 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.1208 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0113 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0295 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.37931486999150366\n",
      "\n",
      "Test set: Loss: 0.9687,Acc_labels: 807/1000 81%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0140 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0302 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0165 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0208 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0091 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0239 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.1862 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0028 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37999518401920795\n",
      "\n",
      "Test set: Loss: 0.9244,Acc_labels: 802/1000 80%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0333 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0253 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0461 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0129 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0039 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3971159979701042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.9564,Acc_labels: 808/1000 81%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0174 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0291 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0065 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3787052360130474\n",
      "\n",
      "Test set: Loss: 0.9913,Acc_labels: 810/1000 81%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0126 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0143 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0129 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0338 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.38780907599721104\n",
      "\n",
      "Test set: Loss: 1.0201,Acc_labels: 805/1000 80%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0247 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0226 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0415 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0200 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0478 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0130 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0116 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3842761400155723\n",
      "\n",
      "Test set: Loss: 1.0449,Acc_labels: 811/1000 81%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0115 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0229 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0035 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3697101370198652\n",
      "\n",
      "Test set: Loss: 1.0173,Acc_labels: 807/1000 81%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0189 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0143 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0300 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0267 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0606 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.40477250795811415\n",
      "\n",
      "Test set: Loss: 1.0945,Acc_labels: 796/1000 80%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0224 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0195 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0250 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0123 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0289 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0126 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0983 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0158 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0447 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0513 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.3958142180927098\n",
      "\n",
      "Test set: Loss: 1.1736,Acc_labels: 774/1000 77%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0606 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0169 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0145 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0178 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0301 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0009 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.45338970806915313\n",
      "\n",
      "Test set: Loss: 1.1022,Acc_labels: 801/1000 80%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0092 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0118 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0105 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0101 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0082 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25],Step [9/64],Loss: 0.0882 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0009 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3729316920507699\n",
      "\n",
      "Test set: Loss: 1.1067,Acc_labels: 792/1000 79%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0468 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0079 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0697 ,Acc_labels: 61/64 (95%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0194 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0031 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36741626902949065\n",
      "\n",
      "Test set: Loss: 0.9753,Acc_labels: 790/1000 79%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0329 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0229 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0110 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0727 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0114 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0216 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0082 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0271 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.39229171199258417\n",
      "\n",
      "Test set: Loss: 0.9359,Acc_labels: 789/1000 79%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0066 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0097 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0713 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0330 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0395 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0079 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0027 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37380632699932903\n",
      "\n",
      "Test set: Loss: 0.9317,Acc_labels: 797/1000 80%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0133 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0137 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0279 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0255 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0112 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0024 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37162704998627305\n",
      "\n",
      "Test set: Loss: 1.0133,Acc_labels: 802/1000 80%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0097 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0287 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0413 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0104 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0335 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0051 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3707623960217461\n",
      "\n",
      "Test set: Loss: 0.9746,Acc_labels: 805/1000 80%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0139 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0134 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0084 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0327 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0179 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0131 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0018 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3668167060241103\n",
      "\n",
      "Test set: Loss: 1.1116,Acc_labels: 800/1000 80%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0091 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0848 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0176 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0403 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0278 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0092 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3989249950973317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.0041,Acc_labels: 801/1000 80%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0231 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0155 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0129 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0062 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0070 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.48757789097726345\n",
      "\n",
      "Test set: Loss: 1.0258,Acc_labels: 805/1000 80%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0186 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0083 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38202556502074003\n",
      "\n",
      "Test set: Loss: 1.0759,Acc_labels: 803/1000 80%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.0160 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0259 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3703118780395016\n",
      "\n",
      "Test set: Loss: 1.0158,Acc_labels: 803/1000 80%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0151 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0112 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0347 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0223 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0014 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38487242197152227\n",
      "\n",
      "Test set: Loss: 1.0851,Acc_labels: 813/1000 81%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0125 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0020 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3683762160362676\n",
      "\n",
      "Test set: Loss: 1.1009,Acc_labels: 806/1000 81%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0132 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0620 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0186 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0252 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0235 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0044 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3676987079670653\n",
      "\n",
      "Test set: Loss: 1.1359,Acc_labels: 798/1000 80%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0130 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0581 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0744 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0239 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3917416479671374\n",
      "\n",
      "Test set: Loss: 1.0594,Acc_labels: 801/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0106 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0097 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0787 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0415 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25],Step [8/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0131 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0094 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0108 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0053 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0015 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.423376232967712\n",
      "\n",
      "Test set: Loss: 0.9392,Acc_labels: 807/1000 81%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0195 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0131 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0900 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0148 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.42057988804299384\n",
      "\n",
      "Test set: Loss: 0.9611,Acc_labels: 804/1000 80%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0151 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38689719804096967\n",
      "\n",
      "Test set: Loss: 0.9615,Acc_labels: 808/1000 81%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0157 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0101 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0153 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3984992849873379\n",
      "\n",
      "Test set: Loss: 0.9963,Acc_labels: 813/1000 81%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0093 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0346 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0091 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3905823410023004\n",
      "\n",
      "Test set: Loss: 0.9959,Acc_labels: 805/1000 80%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0337 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0158 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.0148 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0172 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3824493719730526\n",
      "\n",
      "Test set: Loss: 1.0102,Acc_labels: 803/1000 80%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0105 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0247 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0276 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39321805199142545\n",
      "\n",
      "Test set: Loss: 1.0534,Acc_labels: 806/1000 81%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0416 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0111 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0343 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0113 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0161 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0602 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0124 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.381980391102843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.0158,Acc_labels: 806/1000 81%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0290 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0160 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.1074 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0076 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.42453554505482316\n",
      "\n",
      "Test set: Loss: 1.0114,Acc_labels: 813/1000 81%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0084 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0133 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0044 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41900438093580306\n",
      "\n",
      "Test set: Loss: 1.0525,Acc_labels: 807/1000 81%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0115 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0135 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0118 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0453 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0201 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0533 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0124 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0150 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4893677829531953\n",
      "\n",
      "Test set: Loss: 1.0945,Acc_labels: 803/1000 80%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0141 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0302 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0088 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0112 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0213 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0047 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4024600370321423\n",
      "\n",
      "Test set: Loss: 1.0547,Acc_labels: 804/1000 80%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0108 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0098 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3719875799724832\n",
      "\n",
      "Test set: Loss: 1.0934,Acc_labels: 811/1000 81%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0266 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0177 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0066 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0682 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3806907170219347\n",
      "\n",
      "Test set: Loss: 1.1257,Acc_labels: 802/1000 80%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0476 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0130 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0250 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0086 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0236 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0053 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0218 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0356 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0176 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.42002063000109047\n",
      "\n",
      "Test set: Loss: 1.1061,Acc_labels: 810/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0084 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0160 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25],Step [8/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0468 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0504 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0144 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0500 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0021 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4386784170055762\n",
      "\n",
      "Test set: Loss: 1.0997,Acc_labels: 804/1000 80%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0066 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4051401859614998\n",
      "\n",
      "Test set: Loss: 1.1734,Acc_labels: 815/1000 82%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0196 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0287 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0429 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0086 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39870499004609883\n",
      "\n",
      "Test set: Loss: 1.0943,Acc_labels: 802/1000 80%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0398 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0216 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0210 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0667 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0274 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0010 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3983143060468137\n",
      "\n",
      "Test set: Loss: 1.0383,Acc_labels: 806/1000 81%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0098 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0328 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0019 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39557854598388076\n",
      "\n",
      "Test set: Loss: 1.0842,Acc_labels: 808/1000 81%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0457 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0509 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0163 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0096 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38377223804127425\n",
      "\n",
      "Test set: Loss: 1.0611,Acc_labels: 794/1000 79%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0104 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0244 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0358 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0110 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37252866802737117\n",
      "\n",
      "Test set: Loss: 1.0829,Acc_labels: 806/1000 81%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0299 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0010 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3753227120032534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.0976,Acc_labels: 806/1000 81%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0125 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0895 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0005 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38397897593677044\n",
      "\n",
      "Test set: Loss: 1.2856,Acc_labels: 790/1000 79%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0939 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0182 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0554 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0272 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0032 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37355774093884975\n",
      "\n",
      "Test set: Loss: 1.0509,Acc_labels: 799/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0355 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0150 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0252 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0390 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0143 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3788876330945641\n",
      "\n",
      "Test set: Loss: 1.0798,Acc_labels: 806/1000 81%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0144 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0079 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.373145908000879\n",
      "\n",
      "Test set: Loss: 1.0727,Acc_labels: 798/1000 80%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0129 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0097 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0272 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36856253899168223\n",
      "\n",
      "Test set: Loss: 1.1344,Acc_labels: 798/1000 80%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0654 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3728959660511464\n",
      "\n",
      "Test set: Loss: 1.0524,Acc_labels: 795/1000 80%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0190 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36008642497472465\n",
      "\n",
      "Test set: Loss: 1.0755,Acc_labels: 804/1000 80%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25],Step [9/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37133619596716017\n",
      "\n",
      "Test set: Loss: 1.0850,Acc_labels: 805/1000 80%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0318 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0120 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0183 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0033 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.365634900983423\n",
      "\n",
      "Test set: Loss: 1.1182,Acc_labels: 805/1000 80%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0663 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3998933880357072\n",
      "\n",
      "Test set: Loss: 1.0655,Acc_labels: 797/1000 80%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0293 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0108 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36227311298716813\n",
      "\n",
      "Test set: Loss: 1.0285,Acc_labels: 804/1000 80%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0351 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0122 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0404 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37670957494992763\n",
      "\n",
      "Test set: Loss: 1.1442,Acc_labels: 793/1000 79%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0238 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0249 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0128 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36713750602211803\n",
      "\n",
      "Test set: Loss: 0.9952,Acc_labels: 808/1000 81%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0213 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0132 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38320801698137075\n",
      "\n",
      "Test set: Loss: 1.0051,Acc_labels: 804/1000 80%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0085 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37834560906048864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.0273,Acc_labels: 808/1000 81%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0082 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0101 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36144988401792943\n",
      "\n",
      "Test set: Loss: 1.0492,Acc_labels: 807/1000 81%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0110 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0168 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0123 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.42707642901223153\n",
      "\n",
      "Test set: Loss: 1.0780,Acc_labels: 809/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0062 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0028 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3679270299617201\n",
      "\n",
      "Test set: Loss: 1.0706,Acc_labels: 807/1000 81%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0550 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0138 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.1309 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0021 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36392937297932804\n",
      "\n",
      "Test set: Loss: 1.1451,Acc_labels: 800/1000 80%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0230 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0367 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0344 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0327 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0016 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3632760150358081\n",
      "\n",
      "Test set: Loss: 1.0206,Acc_labels: 809/1000 81%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0162 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0766 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0479 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0520 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.375508414930664\n",
      "\n",
      "Test set: Loss: 1.0620,Acc_labels: 806/1000 81%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0324 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0192 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0153 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0085 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3748681000433862\n",
      "\n",
      "Test set: Loss: 1.0786,Acc_labels: 801/1000 80%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0442 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0202 ,Acc_labels: 63/64 (98%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25],Step [9/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0088 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0107 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0209 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0648 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0047 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3678567939205095\n",
      "\n",
      "Test set: Loss: 1.0693,Acc_labels: 789/1000 79%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0214 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0975 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0215 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0538 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0009 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3683564370730892\n",
      "\n",
      "Test set: Loss: 1.1074,Acc_labels: 806/1000 81%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0053 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0062 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0163 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3657733959844336\n",
      "\n",
      "Test set: Loss: 1.1024,Acc_labels: 808/1000 81%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0195 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0366 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0092 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3693285499466583\n",
      "\n",
      "Test set: Loss: 1.1404,Acc_labels: 799/1000 80%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0473 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0157 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0166 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0007 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3980141159845516\n",
      "\n",
      "Test set: Loss: 1.0546,Acc_labels: 811/1000 81%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3662853160640225\n",
      "\n",
      "Test set: Loss: 1.0903,Acc_labels: 808/1000 81%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0137 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0103 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0965 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3742880899226293\n",
      "\n",
      "Test set: Loss: 1.0440,Acc_labels: 807/1000 81%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0480 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0269 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36017499689478427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 0.9992,Acc_labels: 811/1000 81%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0069 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0313 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36765182099770755\n",
      "\n",
      "Test set: Loss: 1.0335,Acc_labels: 811/1000 81%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37270755192730576\n",
      "\n",
      "Test set: Loss: 1.0607,Acc_labels: 811/1000 81%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0009 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36533993296325207\n",
      "\n",
      "Test set: Loss: 1.0935,Acc_labels: 811/1000 81%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0103 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36989899596665055\n",
      "\n",
      "Test set: Loss: 1.1210,Acc_labels: 810/1000 81%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0185 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0118 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0116 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39198250405024737\n",
      "\n",
      "Test set: Loss: 1.0666,Acc_labels: 808/1000 81%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0141 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0118 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0017 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3568598680431023\n",
      "\n",
      "Test set: Loss: 1.0562,Acc_labels: 803/1000 80%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0400 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0101 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0006 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3703866070136428\n",
      "\n",
      "Test set: Loss: 1.1487,Acc_labels: 805/1000 80%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0068 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0614 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0097 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25],Step [9/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.40335763595066965\n",
      "\n",
      "Test set: Loss: 1.1327,Acc_labels: 798/1000 80%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0110 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4348231869516894\n",
      "\n",
      "Test set: Loss: 1.1666,Acc_labels: 805/1000 80%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0177 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0105 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0008 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4077166640199721\n",
      "\n",
      "Test set: Loss: 1.3397,Acc_labels: 807/1000 81%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0216 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0257 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0154 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41441667499020696\n",
      "\n",
      "Test set: Loss: 1.2026,Acc_labels: 799/1000 80%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0134 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38936877402011305\n",
      "\n",
      "Test set: Loss: 1.2121,Acc_labels: 813/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0116 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0091 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39074722200166434\n",
      "\n",
      "Test set: Loss: 1.2557,Acc_labels: 811/1000 81%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0112 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0270 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0167 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.2357 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0511 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38838136091362685\n",
      "\n",
      "Test set: Loss: 1.3033,Acc_labels: 798/1000 80%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0479 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0146 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0248 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0082 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3871041829697788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.1711,Acc_labels: 816/1000 82%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0142 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0122 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36867404903750867\n",
      "\n",
      "Test set: Loss: 1.2560,Acc_labels: 804/1000 80%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0486 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0118 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0079 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3848622030345723\n",
      "\n",
      "Test set: Loss: 1.2106,Acc_labels: 807/1000 81%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0422 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0137 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39269912196323276\n",
      "\n",
      "Test set: Loss: 1.3263,Acc_labels: 803/1000 80%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0199 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0141 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3788352790288627\n",
      "\n",
      "Test set: Loss: 1.2553,Acc_labels: 811/1000 81%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0077 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0279 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0463 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0085 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.1607 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.40995196800213307\n",
      "\n",
      "Test set: Loss: 1.3549,Acc_labels: 797/1000 80%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0137 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0287 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0375 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0136 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0228 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41606348601635545\n",
      "\n",
      "Test set: Loss: 1.1988,Acc_labels: 802/1000 80%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0178 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37130890402477235\n",
      "\n",
      "Test set: Loss: 1.1918,Acc_labels: 804/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0165 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0084 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0112 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0106 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0306 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0080 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.43616146908607334\n",
      "\n",
      "Test set: Loss: 1.1800,Acc_labels: 799/1000 80%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0051 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0151 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39250968594569713\n",
      "\n",
      "Test set: Loss: 1.2223,Acc_labels: 801/1000 80%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0096 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0508 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0057 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38934035203419626\n",
      "\n",
      "Test set: Loss: 1.2226,Acc_labels: 812/1000 81%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0050 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0122 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0149 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0143 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0018 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41546896495856345\n",
      "\n",
      "Test set: Loss: 1.1883,Acc_labels: 809/1000 81%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0062 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0011 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41221160907298326\n",
      "\n",
      "Test set: Loss: 1.2081,Acc_labels: 810/1000 81%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0338 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.1180 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0092 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.0470 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4007922849850729\n",
      "\n",
      "Test set: Loss: 1.0956,Acc_labels: 810/1000 81%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0086 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0113 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0112 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4218609619420022\n",
      "\n",
      "Test set: Loss: 1.1534,Acc_labels: 807/1000 81%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4279891879996285\n",
      "\n",
      "Test set: Loss: 1.1668,Acc_labels: 809/1000 81%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0135 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38039375201333314\n",
      "\n",
      "Test set: Loss: 1.1876,Acc_labels: 809/1000 81%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0225 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0540 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0104 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0428 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37257079198025167\n",
      "\n",
      "Test set: Loss: 1.1136,Acc_labels: 816/1000 82%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0180 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0276 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0150 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3918963089818135\n",
      "\n",
      "Test set: Loss: 1.1988,Acc_labels: 809/1000 81%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0293 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0168 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0211 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0115 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.369888287037611\n",
      "\n",
      "Test set: Loss: 1.2499,Acc_labels: 803/1000 80%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0626 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3789288100088015\n",
      "\n",
      "Test set: Loss: 1.1721,Acc_labels: 809/1000 81%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.44805258803535253\n",
      "\n",
      "Test set: Loss: 1.1736,Acc_labels: 816/1000 82%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4009224110050127\n",
      "\n",
      "Test set: Loss: 1.2021,Acc_labels: 813/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0072 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3904881359776482\n",
      "\n",
      "Test set: Loss: 1.2345,Acc_labels: 811/1000 81%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0353 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0191 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0297 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0163 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37214133504312485\n",
      "\n",
      "Test set: Loss: 1.2542,Acc_labels: 809/1000 81%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0203 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3698974560247734\n",
      "\n",
      "Test set: Loss: 1.2536,Acc_labels: 814/1000 81%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0099 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0116 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36999950802419335\n",
      "\n",
      "Test set: Loss: 1.2595,Acc_labels: 811/1000 81%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0085 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36949238390661776\n",
      "\n",
      "Test set: Loss: 1.2391,Acc_labels: 820/1000 82%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39669632399454713\n",
      "\n",
      "Test set: Loss: 1.2358,Acc_labels: 809/1000 81%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0059 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0098 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0125 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0466 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0050 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3745674790116027\n",
      "\n",
      "Test set: Loss: 1.2669,Acc_labels: 818/1000 82%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0361 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0223 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0705 ,Acc_labels: 39/40 (98%)\n",
      "\n",
      " Time needed to train  0.40137719199992716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.2708,Acc_labels: 803/1000 80%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0170 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0033 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3711711170617491\n",
      "\n",
      "Test set: Loss: 1.2315,Acc_labels: 802/1000 80%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0137 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0070 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0205 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3905536769889295\n",
      "\n",
      "Test set: Loss: 1.2909,Acc_labels: 803/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0231 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0391 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0433 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0399 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0813 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0173 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0091 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3733007190749049\n",
      "\n",
      "Test set: Loss: 1.3229,Acc_labels: 796/1000 80%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0055 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0316 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39656610100064427\n",
      "\n",
      "Test set: Loss: 1.2013,Acc_labels: 794/1000 79%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0146 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0010 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37288655701559037\n",
      "\n",
      "Test set: Loss: 1.1826,Acc_labels: 799/1000 80%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0205 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0084 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0013 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38032803498208523\n",
      "\n",
      "Test set: Loss: 1.1853,Acc_labels: 800/1000 80%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0118 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0098 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0244 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0058 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.40081322204787284\n",
      "\n",
      "Test set: Loss: 1.2610,Acc_labels: 801/1000 80%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3827363080345094\n",
      "\n",
      "Test set: Loss: 1.2628,Acc_labels: 800/1000 80%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0123 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0681 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3991034140344709\n",
      "\n",
      "Test set: Loss: 1.2914,Acc_labels: 792/1000 79%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0299 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0312 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0958 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38198036095127463\n",
      "\n",
      "Test set: Loss: 1.2646,Acc_labels: 798/1000 80%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0590 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0126 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0086 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0115 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.40928644593805075\n",
      "\n",
      "Test set: Loss: 1.3497,Acc_labels: 805/1000 80%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0112 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4322306190151721\n",
      "\n",
      "Test set: Loss: 1.3052,Acc_labels: 805/1000 80%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0286 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0094 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [9/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0174 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0015 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4035387629410252\n",
      "\n",
      "Test set: Loss: 1.2964,Acc_labels: 807/1000 81%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0101 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0056 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0078 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0059 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.374632628983818\n",
      "\n",
      "Test set: Loss: 1.2483,Acc_labels: 810/1000 81%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0137 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0062 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0161 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39435835799667984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.3510,Acc_labels: 800/1000 80%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0896 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0349 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0030 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0254 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0019 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3806328399805352\n",
      "\n",
      "Test set: Loss: 1.2571,Acc_labels: 795/1000 80%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0157 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0044 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0148 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3904496810864657\n",
      "\n",
      "Test set: Loss: 1.3261,Acc_labels: 795/1000 80%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0081 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0053 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.385833473992534\n",
      "\n",
      "Test set: Loss: 1.2610,Acc_labels: 794/1000 79%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37820617691613734\n",
      "\n",
      "Test set: Loss: 1.2783,Acc_labels: 799/1000 80%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0039 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3822118879761547\n",
      "\n",
      "Test set: Loss: 1.2984,Acc_labels: 794/1000 79%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0750 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0413 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0012 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3854272650787607\n",
      "\n",
      "Test set: Loss: 1.1807,Acc_labels: 793/1000 79%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3969341419870034\n",
      "\n",
      "Test set: Loss: 1.1938,Acc_labels: 798/1000 80%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0048 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25],Step [9/64],Loss: 0.0035 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0160 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3793593239970505\n",
      "\n",
      "Test set: Loss: 1.1662,Acc_labels: 804/1000 80%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39182637992780656\n",
      "\n",
      "Test set: Loss: 1.1669,Acc_labels: 800/1000 80%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0271 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0042 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0012 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39383840397931635\n",
      "\n",
      "Test set: Loss: 1.2074,Acc_labels: 800/1000 80%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0061 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0195 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0100 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0208 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0007 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37989748001564294\n",
      "\n",
      "Test set: Loss: 1.2264,Acc_labels: 800/1000 80%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0103 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0882 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0098 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0080 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0756 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0036 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0113 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.0138 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4133867690106854\n",
      "\n",
      "Test set: Loss: 1.2328,Acc_labels: 793/1000 79%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0027 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [8/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0395 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3931876589776948\n",
      "\n",
      "Test set: Loss: 1.1420,Acc_labels: 801/1000 80%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0045 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4131689969217405\n",
      "\n",
      "Test set: Loss: 1.1541,Acc_labels: 801/1000 80%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0092 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0095 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.39091150101739913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.1823,Acc_labels: 801/1000 80%\n",
      "Epoch [2/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [2/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [3/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [4/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [5/64],Loss: 0.0273 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [6/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [7/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [8/64],Loss: 0.0089 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [10/64],Loss: 0.0124 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [12/64],Loss: 0.0073 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [13/64],Loss: 0.0082 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [2/25],Step [15/64],Loss: 0.0119 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [2/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38554620498325676\n",
      "\n",
      "Test set: Loss: 1.1816,Acc_labels: 802/1000 80%\n",
      "Epoch [3/25],Step [1/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [2/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [3/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [5/64],Loss: 0.0259 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [6/64],Loss: 0.0168 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [7/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [8/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [9/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [10/64],Loss: 0.0125 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [3/25],Step [11/64],Loss: 0.0064 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [12/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [13/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [14/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [15/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [3/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4058575719827786\n",
      "\n",
      "Test set: Loss: 1.1976,Acc_labels: 800/1000 80%\n",
      "Epoch [4/25],Step [1/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [2/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [3/64],Loss: 0.0139 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [4/25],Step [4/64],Loss: 0.0052 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [6/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [8/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [9/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [10/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [11/64],Loss: 0.0018 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [12/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [14/64],Loss: 0.0057 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [4/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4113298109732568\n",
      "\n",
      "Test set: Loss: 1.2341,Acc_labels: 797/1000 80%\n",
      "Epoch [5/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [2/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [4/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [6/64],Loss: 0.0054 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [8/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [12/64],Loss: 0.0086 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [13/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [14/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [5/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3806479499908164\n",
      "\n",
      "Test set: Loss: 1.2200,Acc_labels: 802/1000 80%\n",
      "Epoch [6/25],Step [1/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [3/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [4/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [6/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [8/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [10/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [11/64],Loss: 0.0149 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [6/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [14/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [6/25],Step [16/40],Loss: 0.0004 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37922768003772944\n",
      "\n",
      "Test set: Loss: 1.2773,Acc_labels: 804/1000 80%\n",
      "Epoch [7/25],Step [1/64],Loss: 0.1031 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [5/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [6/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [7/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [8/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [9/64],Loss: 0.0163 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [10/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [12/64],Loss: 0.0545 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [7/25],Step [13/64],Loss: 0.0029 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [7/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.37993964995257556\n",
      "\n",
      "Test set: Loss: 1.2004,Acc_labels: 809/1000 81%\n",
      "Epoch [8/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [2/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [3/64],Loss: 0.0088 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [6/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [9/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [10/64],Loss: 0.0211 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [12/64],Loss: 0.0138 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [8/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [14/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [15/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [8/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38636855396907777\n",
      "\n",
      "Test set: Loss: 1.2547,Acc_labels: 807/1000 81%\n",
      "Epoch [9/25],Step [1/64],Loss: 0.0015 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [4/64],Loss: 0.0066 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [5/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [6/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [8/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25],Step [9/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [10/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [11/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [13/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [15/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [9/25],Step [16/40],Loss: 0.0007 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36621432402171195\n",
      "\n",
      "Test set: Loss: 1.2341,Acc_labels: 816/1000 82%\n",
      "Epoch [10/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [2/64],Loss: 0.0176 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [4/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [6/64],Loss: 0.0075 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [7/64],Loss: 0.0102 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [8/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [9/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [10/64],Loss: 0.0157 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [10/25],Step [11/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [12/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [10/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3834823089418933\n",
      "\n",
      "Test set: Loss: 1.2945,Acc_labels: 802/1000 80%\n",
      "Epoch [11/25],Step [1/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [2/64],Loss: 0.0031 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [4/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [6/64],Loss: 0.0622 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [8/64],Loss: 0.0071 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [9/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [13/64],Loss: 0.0254 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [11/25],Step [14/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [15/64],Loss: 0.0087 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [11/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38458207598887384\n",
      "\n",
      "Test set: Loss: 1.4068,Acc_labels: 813/1000 81%\n",
      "Epoch [12/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [3/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [4/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [5/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [7/64],Loss: 0.0049 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [8/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [9/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [10/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [11/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [13/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [15/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [12/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3893073709914461\n",
      "\n",
      "Test set: Loss: 1.3593,Acc_labels: 813/1000 81%\n",
      "Epoch [13/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [3/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [4/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [5/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [6/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [7/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [8/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [9/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [10/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [12/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [14/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [15/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [13/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36782578099519014\n",
      "\n",
      "Test set: Loss: 1.3643,Acc_labels: 809/1000 81%\n",
      "Epoch [14/25],Step [1/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [2/64],Loss: 0.0125 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [3/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [8/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [9/64],Loss: 0.0010 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [10/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [13/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [14/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [14/25],Step [15/64],Loss: 0.0276 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [14/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3624646939570084\n",
      "\n",
      "Test set: Loss: 1.2782,Acc_labels: 810/1000 81%\n",
      "Epoch [15/25],Step [1/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [3/64],Loss: 0.0040 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [4/64],Loss: 0.0022 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [5/64],Loss: 0.0038 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [6/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [7/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [8/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [9/64],Loss: 0.0063 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [13/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [14/64],Loss: 0.0759 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [15/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [15/25],Step [16/40],Loss: 0.0023 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.36510359100066125\n",
      "\n",
      "Test set: Loss: 1.2714,Acc_labels: 807/1000 81%\n",
      "Epoch [16/25],Step [1/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [4/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [5/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [7/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [8/64],Loss: 0.0283 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [16/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [10/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [11/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [12/64],Loss: 0.0065 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [13/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [14/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [15/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [16/25],Step [16/40],Loss: 0.0020 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.3645041830604896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Loss: 1.2618,Acc_labels: 806/1000 81%\n",
      "Epoch [17/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [2/64],Loss: 0.0235 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [17/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [4/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [8/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [10/64],Loss: 0.0153 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [11/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [12/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [14/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [15/64],Loss: 0.0037 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [17/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.41155998304020613\n",
      "\n",
      "Test set: Loss: 1.2894,Acc_labels: 811/1000 81%\n",
      "Epoch [18/25],Step [1/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [3/64],Loss: 0.0083 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [4/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [5/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [6/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [7/64],Loss: 0.0023 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [8/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [9/64],Loss: 0.0090 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [10/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [11/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [13/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [14/64],Loss: 0.0028 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [15/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [18/25],Step [16/40],Loss: 0.0000 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.397529762936756\n",
      "\n",
      "Test set: Loss: 1.2584,Acc_labels: 810/1000 81%\n",
      "Epoch [19/25],Step [1/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [3/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [5/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [6/64],Loss: 0.0147 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [19/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [8/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [9/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [11/64],Loss: 0.0012 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [12/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [13/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [15/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [19/25],Step [16/40],Loss: 0.0018 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.4671846760902554\n",
      "\n",
      "Test set: Loss: 1.2982,Acc_labels: 807/1000 81%\n",
      "Epoch [20/25],Step [1/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [2/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [4/64],Loss: 0.0047 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [5/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [8/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [9/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [10/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [11/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [12/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [13/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [14/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [15/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [20/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.5846643190598115\n",
      "\n",
      "Test set: Loss: 1.3077,Acc_labels: 811/1000 81%\n",
      "Epoch [21/25],Step [1/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [2/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [4/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [6/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [9/64],Loss: 0.0016 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [10/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [11/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [12/64],Loss: 0.0693 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [21/25],Step [13/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [14/64],Loss: 0.0074 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [15/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [21/25],Step [16/40],Loss: 0.0010 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.45973772602155805\n",
      "\n",
      "Test set: Loss: 1.3749,Acc_labels: 801/1000 80%\n",
      "Epoch [22/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [2/64],Loss: 0.0034 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [4/64],Loss: 0.0120 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [22/25],Step [5/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [7/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [8/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [9/64],Loss: 0.0043 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [10/64],Loss: 0.0013 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [11/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [12/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [14/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [15/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [22/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.425748276989907\n",
      "\n",
      "Test set: Loss: 1.2728,Acc_labels: 803/1000 80%\n",
      "Epoch [23/25],Step [1/64],Loss: 0.0007 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [2/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [4/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [5/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [6/64],Loss: 0.0067 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [7/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [8/64],Loss: 0.0019 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [9/64],Loss: 0.0247 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [10/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [11/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [12/64],Loss: 0.0041 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [13/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [14/64],Loss: 0.1244 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [23/25],Step [15/64],Loss: 0.0005 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [23/25],Step [16/40],Loss: 0.0003 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.45315636007580906\n",
      "\n",
      "Test set: Loss: 1.2262,Acc_labels: 800/1000 80%\n",
      "Epoch [24/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [2/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [3/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [5/64],Loss: 0.0196 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [6/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [7/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25],Step [8/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [9/64],Loss: 0.0011 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [10/64],Loss: 0.0117 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [24/25],Step [11/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [12/64],Loss: 0.0000 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [13/64],Loss: 0.0014 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [14/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [15/64],Loss: 0.0046 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [24/25],Step [16/40],Loss: 0.0001 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.43399255292024463\n",
      "\n",
      "Test set: Loss: 1.1841,Acc_labels: 809/1000 81%\n",
      "Epoch [25/25],Step [1/64],Loss: 0.0026 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [2/64],Loss: 0.0473 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [3/64],Loss: 0.0001 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [4/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [5/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [6/64],Loss: 0.0076 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [7/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [8/64],Loss: 0.0200 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [9/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [10/64],Loss: 0.0174 ,Acc_labels: 63/64 (98%)\n",
      "Epoch [25/25],Step [11/64],Loss: 0.0024 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [12/64],Loss: 0.0020 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [13/64],Loss: 0.0009 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [14/64],Loss: 0.0025 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [25/25],Step [16/40],Loss: 0.0005 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.38737575395498425\n",
      "\n",
      "Test set: Loss: 1.1815,Acc_labels: 813/1000 81%\n",
      "Epoch [26/25],Step [1/64],Loss: 0.0003 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [2/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [3/64],Loss: 0.0833 ,Acc_labels: 62/64 (97%)\n",
      "Epoch [26/25],Step [4/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [5/64],Loss: 0.0060 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [6/64],Loss: 0.0017 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [7/64],Loss: 0.0106 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [8/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [9/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [10/64],Loss: 0.0032 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [11/64],Loss: 0.0021 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [12/64],Loss: 0.0002 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [13/64],Loss: 0.0008 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [14/64],Loss: 0.0004 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [15/64],Loss: 0.0006 ,Acc_labels: 64/64 (100%)\n",
      "Epoch [26/25],Step [16/40],Loss: 0.0002 ,Acc_labels: 40/40 (100%)\n",
      "\n",
      " Time needed to train  0.40756089601200074\n",
      "\n",
      "Test set: Loss: 1.2750,Acc_labels: 799/1000 80%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " ************************ Statistics of Network 2 ************************ \n",
      "\n",
      "\n",
      " Average time needed for training:  0.23864598533604295\n",
      "\n",
      " Training:  \n",
      " Average accuracy of labels: 82.334140625 \n",
      " Median accuracy of labels: 84.375 \n",
      " Average accuracy on labels: 82.334140625 \n",
      " Maximum accuracy on labels: 100.0 \n",
      " Minimum accuracy on labels: 35.9375 \n",
      " Spread of accuracy on labels: 8.560299374912898 \n",
      " Average accuracy of digits: 75.7025390625 \n",
      " Median accuracy of digits: 79.6875 \n",
      " Average accuracy on digits: 75.7025390625 \n",
      " Maximum accuracy on digits: 95.0 \n",
      " Minimum accuracy on digits: 7.8125 \n",
      " Spread of accuracy on digits: 13.787749256444945\n",
      "\n",
      " Testing:  \n",
      " Average accuracy of labels: 82.4088 \n",
      " Median accuracy of labels: 83.8 \n",
      " Average accuracy on labels: 82.4088 \n",
      " Maximum accuracy on labels: 88.8 \n",
      " Minimum accuracy on labels: 57.0 \n",
      " Spread of accuracy on labels: 5.988637380129368 \n",
      " Average accuracy of digits: 89.4296 \n",
      " Median accuracy of digits: 93.475 \n",
      " Average accuracy on digits: 89.4296 \n",
      " Maximum accuracy on digits: 95.3 \n",
      " Minimum accuracy on digits: 23.9 \n",
      " Spread of accuracy on digits: 10.811490050332273\n",
      "\n",
      " ************************ Statistics of Network 1 ************************ \n",
      "\n",
      "\n",
      " Average time needed for training  0.3934593688538298\n",
      "\n",
      " Training:  \n",
      " Average accuracy of labels: 96.38690135542168 \n",
      " Median accuracy of labels: 99.55078125 \n",
      " Average accuracy on labels: 96.38690135542168 \n",
      " Maximum accuracy on labels: 100.0 \n",
      " Minimum accuracy on labels: 56.85546875 \n",
      " Spread of accuracy on labels: 8.138329875088237\n",
      "\n",
      " Testing:  \n",
      " Average accuracy of labels: 79.7676 \n",
      " Median accuracy of labels: 80.3 \n",
      " Average accuracy on labels: 79.7676 \n",
      " Maximum accuracy on labels: 82.0 \n",
      " Minimum accuracy on labels: 67.1 \n",
      " Spread of accuracy on labels: 1.8977343921880037\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import statistics as st\n",
    "from utils.utils import *\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from timeit import default_timer as timer\n",
    "from nets.NN1 import NN1\n",
    "from nets.NN2 import NN2\n",
    "from nets.train_test import * \n",
    "from utils.MnistPairs import MnistPairs\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#### Parameters ####\n",
    "n_epochs = 25\n",
    "iters = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "#### Parameters ####\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The function \n",
    "    - loads the train and test dataset according to given batchsizes \n",
    "    - trains and tests the first network\n",
    "    - trains and tests the second network\n",
    "    \"\"\"\n",
    "    #Loading dataset\n",
    "    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "    \n",
    "    train_dataset = MnistPairs('data/',train=True, transform=None)\n",
    "    test_dataset = MnistPairs('data/',train=False, transform=None)\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=batch_size_train, \n",
    "                                               shuffle=False)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                              batch_size=batch_size_test, \n",
    "                                              shuffle=False)\n",
    "    \n",
    "    print(\"\\n Start of the Network 2 \")\n",
    "    network2 = NN2()\n",
    "    optimizer2 = optim.SGD(network2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    print(\"The number of parameters of the network is \", count_parameters(network2))\n",
    "    \n",
    "    \n",
    "    train_loss__ = []\n",
    "    train_acc_digits__ = []\n",
    "    train_acc_classes__ = []\n",
    "    \n",
    "    train_loss_ = []\n",
    "    train_acc_digits_ = []\n",
    "    train_acc_classes_= []\n",
    "\n",
    "    test_loss__ = []\n",
    "    test_acc_digits__ = []\n",
    "    test_acc_classes__ = []\n",
    "    avg_time2 = 0\n",
    "    for i in range(iters):\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            start = timer()\n",
    "            train_loss, train_acc_digits, train_acc_classes  = train2(epoch, network2,train_loader,optimizer2) \n",
    "            end = timer()\n",
    "            print(\"Time needed to train \", end - start)\n",
    "            avg_time2 += end - start\n",
    "            test_loss, test_acc_digits, test_acc_classes = test2(network2, test_loader)\n",
    "            \n",
    "            train_loss__ += train_loss\n",
    "            train_acc_digits__ += train_acc_digits\n",
    "            train_acc_classes__ += train_acc_classes\n",
    "            \n",
    "            train_loss_.append(mean(train_loss))\n",
    "            train_acc_digits_.append(mean(train_acc_digits))\n",
    "            train_acc_classes_.append(mean(train_acc_classes))\n",
    "            \n",
    "            test_loss__.append(test_loss)\n",
    "            test_acc_digits__.append(test_acc_digits)\n",
    "            test_acc_classes__.append(test_acc_classes)\n",
    "            \n",
    "    avg_time2 /=(iters * n_epochs)\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss__, label = \"train_loss__\") \n",
    "    plt.ylabel(\"Negative log likelihood loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(\"Network 2 Classification Loss \")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_acc_digits__, label = \"train_acc_digits\")\n",
    "    plt.plot(train_acc_classes__, label = \"train_acc_classes\") \n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(\"Network 2 Classification Accuracy \")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_loss__, label = \"test_loss\") \n",
    "    plt.plot(train_loss_, label = \"train_loss\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Negative log likelihood loss\")\n",
    "    plt.xlabel(\"Number of seen testing data\")\n",
    "    plt.title(\"Network 2 Classification Loss \")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_acc_digits__, label = \"test_acc_digits\")\n",
    "    plt.plot(test_acc_classes__, label = \"test_acc_classes\")\n",
    "    plt.plot(train_acc_digits_, label = \"train_acc_digits\")\n",
    "    plt.plot(train_acc_classes_, label = \"train_acc_classes\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Number of seen data\")\n",
    "    plt.title(\"Network 2 Classification Accuracy\")\n",
    "    plt.show()\n",
    "                      \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    print(\"*************************** Start of the Network 1 ***************************\")\n",
    "    network1 = NN1()\n",
    "    optimizer1 = optim.SGD(network1.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    train_loss__ = []\n",
    "    train_acc__ = []\n",
    "    \n",
    "    train_loss_ = []\n",
    "    train_acc_ = []\n",
    "\n",
    "    test_loss__ = []\n",
    "    test_acc__ = []\n",
    "    avg_time1 = 0\n",
    "    \n",
    "    print(\"\\n The number of parameters of the network is \\n \", count_parameters(network1))\n",
    "    for i in range(iters):\n",
    "        t_loss = []\n",
    "        t_acc = []\n",
    "        test_loss_ = []\n",
    "        test_acc_ = []\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            start = timer()\n",
    "            train_loss, train_acc  = train1(epoch, network1, train_loader, optimizer1) \n",
    "            end = timer()\n",
    "            print(\"\\n Time needed to train \", end - start)\n",
    "            avg_time1 += end - start\n",
    "            \n",
    "            test_loss, test_acc = test1(network1, test_loader)\n",
    "            \n",
    "            train_loss__ += train_loss\n",
    "            train_acc__ += train_acc_\n",
    "            \n",
    "            train_loss_.append(mean(train_loss))\n",
    "            train_acc_.append(mean(train_acc))\n",
    "            \n",
    "            test_loss__.append(test_loss)\n",
    "            test_acc__.append(test_acc)\n",
    "            \n",
    "    avg_time1 /= (iters * n_epochs)\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss__, label = \"train_loss__\") \n",
    "    plt.ylabel(\"Cross Entropy loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(\"Network 1 Classification Loss \")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_loss__, label = \"test_loss\") \n",
    "    plt.plot(train_loss_, label = \"train_loss\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Cross Entropy loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(\"Network 1 Classification Loss \")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(test_acc_digits__, label = \"test_acc_digits\")\n",
    "    plt.plot(test_acc_classes__, label = \"test_acc_classes\")\n",
    "    plt.plot(train_acc_digits_, label = \"train_acc_digits\")\n",
    "    plt.plot(train_acc_classes_, label = \"train_acc_classes\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.title(\"Network 1 Classification Accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_time2, train_acc_classes__, train_acc_digits__ , test_acc_classes__, test_acc_digits__, avg_time1, train_acc__, test_acc__\n",
    "      \n",
    "        \n",
    "        \n",
    "       \n",
    "if __name__ == '__main__':\n",
    "    avg_time2, train_acc_classes__, train_acc_digits__ , test_acc_classes__, test_acc_digits__, avg_time1, train_acc__, test_acc__ = main()\n",
    "    print(\" \\n ************************ Statistics of Network 2 ************************ \\n\")\n",
    "    print(\"\\n Average time needed for training: \", avg_time2)\n",
    "    print(\"\\n Training: \",\"\\n Average accuracy of labels: {}\".format(st.mean(train_acc_classes__)),\"\\n Median accuracy of labels: {}\".format(st.median(train_acc_classes__)),\"\\n Average accuracy on labels: {}\".format(st.mean(train_acc_classes__)),\"\\n Maximum accuracy on labels: {}\".format(max(train_acc_classes__)),\"\\n Minimum accuracy on labels: {}\".format(min(train_acc_classes__)),\"\\n Spread of accuracy on labels: {}\".format(st.stdev(train_acc_classes__)),\"\\n Average accuracy of digits: {}\".format(st.mean(train_acc_digits__)),\"\\n Median accuracy of digits: {}\".format(st.median(train_acc_digits__)),\"\\n Average accuracy on digits: {}\".format(st.mean(train_acc_digits__)),\"\\n Maximum accuracy on digits: {}\".format(max(train_acc_digits__)),\"\\n Minimum accuracy on digits: {}\".format(min(train_acc_digits__)),\"\\n Spread of accuracy on digits: {}\".format(st.stdev(train_acc_digits__)))\n",
    "    print(\"\\n Testing: \",\"\\n Average accuracy of labels: {}\".format(st.mean(test_acc_classes__)),\"\\n Median accuracy of labels: {}\".format(st.median(test_acc_classes__)),\"\\n Average accuracy on labels: {}\".format(st.mean(test_acc_classes__)),\"\\n Maximum accuracy on labels: {}\".format(max(test_acc_classes__)),\"\\n Minimum accuracy on labels: {}\".format(min(test_acc_classes__)),\"\\n Spread of accuracy on labels: {}\".format(st.stdev(test_acc_classes__)),\"\\n Average accuracy of digits: {}\".format(st.mean(test_acc_digits__)),\"\\n Median accuracy of digits: {}\".format(st.median(test_acc_digits__)),\"\\n Average accuracy on digits: {}\".format(st.mean(test_acc_digits__)),\"\\n Maximum accuracy on digits: {}\".format(max(test_acc_digits__)),\"\\n Minimum accuracy on digits: {}\".format(min(test_acc_digits__)),\"\\n Spread of accuracy on digits: {}\".format(st.stdev(test_acc_digits__)))\n",
    "    \n",
    "    \n",
    "    print(\"\\n ************************ Statistics of Network 1 ************************ \\n\")\n",
    "    print(\"\\n Average time needed for training \", avg_time1)\n",
    "    print(\"\\n Training: \", \"\\n Average accuracy of labels: {}\".format(st.mean(train_acc__)),\"\\n Median accuracy of labels: {}\".format(st.median(train_acc__)),\"\\n Average accuracy on labels: {}\".format(st.mean(train_acc__)),\"\\n Maximum accuracy on labels: {}\".format(max(train_acc__)),\"\\n Minimum accuracy on labels: {}\".format(min(train_acc__)),\"\\n Spread of accuracy on labels: {}\".format(st.stdev(train_acc__)))\n",
    "    print(\"\\n Testing: \", \"\\n Average accuracy of labels: {}\".format(st.mean(test_acc__)),\"\\n Median accuracy of labels: {}\".format(st.median(test_acc__)), \"\\n Average accuracy on labels: {}\".format(st.mean(test_acc__)),\"\\n Maximum accuracy on labels: {}\".format(max(test_acc__)), \"\\n Minimum accuracy on labels: {}\".format(min(test_acc__)), \"\\n Spread of accuracy on labels: {}\".format(st.stdev(test_acc__)))\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
