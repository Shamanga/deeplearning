{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.output = input[0]\n",
    "        \n",
    "        return self.relu(self.output)\n",
    "    \n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.relu_p(self.output)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "    \n",
    "    \n",
    "    def relu(self, x):\n",
    "        \n",
    "        return torch.clamp(x, min =0)\n",
    "    \n",
    "    def relu_p(self, x):\n",
    "\n",
    "        x[x>0] = 1\n",
    "        x[x<=0] = 0\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.output = input[0]\n",
    "        \n",
    "        return self.tanh(self.output)\n",
    "    \n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.tanh_p(self.output)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "    \n",
    "    def tanh(self, to_compute):\n",
    "        \n",
    "        numerator = torch.exp(to_compute) - torch.exp(-to_compute)\n",
    "        denominator = torch.exp(to_compute) + torch.exp(-to_compute)\n",
    "        \n",
    "        return numerator/denominator\n",
    "        \n",
    "        \n",
    "    def tanh_p(self, x):\n",
    "\n",
    "        return (1 - torch.pow(self.tanh(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.input_from_layer = input[0]\n",
    "        \n",
    "        return self.softmax(input_from_layer)\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.softmax_p(self.input_from_layer)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "        \n",
    "    \n",
    "    def softmax(self, input_to_compute):\n",
    "        \n",
    "        input_to_compute_v = input_to_compute.view(-1,1)\n",
    "        \n",
    "        norm_value = input_to_compute_v.max()\n",
    "        \n",
    "        stable_input_to_compute_v = input_to_compute_v - norm_value\n",
    "        \n",
    "        exponentials = torch.exp(stable_input_to_compute_v)\n",
    "        \n",
    "        sum_exponentials = torch.sum(exponentials)\n",
    "        \n",
    "        return (exponentials/sum_exponentials).view(-1)\n",
    "    \n",
    "    def softmax_p(self, input_to_compute_p):\n",
    "        \n",
    "        softmax_res = self.softmax(input_to_compute_p)\n",
    "        \n",
    "        diag_softm = torch.diag(softmax_res)\n",
    "        \n",
    "        derivative_soft = torch.FloatTensor(diag_softm.shape[0], diag_softm.shape[0])\n",
    "        \n",
    "        for i in range((diag_softm.shape[0])):\n",
    "            for j in range((diag_softm.shape[0])):\n",
    "                if i == j:\n",
    "                    derivative_soft[i][j] = softmax_res[i] * (1 - softmax_res[i])\n",
    "                else:\n",
    "                    derivative_soft[i][j] = -softmax_res[i] * softmax_res[j]\n",
    "                    \n",
    "        return derivative_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, initial_input):\n",
    "        \n",
    "        # initial input\n",
    "        output_single_layer = initial_input\n",
    "        \n",
    "        # Iterate through the layers of the network\n",
    "        # Pass the input to the forward function of the first layer\n",
    "        # and keep iterating over the layers and passing the output\n",
    "        # of the layer before to the one after\n",
    "        for layer in self.layers:\n",
    "            output_single_layer = layer.forward(output_single_layer)\n",
    "        \n",
    "        # The last output of the network\n",
    "        return output_single_layer\n",
    "    \n",
    "    def backward(self, initial_backward_input):\n",
    "        \n",
    "        # Starting with the derivative of the loss\n",
    "        # We backpropagate by calling the backward\n",
    "        # function of each layer \n",
    "        output_single_layer_backward = initial_backward_input\n",
    "        \n",
    "        for layer in self.layers[::-1]:\n",
    "            \n",
    "            output_single_layer_backward = layer.backward(output_single_layer_backward)\n",
    "            \n",
    "        return output_single_layer_backward\n",
    "    \n",
    "    def param(self):\n",
    "        \n",
    "        parameters_of_each_layer = []\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            parameters_of_each_layer.append(layer.param)\n",
    "            \n",
    "        return parameters_of_each_layer\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, number_of_examples: int, number_of_features: int, batch_size = 2, shuffle= True):\n",
    "        \n",
    "        self.number_of_examples = number_of_examples\n",
    "        self.number_of_features = number_of_features\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.radius_circle = 1/math.sqrt(2*math.pi)\n",
    "        self.center_circle = [0.5,0.5]\n",
    "    \n",
    "    def check_target(self, input_example):\n",
    "\n",
    "\n",
    "        if math.pow(input_example[0] - self.center_circle[0], 2)\\\n",
    "            + math.pow(input_example[1] - self.center_circle[1], 2)\\\n",
    "                < math.pow(self.radius_circle,2):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "\n",
    "        self.data = torch.FloatTensor(self.number_of_examples, self.number_of_features).uniform_(0,1)\n",
    "\n",
    "        self.targets = torch.LongTensor(self.number_of_examples)\n",
    "        index_targets = torch.arange(0, self.number_of_examples)\n",
    "\n",
    "        self.targets = index_targets.apply_(lambda i: self.check_target(self.data[i]))\n",
    "\n",
    "        return self.data, self.targets    \n",
    "    \n",
    "    def yield_data(self):\n",
    "        \n",
    "        if self.shuffle==True:\n",
    "            \n",
    "            shuffled_indexes = torch.randperm(self.number_of_examples)\n",
    "            self.data_shuffled = self.data[shuffled_indexes]\n",
    "            self.targets_shuffled = self.targets[shuffled_indexes]\n",
    "            \n",
    "            for batch_start in range(0, self.number_of_examples, self.batch_size):\n",
    "                \n",
    "                if self.number_of_examples - batch_start >= self.batch_size:\n",
    "                    yield self.data_shuffled.narrow(0, batch_start, self.batch_size),\\\n",
    "                          self.targets_shuffled.narrow(0, batch_start, self.batch_size)\n",
    "                else:\n",
    "                    yield self.data_shuffled.narrow(0, batch_start, self.number_of_examples - batch_start),\\\n",
    "                          self.targets_shuffled.narrow(0, batch_start, self.number_of_examples - batch_start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for batch_start in range(0, self.number_of_examples, self.batch_size):\n",
    "                \n",
    "                if self.number_of_examples - batch_start >= self.batch_size:\n",
    "                    yield self.data.narrow(0, batch_start, self.batch_size),\\\n",
    "                          self.targets.narrow(0, batch_start, self.batch_size)\n",
    "                else:\n",
    "                    yield self.data.narrow(0, batch_start, self.number_of_examples - batch_start),\\\n",
    "                          self.targets.narrow(0, batch_start, self.number_of_examples - batch_start)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        \n",
    "        # Number of input neurons\n",
    "        self.in_features = in_features\n",
    "        # Number of output neurons\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initializing the weights with Xavierâ€™s initialization\n",
    "        # First generate the weights from a normal distribution with mean 0 and std 1\n",
    "        # Then multiply the samples by sqrt(1 / (number_of_input_neurons + number_of_output_neurons))\n",
    "        self.weight = torch.mul(torch.Tensor(out_features, in_features).normal_(mean=0, std=1), \\\n",
    "                                torch.sqrt(torch.FloatTensor([1/ (self.in_features + self.out_features)])))\n",
    "        \n",
    "        # Zero bias initialization\n",
    "        self.bias = torch.Tensor(out_features).zero_()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        # Input from the layer\n",
    "        self.input_from_layer = input[0]\n",
    "        \n",
    "        # Calculating the output, which is basically the multiplication\n",
    "        # of the weights with the input layer and adding the bias\n",
    "        self.output = torch.mv(self.weights, self.input_from_layer) + self.bias\n",
    "        \n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tensors to accumulate the gradients during backprop\n",
    "# remember to initialize to zero at the beginning of every mini-batch step\n",
    "self.dl_dw = Tensor(self.weights.size()).zero_()\n",
    "self.dl_db = Tensor(self.bias.size()).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, *gradwrtoutput):\n",
    "\n",
    "        # for a linear layer l, the gradwrtoutput will be the grad output\n",
    "        # from the activation module, that is the product of dsigma(s_{l})\n",
    "        # and the grad wrt the output of the activation function\n",
    "        grad_wrt_s_l = gradwrtoutput[0]\n",
    "\n",
    "        # compute the grad wrt the input of previous layer (x_{l-1})\n",
    "        grad_wrt_input_prev_layer = self.weights.t().mv(grad_wrt_s_l)\n",
    "\n",
    "        # compute the grad wrt the weights of this layer\n",
    "        # accumulate the grad in our specific tensor\n",
    "        self.dl_dw.add_(grad_wrt_s_l.view(-1, 1).mm(self.input_prec_layer.view(1, -1)))\n",
    "\n",
    "        # compute grad wrt the bias term\n",
    "        self.dl_db.add_(grad_wrt_s_l)\n",
    "\n",
    "        return grad_wrt_input_prev_layer\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        returns pair of tensors: first is a parameter tensor,\n",
    "        the second is the gradient accumulator for this parameter tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return [(self.weights, self.dl_dw), (self.bias, self.dl_db)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \n",
    "    def cross_entropy_torch(predictions, targets, epsilon=1e-20):\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets\n",
    "        and predictions. \n",
    "        Input: predictions (N, #ofclasses) FloatTensor\n",
    "               targets (N, 1) LongTensor        \n",
    "        Returns: average loss\n",
    "        \"\"\"\n",
    "        # Clamping the predictions between epsilon and 1 - epsilon\n",
    "        predictions_clamped = predictions.clamp(epsilon, 1-epsilon)\n",
    "        # Obtaining the probabilities of the target class\n",
    "        to_compute_loss = predictions_clamped.gather(1, targets.unsqueeze(1))\n",
    "\n",
    "        # Computing the loss\n",
    "        log_loss = -torch.log(to_compute_loss)\n",
    "\n",
    "        # Computing the mean of the loss\n",
    "        average_ce_loss = torch.mean(log_loss)\n",
    "\n",
    "        return average_ce_loss\n",
    "    \n",
    "    \n",
    "    def cross_entropy_troch_p(predictions, targets, epsilon=1e-20):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes cross entropy derivative between targets\n",
    "        and predictions. \n",
    "        Input: predictions (N, #ofclasses) FloatTensor\n",
    "               targets (N, 1) LongTensor        \n",
    "        Returns: derivative\n",
    "        \"\"\"\n",
    "        \n",
    "        # Number of examples\n",
    "        numb_ex = targets.shape[0]\n",
    "\n",
    "        # Clamping the predictions between epsilon and 1 - epsilon\n",
    "        predictions_clamped = predictions.clamp(epsilon, 1-epsilon)\n",
    "\n",
    "        # Computing derivative\n",
    "    \n",
    "        predictions_clamped[range(predictions_clamped.shape[0]), targets] -= 1\n",
    "\n",
    "        derivative = predictions_clamped / numb_ex\n",
    "\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse:\n",
    "    \n",
    "    def mse(self, predictions, targets):\n",
    "        \n",
    "        # 1/2n (x - y) ^2\n",
    "        return (torch.pow(predictions - targets, 2).mean()) / 2\n",
    "\n",
    "    def mse_p(self, predictions, targets):\n",
    "        \n",
    "        # 1/n (x - y)\n",
    "        return (predictions - targets) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGD:\n",
    "    \n",
    "    def __init__(self, parameters_list, learning_rate = 0.01):\n",
    "        \n",
    "        self.parameters_list = parameters_list\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "samples = 1000\n",
    "features = 2\n",
    "batch = 100\n",
    "to_shuffle = True\n",
    "#### Parameters ####\n",
    "\n",
    "train_class = DataGenerator(number_of_examples= samples, number_of_features= features, batch_size = batch, shuffle = to_shuffle)\n",
    "test_class = DataGenerator(number_of_examples=samples, number_of_features= features, batch_size = batch, shuffle = to_shuffle)\n",
    "\n",
    "train_data, train_target = train_class.generate_data()\n",
    "test_data, test_target = test_class.generate_data()\n",
    "\n",
    "train_generator = train_class.yield_data()\n",
    "test_generator = test_class.yield_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Linear(in_features= features, out_features = 25)\n",
    "first_hidden_layer = Linear(in_features = 25, out_features = 25)\n",
    "second_hidden_layer = Linear(in_features= 25, out_features = 25)\n",
    "output_layer = Linear(in_features= 25, out_features = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# generating our data\n",
    "train_inputs, train_targets = generate_data(1000, 2)\n",
    "test_inputs, test_targets = generate_data(1000, 2)\n",
    "\n",
    "# creating our loaders for training and test sets\n",
    "train_loader = DataLoader(train_inputs, train_targets, batch_size)\n",
    "test_loader = DataLoader(test_inputs, test_targets, batch_size)\n",
    "\n",
    "# defining our layers\n",
    "layers = [Linear(input_dim=train_inputs[0].shape[0], output_dim=25), Relu(),\n",
    "          Linear(input_dim=25, output_dim=25), Relu(),\n",
    "          Linear(input_dim=25, output_dim=2), Tanh()]\n",
    "\n",
    "# creating our model\n",
    "model = Sequential(layers)\n",
    "\n",
    "# init our optimizer\n",
    "optimizer = SGD(model.get_params(), lr=0.01)\n",
    "\n",
    "# init our trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  epochs=500,\n",
    "                  loss=LossMSE(),\n",
    "                  train_loader=train_loader,\n",
    "                  test_loader=test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
