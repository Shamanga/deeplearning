{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks of a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    abstract class used for our layers\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "\n",
    "def tanh(x: torch.FloatTensor):\n",
    "    \n",
    "    numerator = torch.exp(x) - torch.exp(-x)\n",
    "    denominator = torch.exp(x) + torch.exp(-x)\n",
    "    \n",
    "    return numerator/denominator\n",
    "\n",
    "# Derivative\n",
    "def tanh_p(x: torch.FloatTensor):\n",
    "    \n",
    "    return (1 - torch.pow(tanh(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9640])\n",
      "tensor([0.0707])\n"
     ]
    }
   ],
   "source": [
    "print(tanh(a))\n",
    "print(tanh_p(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9640], grad_fn=<TanhBackward>)\n",
      "tensor([0.0707])\n"
     ]
    }
   ],
   "source": [
    "b = torch.FloatTensor([2.0]) \n",
    "b.requires_grad_(True)\n",
    "\n",
    "y = torch.tanh(b)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: torch.FloatTensor):\n",
    "    return torch.clamp(x,min =0)\n",
    "\n",
    "def relu_p(x: torch.FloatTensor):\n",
    "    \n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = 0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "print(relu(a))\n",
    "print(relu_p(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], grad_fn=<ReluBackward0>)\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "b = torch.FloatTensor([2.0]) \n",
    "b.requires_grad_(True)\n",
    "\n",
    "y = torch.relu(b)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: torch.FloatTensor):\n",
    "    return (1 / (1 + torch.exp(-x)))\n",
    "\n",
    "def sigmoid_p(x: torch.FloatTensor):\n",
    "\n",
    "    return (sigmoid(x)*(1 - sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8808])\n",
      "tensor([0.1050])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "print(sigmoid(a))\n",
    "print(sigmoid_p(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8808], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.1050])\n"
     ]
    }
   ],
   "source": [
    "b = torch.FloatTensor([2.0]) \n",
    "b.requires_grad_(True)\n",
    "\n",
    "y = torch.sigmoid(b)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeakyRelu Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(torch.FloatTensor([1.0,2.0,3.0]), max = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRelu(x: torch.FloatTensor, slope: float):\n",
    "    return torch.clamp(x, min = 0) + slope*torch.clamp(x, max=0)\n",
    "\n",
    "def LRelu_p(x: torch.FloatTensor, slope: float):\n",
    "    \n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = slope\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "print(LRelu(a, slope = 0.01))\n",
    "print(LRelu_p(a, slope = 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.FloatTensor([1.0,1.0,1.0,0.0,0.0,0.0])\n",
    "target = torch.FloatTensor([1.0,1.0,0.0,1.0,0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_output: torch.FloatTensor, target_output: torch.FloatTensor):\n",
    "    \n",
    "    return torch.pow(predicted_output - target_output, 2).sum()\n",
    "\n",
    "def mse_p(predicted_output: torch.FloatTensor, target_output: torch.FloatTensor):\n",
    "    return 2*(predicted_output - target_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([1.0,2.0,3.0]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.999999e-01, 1.000000e-07, 1.000000e-07, 1.000000e-07])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip([1,0,0,0], 0.0000001, 0.9999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_params, lr=0.01):\n",
    "        \"\"\"\n",
    "        saves the learning rate  and all the parameters\n",
    "        and gradient accumulators of the network to optimize\n",
    "        \"\"\"\n",
    "        self.model_params = model_params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        updates all the parameters of the models\n",
    "        using the respective gradient accumulator\n",
    "        \"\"\"\n",
    "        for layers_params in self.model_params:\n",
    "            for param_update in layers_params:\n",
    "                param = param_update[0]\n",
    "                update = param_update[1]\n",
    "\n",
    "                # updating the parameter\n",
    "                param -= self.lr * update\n",
    "\n",
    "                # initialize to zero the accumulator\n",
    "                update.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGD:\n",
    "    \n",
    "    def __init__(self, parameters_list, learning_rate = 0.01):\n",
    "        \n",
    "        self.parameters_list = parameters_list\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_torch(predictions, targets, epsilon=1e-20):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, #ofclasses) FloatTensor\n",
    "           targets (N, 1) LongTensor        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    # Clamping the predictions between epsilon and 1 - epsilon\n",
    "    predictions_clamped = predictions.clamp(epsilon, 1-epsilon)\n",
    "    # Obtaining the probabilities of the target class\n",
    "    to_compute_loss = predictions_clamped.gather(1, targets.unsqueeze(1))\n",
    "    \n",
    "    # Computing the loss\n",
    "    log_loss = -torch.log(to_compute_loss)\n",
    "    \n",
    "    # Computing the mean of the loss\n",
    "    average_ce_loss = torch.mean(log_loss)\n",
    "    \n",
    "    return average_ce_loss\n",
    "\n",
    "\n",
    "def cross_entropy_troch_p(predictions, targets, epsilon=1e-20):\n",
    "    \n",
    "    # Number of examples\n",
    "    numb_ex = targets.shape[0]\n",
    "    \n",
    "    # Clamping the predictions between epsilon and 1 - epsilon\n",
    "    predictions_clamped = predictions.clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "    # Computing derivative\n",
    "    \n",
    "    predictions_clamped[range(predictions_clamped.shape[0]), targets] -= 1\n",
    "    \n",
    "    derivative = predictions_clamped / numb_ex\n",
    "    \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, number_of_examples, number_of_features, batch_size = 2, shuffle= True):\n",
    "        \n",
    "        self.number_of_examples = number_of_examples\n",
    "        self.number_of_features = number_of_features\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.radius_circle = 1/math.sqrt(2*math.pi)\n",
    "        self.center_circle = [0.5,0.5]\n",
    "    \n",
    "    def check_target(self, input_example):\n",
    "\n",
    "\n",
    "        if math.pow(input_example[0] - self.center_circle[0], 2)\\\n",
    "            + math.pow(input_example[1] - self.center_circle[1], 2)\\\n",
    "                < math.pow(self.radius_circle,2):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "\n",
    "        self.data = torch.FloatTensor(self.number_of_examples, self.number_of_features).uniform_(0,1)\n",
    "\n",
    "        self.targets = torch.LongTensor(self.number_of_examples)\n",
    "        index_targets = torch.arange(0, self.number_of_examples)\n",
    "\n",
    "        self.targets = index_targets.apply_(lambda i: check_target(self.data[i]))\n",
    "\n",
    "        return self.data, self.targets    \n",
    "    \n",
    "    def yield_data(self):\n",
    "        \n",
    "        if self.shuffle==True:\n",
    "            \n",
    "            shuffled_indexes = torch.randperm(self.number_of_examples)\n",
    "            self.data_shuffled = self.data[shuffled_indexes]\n",
    "            self.targets_shuffled = self.targets[shuffled_indexes]\n",
    "            \n",
    "            for batch_start in range(0, self.number_of_examples, self.batch_size):\n",
    "                \n",
    "                if self.number_of_examples - batch_start >= self.batch_size:\n",
    "                    yield self.data_shuffled.narrow(0, batch_start, self.batch_size),\\\n",
    "                          self.targets_shuffled.narrow(0, batch_start, self.batch_size)\n",
    "                else:\n",
    "                    yield self.data_shuffled.narrow(0, batch_start, self.number_of_examples - batch_start),\\\n",
    "                          self.targets_shuffled.narrow(0, batch_start, self.number_of_examples - batch_start)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for batch_start in range(0, self.number_of_examples, self.batch_size):\n",
    "                \n",
    "                if self.number_of_examples - batch_start >= self.batch_size:\n",
    "                    yield self.data.narrow(0, batch_start, self.batch_size),\\\n",
    "                          self.targets.narrow(0, batch_start, self.batch_size)\n",
    "                else:\n",
    "                    yield self.data.narrow(0, batch_start, self.number_of_examples - batch_start),\\\n",
    "                          self.targets.narrow(0, batch_start, self.number_of_examples - batch_start)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_accuracy(self, is_training=True):\n",
    "\n",
    "        epoch_losses = []\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        loader = self.train_loader if is_training else self.test_loader\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(loader.get_loader()):\n",
    "\n",
    "            batch_loss = 0\n",
    "            for x, y in zip(x_batch, y_batch):\n",
    "                predicted = self.model.forward(x)\n",
    "                total += 1\n",
    "                if predicted.max(0)[1] == y.max(0)[1]:\n",
    "                    correct += 1\n",
    "                loss_value = self.loss.compute(predicted, y)\n",
    "                batch_loss += loss_value\n",
    "                epoch_losses.append(loss_value)\n",
    "\n",
    "                # computing the backward pass for the training part\n",
    "                if is_training:\n",
    "                    derivative_loss = self.loss.derivative(predicted, y)\n",
    "                    self.model.backward(derivative_loss)\n",
    "\n",
    "            # updating the model weights during training at the end of the batch\n",
    "            if is_training:\n",
    "                self.optimizer.step()\n",
    "\n",
    "        epoch_val_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        return epoch_val_loss, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataGenerator(number_of_examples=5, number_of_features=2)\n",
    "training_data, training_target = train_data.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(predicted, target):\n",
    "        target = target.view(-1,1).max(0)[1]\n",
    "        predicted = predicted.view(1, -1)\n",
    "\n",
    "        # computing the negative log likelihood\n",
    "        log_likelihood = -torch.log(predicted[range(predicted.shape[0]), target])\n",
    "        loss = torch.sum(log_likelihood) / predicted.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.96]])\n",
    "targets = np.array([[0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "ans = 0.71355817782  #Correct answer\n",
    "x = cross_entropy(predictions, targets)\n",
    "print(np.isclose(x,ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_t = torch.FloatTensor([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.96]])\n",
    "\n",
    "targets_t = torch.FloatTensor([[0,0,0,1],\n",
    "                   [0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = cross_entropy_torch(predictions_t,targets_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7136)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7135581752992395"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(torch.FloatTensor([1,2,4]) - torch.FloatTensor([0,1,2]), 2).mean() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.FloatTensor([1,2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse:\n",
    "    \n",
    "    def mse(predicted_output: torch.FloatTensor, target_output: torch.FloatTensor):\n",
    "    \n",
    "        return torch.pow(predicted_output - target_output, 2).sum()\n",
    "\n",
    "    def mse_p(predicted_output: torch.FloatTensor, target_output: torch.FloatTensor):\n",
    "        return 2*(predicted_output - target_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0662,  4.4729])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.randn(2, 3)\n",
    "vec = torch.randn(3)\n",
    "torch.mv(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (0.20, 0.40, 0.30, 0.10). Then you roll the dice many thousands of times and determine that the true probabilities are (0.15, 0.35, 0.25, 0.25). The CE error for your prediction is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]],\n",
    "          [[9.0, 10.0], [11.0, 12.0]]]\n",
    "T = torch.tensor(T_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([[[0.0,1.0]],[]])\n",
    "b = torch.FloatTensor([[0.15,0.35,0.25,0.25],[0.4,0.2,0.2,0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.log(predicted[range(predicted.shape[0]), target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.output = input[0]\n",
    "        \n",
    "        return self.relu(self.output)\n",
    "    \n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.relu_p(self.output)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "    \n",
    "    \n",
    "    def relu(self, x):\n",
    "        \n",
    "        return torch.clamp(x, min =0)\n",
    "    \n",
    "    def relu_p(self, x):\n",
    "\n",
    "        x[x>0] = 1\n",
    "        x[x<=0] = 0\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.output = input[0]\n",
    "        \n",
    "        return self.tanh(self.output)\n",
    "    \n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.tanh_p(self.output)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "    \n",
    "    def tanh(self, to_compute):\n",
    "        \n",
    "        numerator = torch.exp(to_compute) - torch.exp(-to_compute)\n",
    "        denominator = torch.exp(to_compute) + torch.exp(-to_compute)\n",
    "        \n",
    "        return numerator/denominator\n",
    "        \n",
    "        \n",
    "    def tanh_p(self, x):\n",
    "\n",
    "        return (1 - torch.pow(self.tanh(x),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        \n",
    "        # Number of input neurons\n",
    "        self.in_features = in_features\n",
    "        # Number of output neurons\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initializing the weights with Xavier’s initialization\n",
    "        # First generate the weights from a normal distribution with mean 0 and std 1\n",
    "        # Then multiply the samples by sqrt(1 / (number_of_input_neurons + number_of_output_neurons))\n",
    "        self.weight = torch.mul(torch.Tensor(out_features, in_features).normal_(mean=0, std=1), \\ \n",
    "                                torch.sqrt(torch.FloatTensor([1/ (self.in_features + self.out_features)])))\n",
    "        \n",
    "        # Zero bias initialization\n",
    "        self.bias = torch.Tensor(out_features).zero_()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        # Input from the layer\n",
    "        self.input_from_layer = input[0]\n",
    "        \n",
    "        # Calculating the output, which is basically the multiplication\n",
    "        # of the weights with the input layer and adding the bias\n",
    "        self.output = torch.mv(weights, self.input_from_layer) + self.bias\n",
    "        \n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, initial_input):\n",
    "        \n",
    "        # initial input\n",
    "        output_single_layer = initial_input\n",
    "        \n",
    "        # Iterate through the layers of the network\n",
    "        # Pass the input to the forward function of the first layer\n",
    "        # and keep iterating over the layers and passing the output\n",
    "        # of the layer before to the one after\n",
    "        for layer in self.layers:\n",
    "            output_single_layer = layer.forward(output_single_layer)\n",
    "        \n",
    "        # The last output of the network\n",
    "        return output_single_layer\n",
    "    \n",
    "    def backward(self, initial_backward_input):\n",
    "        \n",
    "        # Starting with the derivative of the loss\n",
    "        # We backpropagate by calling the backward\n",
    "        # function of each layer \n",
    "        output_single_layer_backward = initial_backward_input\n",
    "        \n",
    "        for layer in self.layers[::-1]:\n",
    "            \n",
    "            output_single_layer_backward = layer.backward(output_single_layer_backward)\n",
    "            \n",
    "        return output_single_layer_backward\n",
    "    \n",
    "    def param(self):\n",
    "        \n",
    "        parameters_of_each_layer = []\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            parameters_of_each_layer.append(layer.param)\n",
    "            \n",
    "        return parameters_of_each_layer\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.input_from_layer = input[0]\n",
    "        \n",
    "        return self.softmax(input_from_layer)\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.softmax_p(self.input_from_layer)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "        \n",
    "    \n",
    "    def softmax(self, input_to_compute):\n",
    "        \n",
    "        input_to_compute_v = input_to_compute.view(-1,1)\n",
    "        \n",
    "        norm_value = input_to_compute_v.max()\n",
    "        \n",
    "        stable_input_to_compute_v = input_to_compute_v - norm_value\n",
    "        \n",
    "        exponentials = torch.exp(stable_input_to_compute_v)\n",
    "        \n",
    "        sum_exponentials = torch.sum(exponentials)\n",
    "        \n",
    "        return (exponentials/sum_exponentials).view(-1)\n",
    "    \n",
    "    def softmax_p(self, input_to_compute_p):\n",
    "        \n",
    "        softmax_res = self.softmax(input_to_compute_p)\n",
    "        \n",
    "        diag_softm = torch.diag(softmax_res)\n",
    "        \n",
    "        derivative_soft = torch.FloatTensor(diag_softm.shape[0], diag_softm.shape[0])\n",
    "        \n",
    "        for i in range((diag_softm.shape[0])):\n",
    "            for j in range((diag_softm.shape[0])):\n",
    "                if i == j:\n",
    "                    derivative_soft[i][j] = softmax_res[i] * (1 - softmax_res[i])\n",
    "                else:\n",
    "                    derivative_soft[i][j] = -softmax_res[i] * softmax_res[j]\n",
    "                    \n",
    "        return derivative_soft\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_accuracy(self, is_training=True):\n",
    "\n",
    "    epoch_losses = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    loader = self.train_loader if is_training else self.test_loader\n",
    "\n",
    "    for j, (x_batch, y_batch) in enumerate(loader.get_loader()):\n",
    "\n",
    "        batch_loss = 0\n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            predicted = self.model.forward(x)\n",
    "            total += 1\n",
    "            if predicted.max(0)[1] == y.max(0)[1]:\n",
    "                correct += 1\n",
    "            loss_value = self.loss.compute(predicted, y)\n",
    "            batch_loss += loss_value\n",
    "            epoch_losses.append(loss_value)\n",
    "\n",
    "            # computing the backward pass for the training part\n",
    "            if is_training:\n",
    "                derivative_loss = self.loss.derivative(predicted, y)\n",
    "                self.model.backward(derivative_loss)\n",
    "\n",
    "        # updating the model weights during training at the end of the batch\n",
    "        if is_training:\n",
    "            self.optimizer.step()\n",
    "\n",
    "    epoch_val_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    return epoch_val_loss, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, \\text{in\\_features})` where :math:`*` means any number of\n",
    "          additional dimensions\n",
    "        - Output: :math:`(N, *, \\text{out\\_features})` where all but the last dimension\n",
    "          are the same shape as the input.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['bias']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
