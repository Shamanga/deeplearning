{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks of a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    abstract class used for our layers\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "\n",
    "def tanh(x: torch.FloatTensor):\n",
    "    \n",
    "    numerator = torch.exp(x) - torch.exp(-x)\n",
    "    denominator = torch.exp(x) + torch.exp(-x)\n",
    "    \n",
    "    return numerator/denominator\n",
    "\n",
    "# Derivative\n",
    "def tanh_p(x: torch.FloatTensor):\n",
    "    \n",
    "    return (1 - torch.pow(tanh(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9640])\n",
      "tensor([0.0707])\n"
     ]
    }
   ],
   "source": [
    "print(tanh(a))\n",
    "print(tanh_p(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9640], grad_fn=<TanhBackward>)\n",
      "tensor([0.0707])\n"
     ]
    }
   ],
   "source": [
    "b = torch.FloatTensor([2.0]) \n",
    "b.requires_grad_(True)\n",
    "\n",
    "y = torch.tanh(b)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: torch.FloatTensor):\n",
    "    return torch.clamp(x,min =0)\n",
    "\n",
    "def relu_p(x: torch.FloatTensor):\n",
    "    \n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = 0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "print(relu(a))\n",
    "print(relu_p(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], grad_fn=<ReluBackward0>)\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "b = torch.FloatTensor([2.0]) \n",
    "b.requires_grad_(True)\n",
    "\n",
    "y = torch.relu(b)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: torch.FloatTensor):\n",
    "    return (1 / (1 + torch.exp(-x)))\n",
    "\n",
    "def sigmoid_p(x: torch.FloatTensor):\n",
    "\n",
    "    return (sigmoid(x)*(1 - sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8808])\n",
      "tensor([0.1050])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "print(sigmoid(a))\n",
    "print(sigmoid_p(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8808], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.1050])\n"
     ]
    }
   ],
   "source": [
    "b = torch.FloatTensor([2.0]) \n",
    "b.requires_grad_(True)\n",
    "\n",
    "y = torch.sigmoid(b)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeakyRelu Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(torch.FloatTensor([1.0,2.0,3.0]), max = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRelu(x: torch.FloatTensor, slope: float):\n",
    "    return torch.clamp(x, min = 0) + slope*torch.clamp(x, max=0)\n",
    "\n",
    "def LRelu_p(x: torch.FloatTensor, slope: float):\n",
    "    \n",
    "    x[x>0] = 1\n",
    "    x[x<=0] = slope\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2.0]) \n",
    "print(LRelu(a, slope = 0.01))\n",
    "print(LRelu_p(a, slope = 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.FloatTensor([1.0,1.0,1.0,0.0,0.0,0.0])\n",
    "target = torch.FloatTensor([1.0,1.0,0.0,1.0,0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_output: torch.FloatTensor, target_output: torch.FloatTensor):\n",
    "    \n",
    "    return torch.pow(predicted_output - target_output, 2).sum()\n",
    "\n",
    "def mse_p(predicted_output: torch.FloatTensor, target_output: torch.FloatTensor):\n",
    "    return 2*(predicted_output - target_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0662,  4.4729])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.randn(2, 3)\n",
    "vec = torch.randn(3)\n",
    "torch.mv(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.output = input[0]\n",
    "        \n",
    "        return self.relu(self.output)\n",
    "    \n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.relu_p(self.output)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "    \n",
    "    \n",
    "    def relu(self, x):\n",
    "        \n",
    "        return torch.clamp(x, min =0)\n",
    "    \n",
    "    def relu_p(self, x):\n",
    "\n",
    "        x[x>0] = 1\n",
    "        x[x<=0] = 0\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        self.output = input[0]\n",
    "        \n",
    "        return self.tanh(self.output)\n",
    "    \n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \n",
    "        derivatives = self.tanh_p(self.output)\n",
    "        \n",
    "        return derivatives * gradwrtoutput[0]\n",
    "    \n",
    "    def tanh(self, to_compute):\n",
    "        \n",
    "        numerator = torch.exp(to_compute) - torch.exp(-to_compute)\n",
    "        denominator = torch.exp(to_compute) + torch.exp(-to_compute)\n",
    "        \n",
    "        return numerator/denominator\n",
    "        \n",
    "        \n",
    "    def tanh_p(self, x):\n",
    "\n",
    "        return (1 - torch.pow(self.tanh(x),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        \n",
    "        # Number of input neurons\n",
    "        self.in_features = in_features\n",
    "        # Number of output neurons\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initializing the weights with Xavierâ€™s initialization\n",
    "        # First generate the weights from a normal distribution with mean 0 and std 1\n",
    "        # Then multiply the samples by sqrt(1 / (number_of_input_neurons + number_of_output_neurons))\n",
    "        self.weight = torch.mul(torch.Tensor(out_features, in_features).normal_(mean=0, std=1), \\ \n",
    "                                torch.sqrt(torch.FloatTensor([1/ (self.in_features + self.out_features)])))\n",
    "        \n",
    "        # Zero bias initialization\n",
    "        self.bias = torch.Tensor(out_features).zero_()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, *input):\n",
    "        \n",
    "        # Input from the layer\n",
    "        self.input_from_layer = input[0]\n",
    "        \n",
    "        # Calculating the output, which is basically the multiplication\n",
    "        # of the weights with the input layer and adding the bias\n",
    "        self.output = torch.mv(weights, self.input_from_layer) + self.bias\n",
    "        \n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, initial_input):\n",
    "        \n",
    "        \n",
    "        output_single_layer = initial_input\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output_single_layer = layer.forward(output_single_layer)\n",
    "            \n",
    "        return output_single_layer\n",
    "    \n",
    "    def backward(self, initial_backward_input):\n",
    "        \n",
    "        output_single_layer_backward = initial_backward_input\n",
    "        \n",
    "        for layer in self.layers\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, \\text{in\\_features})` where :math:`*` means any number of\n",
    "          additional dimensions\n",
    "        - Output: :math:`(N, *, \\text{out\\_features})` where all but the last dimension\n",
    "          are the same shape as the input.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['bias']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
